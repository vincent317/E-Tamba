{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.4697986577181208,
  "eval_steps": 3500,
  "global_step": 3500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006711409395973154,
      "grad_norm": 9.76291561126709,
      "learning_rate": 0.00019932885906040267,
      "loss": 6.3744,
      "step": 50
    },
    {
      "epoch": 0.013422818791946308,
      "grad_norm": 6.147958278656006,
      "learning_rate": 0.0001986577181208054,
      "loss": 5.2863,
      "step": 100
    },
    {
      "epoch": 0.020134228187919462,
      "grad_norm": 5.431545734405518,
      "learning_rate": 0.00019798657718120806,
      "loss": 5.0889,
      "step": 150
    },
    {
      "epoch": 0.026845637583892617,
      "grad_norm": 3.9404029846191406,
      "learning_rate": 0.00019731543624161075,
      "loss": 4.7939,
      "step": 200
    },
    {
      "epoch": 0.03355704697986577,
      "grad_norm": 3.8592262268066406,
      "learning_rate": 0.00019664429530201342,
      "loss": 4.5786,
      "step": 250
    },
    {
      "epoch": 0.040268456375838924,
      "grad_norm": 3.698657989501953,
      "learning_rate": 0.00019597315436241613,
      "loss": 4.4425,
      "step": 300
    },
    {
      "epoch": 0.04697986577181208,
      "grad_norm": 3.2288243770599365,
      "learning_rate": 0.0001953020134228188,
      "loss": 4.4089,
      "step": 350
    },
    {
      "epoch": 0.053691275167785234,
      "grad_norm": 2.7144501209259033,
      "learning_rate": 0.0001946308724832215,
      "loss": 4.3284,
      "step": 400
    },
    {
      "epoch": 0.06040268456375839,
      "grad_norm": 3.168605327606201,
      "learning_rate": 0.00019395973154362416,
      "loss": 4.2628,
      "step": 450
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 2.6436657905578613,
      "learning_rate": 0.00019328859060402688,
      "loss": 4.264,
      "step": 500
    },
    {
      "epoch": 0.0738255033557047,
      "grad_norm": 3.1202750205993652,
      "learning_rate": 0.00019261744966442954,
      "loss": 4.1816,
      "step": 550
    },
    {
      "epoch": 0.08053691275167785,
      "grad_norm": 4.323246479034424,
      "learning_rate": 0.0001919463087248322,
      "loss": 4.1715,
      "step": 600
    },
    {
      "epoch": 0.087248322147651,
      "grad_norm": 2.914849042892456,
      "learning_rate": 0.0001912751677852349,
      "loss": 4.1779,
      "step": 650
    },
    {
      "epoch": 0.09395973154362416,
      "grad_norm": 2.5681986808776855,
      "learning_rate": 0.0001906040268456376,
      "loss": 4.1389,
      "step": 700
    },
    {
      "epoch": 0.10067114093959731,
      "grad_norm": 2.550813674926758,
      "learning_rate": 0.00018993288590604028,
      "loss": 4.1075,
      "step": 750
    },
    {
      "epoch": 0.10738255033557047,
      "grad_norm": 2.363677978515625,
      "learning_rate": 0.00018926174496644295,
      "loss": 4.097,
      "step": 800
    },
    {
      "epoch": 0.11409395973154363,
      "grad_norm": 2.320943832397461,
      "learning_rate": 0.00018859060402684564,
      "loss": 4.0895,
      "step": 850
    },
    {
      "epoch": 0.12080536912751678,
      "grad_norm": 2.3433618545532227,
      "learning_rate": 0.00018791946308724833,
      "loss": 4.0696,
      "step": 900
    },
    {
      "epoch": 0.12751677852348994,
      "grad_norm": 2.233490467071533,
      "learning_rate": 0.00018724832214765102,
      "loss": 4.051,
      "step": 950
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 2.341381072998047,
      "learning_rate": 0.0001865771812080537,
      "loss": 4.0377,
      "step": 1000
    },
    {
      "epoch": 0.14093959731543623,
      "grad_norm": 2.9077939987182617,
      "learning_rate": 0.00018590604026845638,
      "loss": 4.0099,
      "step": 1050
    },
    {
      "epoch": 0.1476510067114094,
      "grad_norm": 2.1887967586517334,
      "learning_rate": 0.00018523489932885907,
      "loss": 4.0124,
      "step": 1100
    },
    {
      "epoch": 0.15436241610738255,
      "grad_norm": 2.6332523822784424,
      "learning_rate": 0.00018456375838926174,
      "loss": 4.0078,
      "step": 1150
    },
    {
      "epoch": 0.1610738255033557,
      "grad_norm": 2.3427298069000244,
      "learning_rate": 0.00018389261744966443,
      "loss": 4.0036,
      "step": 1200
    },
    {
      "epoch": 0.16778523489932887,
      "grad_norm": 2.49576997756958,
      "learning_rate": 0.00018322147651006712,
      "loss": 4.0017,
      "step": 1250
    },
    {
      "epoch": 0.174496644295302,
      "grad_norm": 2.27463436126709,
      "learning_rate": 0.00018255033557046981,
      "loss": 3.9747,
      "step": 1300
    },
    {
      "epoch": 0.18120805369127516,
      "grad_norm": 2.0607120990753174,
      "learning_rate": 0.00018187919463087248,
      "loss": 4.009,
      "step": 1350
    },
    {
      "epoch": 0.18791946308724833,
      "grad_norm": 2.1656742095947266,
      "learning_rate": 0.00018120805369127517,
      "loss": 3.9373,
      "step": 1400
    },
    {
      "epoch": 0.19463087248322147,
      "grad_norm": 2.2813687324523926,
      "learning_rate": 0.00018053691275167786,
      "loss": 3.9853,
      "step": 1450
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 2.083205223083496,
      "learning_rate": 0.00017986577181208056,
      "loss": 3.9077,
      "step": 1500
    },
    {
      "epoch": 0.2080536912751678,
      "grad_norm": 2.522930145263672,
      "learning_rate": 0.00017919463087248322,
      "loss": 3.944,
      "step": 1550
    },
    {
      "epoch": 0.21476510067114093,
      "grad_norm": 1.9890559911727905,
      "learning_rate": 0.0001785234899328859,
      "loss": 3.9472,
      "step": 1600
    },
    {
      "epoch": 0.2214765100671141,
      "grad_norm": 2.7502059936523438,
      "learning_rate": 0.0001778523489932886,
      "loss": 3.9943,
      "step": 1650
    },
    {
      "epoch": 0.22818791946308725,
      "grad_norm": 2.2661354541778564,
      "learning_rate": 0.0001771812080536913,
      "loss": 3.9344,
      "step": 1700
    },
    {
      "epoch": 0.2348993288590604,
      "grad_norm": 1.9702156782150269,
      "learning_rate": 0.00017651006711409396,
      "loss": 3.9075,
      "step": 1750
    },
    {
      "epoch": 0.24161073825503357,
      "grad_norm": 1.967592716217041,
      "learning_rate": 0.00017583892617449665,
      "loss": 3.9332,
      "step": 1800
    },
    {
      "epoch": 0.2483221476510067,
      "grad_norm": 1.9875054359436035,
      "learning_rate": 0.00017516778523489935,
      "loss": 3.9302,
      "step": 1850
    },
    {
      "epoch": 0.2550335570469799,
      "grad_norm": 2.074561834335327,
      "learning_rate": 0.000174496644295302,
      "loss": 3.8833,
      "step": 1900
    },
    {
      "epoch": 0.26174496644295303,
      "grad_norm": 2.250723123550415,
      "learning_rate": 0.0001738255033557047,
      "loss": 3.9434,
      "step": 1950
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 2.1008102893829346,
      "learning_rate": 0.0001731543624161074,
      "loss": 3.8837,
      "step": 2000
    },
    {
      "epoch": 0.2751677852348993,
      "grad_norm": 2.4409961700439453,
      "learning_rate": 0.0001724832214765101,
      "loss": 3.9174,
      "step": 2050
    },
    {
      "epoch": 0.28187919463087246,
      "grad_norm": 2.200366258621216,
      "learning_rate": 0.00017181208053691275,
      "loss": 3.8413,
      "step": 2100
    },
    {
      "epoch": 0.28859060402684567,
      "grad_norm": 1.953946590423584,
      "learning_rate": 0.00017114093959731544,
      "loss": 3.8873,
      "step": 2150
    },
    {
      "epoch": 0.2953020134228188,
      "grad_norm": 2.2515640258789062,
      "learning_rate": 0.00017046979865771814,
      "loss": 3.8174,
      "step": 2200
    },
    {
      "epoch": 0.30201342281879195,
      "grad_norm": 1.904086947441101,
      "learning_rate": 0.00016979865771812083,
      "loss": 3.8626,
      "step": 2250
    },
    {
      "epoch": 0.3087248322147651,
      "grad_norm": 1.9325824975967407,
      "learning_rate": 0.0001691275167785235,
      "loss": 3.93,
      "step": 2300
    },
    {
      "epoch": 0.31543624161073824,
      "grad_norm": 2.224473476409912,
      "learning_rate": 0.00016845637583892619,
      "loss": 3.8731,
      "step": 2350
    },
    {
      "epoch": 0.3221476510067114,
      "grad_norm": 1.8665449619293213,
      "learning_rate": 0.00016778523489932888,
      "loss": 3.8862,
      "step": 2400
    },
    {
      "epoch": 0.3288590604026846,
      "grad_norm": 2.212570905685425,
      "learning_rate": 0.00016711409395973154,
      "loss": 3.9278,
      "step": 2450
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 2.3300766944885254,
      "learning_rate": 0.00016644295302013423,
      "loss": 3.8391,
      "step": 2500
    },
    {
      "epoch": 0.3422818791946309,
      "grad_norm": 2.237867832183838,
      "learning_rate": 0.00016577181208053693,
      "loss": 3.8496,
      "step": 2550
    },
    {
      "epoch": 0.348993288590604,
      "grad_norm": 2.131697654724121,
      "learning_rate": 0.00016510067114093962,
      "loss": 3.8246,
      "step": 2600
    },
    {
      "epoch": 0.35570469798657717,
      "grad_norm": 2.215324878692627,
      "learning_rate": 0.00016442953020134228,
      "loss": 3.866,
      "step": 2650
    },
    {
      "epoch": 0.3624161073825503,
      "grad_norm": 2.4978621006011963,
      "learning_rate": 0.00016375838926174498,
      "loss": 3.838,
      "step": 2700
    },
    {
      "epoch": 0.3691275167785235,
      "grad_norm": 1.9225152730941772,
      "learning_rate": 0.00016308724832214767,
      "loss": 3.8938,
      "step": 2750
    },
    {
      "epoch": 0.37583892617449666,
      "grad_norm": 2.0942625999450684,
      "learning_rate": 0.00016241610738255036,
      "loss": 3.8599,
      "step": 2800
    },
    {
      "epoch": 0.3825503355704698,
      "grad_norm": 2.0202109813690186,
      "learning_rate": 0.00016174496644295302,
      "loss": 3.7686,
      "step": 2850
    },
    {
      "epoch": 0.38926174496644295,
      "grad_norm": 1.8244993686676025,
      "learning_rate": 0.0001610738255033557,
      "loss": 3.8712,
      "step": 2900
    },
    {
      "epoch": 0.3959731543624161,
      "grad_norm": 1.7788095474243164,
      "learning_rate": 0.0001604026845637584,
      "loss": 3.8156,
      "step": 2950
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 2.077664375305176,
      "learning_rate": 0.00015973154362416107,
      "loss": 3.7651,
      "step": 3000
    },
    {
      "epoch": 0.40939597315436244,
      "grad_norm": 1.8456828594207764,
      "learning_rate": 0.00015906040268456377,
      "loss": 3.8081,
      "step": 3050
    },
    {
      "epoch": 0.4161073825503356,
      "grad_norm": 1.816679835319519,
      "learning_rate": 0.00015838926174496643,
      "loss": 3.841,
      "step": 3100
    },
    {
      "epoch": 0.4228187919463087,
      "grad_norm": 2.017704963684082,
      "learning_rate": 0.00015771812080536915,
      "loss": 3.839,
      "step": 3150
    },
    {
      "epoch": 0.42953020134228187,
      "grad_norm": 1.7677229642868042,
      "learning_rate": 0.00015704697986577181,
      "loss": 3.8112,
      "step": 3200
    },
    {
      "epoch": 0.436241610738255,
      "grad_norm": 2.0390050411224365,
      "learning_rate": 0.0001563758389261745,
      "loss": 3.8483,
      "step": 3250
    },
    {
      "epoch": 0.4429530201342282,
      "grad_norm": 1.9494991302490234,
      "learning_rate": 0.0001557046979865772,
      "loss": 3.8103,
      "step": 3300
    },
    {
      "epoch": 0.44966442953020136,
      "grad_norm": 2.0421464443206787,
      "learning_rate": 0.0001550335570469799,
      "loss": 3.8279,
      "step": 3350
    },
    {
      "epoch": 0.4563758389261745,
      "grad_norm": 1.8679497241973877,
      "learning_rate": 0.00015436241610738256,
      "loss": 3.8216,
      "step": 3400
    },
    {
      "epoch": 0.46308724832214765,
      "grad_norm": 1.9072271585464478,
      "learning_rate": 0.00015369127516778522,
      "loss": 3.8172,
      "step": 3450
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 2.9276883602142334,
      "learning_rate": 0.00015302013422818794,
      "loss": 3.7794,
      "step": 3500
    },
    {
      "epoch": 0.4697986577181208,
      "eval_loss": 3.819903612136841,
      "eval_runtime": 157.4728,
      "eval_samples_per_second": 46.954,
      "eval_steps_per_second": 5.874,
      "step": 3500
    }
  ],
  "logging_steps": 50,
  "max_steps": 14900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 3500,
  "total_flos": 1.9075700883456e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
