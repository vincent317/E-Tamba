{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9395973154362416,
  "eval_steps": 3500,
  "global_step": 7000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006711409395973154,
      "grad_norm": 9.76291561126709,
      "learning_rate": 0.00019932885906040267,
      "loss": 6.3744,
      "step": 50
    },
    {
      "epoch": 0.013422818791946308,
      "grad_norm": 6.147958278656006,
      "learning_rate": 0.0001986577181208054,
      "loss": 5.2863,
      "step": 100
    },
    {
      "epoch": 0.020134228187919462,
      "grad_norm": 5.431545734405518,
      "learning_rate": 0.00019798657718120806,
      "loss": 5.0889,
      "step": 150
    },
    {
      "epoch": 0.026845637583892617,
      "grad_norm": 3.9404029846191406,
      "learning_rate": 0.00019731543624161075,
      "loss": 4.7939,
      "step": 200
    },
    {
      "epoch": 0.03355704697986577,
      "grad_norm": 3.8592262268066406,
      "learning_rate": 0.00019664429530201342,
      "loss": 4.5786,
      "step": 250
    },
    {
      "epoch": 0.040268456375838924,
      "grad_norm": 3.698657989501953,
      "learning_rate": 0.00019597315436241613,
      "loss": 4.4425,
      "step": 300
    },
    {
      "epoch": 0.04697986577181208,
      "grad_norm": 3.2288243770599365,
      "learning_rate": 0.0001953020134228188,
      "loss": 4.4089,
      "step": 350
    },
    {
      "epoch": 0.053691275167785234,
      "grad_norm": 2.7144501209259033,
      "learning_rate": 0.0001946308724832215,
      "loss": 4.3284,
      "step": 400
    },
    {
      "epoch": 0.06040268456375839,
      "grad_norm": 3.168605327606201,
      "learning_rate": 0.00019395973154362416,
      "loss": 4.2628,
      "step": 450
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 2.6436657905578613,
      "learning_rate": 0.00019328859060402688,
      "loss": 4.264,
      "step": 500
    },
    {
      "epoch": 0.0738255033557047,
      "grad_norm": 3.1202750205993652,
      "learning_rate": 0.00019261744966442954,
      "loss": 4.1816,
      "step": 550
    },
    {
      "epoch": 0.08053691275167785,
      "grad_norm": 4.323246479034424,
      "learning_rate": 0.0001919463087248322,
      "loss": 4.1715,
      "step": 600
    },
    {
      "epoch": 0.087248322147651,
      "grad_norm": 2.914849042892456,
      "learning_rate": 0.0001912751677852349,
      "loss": 4.1779,
      "step": 650
    },
    {
      "epoch": 0.09395973154362416,
      "grad_norm": 2.5681986808776855,
      "learning_rate": 0.0001906040268456376,
      "loss": 4.1389,
      "step": 700
    },
    {
      "epoch": 0.10067114093959731,
      "grad_norm": 2.550813674926758,
      "learning_rate": 0.00018993288590604028,
      "loss": 4.1075,
      "step": 750
    },
    {
      "epoch": 0.10738255033557047,
      "grad_norm": 2.363677978515625,
      "learning_rate": 0.00018926174496644295,
      "loss": 4.097,
      "step": 800
    },
    {
      "epoch": 0.11409395973154363,
      "grad_norm": 2.320943832397461,
      "learning_rate": 0.00018859060402684564,
      "loss": 4.0895,
      "step": 850
    },
    {
      "epoch": 0.12080536912751678,
      "grad_norm": 2.3433618545532227,
      "learning_rate": 0.00018791946308724833,
      "loss": 4.0696,
      "step": 900
    },
    {
      "epoch": 0.12751677852348994,
      "grad_norm": 2.233490467071533,
      "learning_rate": 0.00018724832214765102,
      "loss": 4.051,
      "step": 950
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 2.341381072998047,
      "learning_rate": 0.0001865771812080537,
      "loss": 4.0377,
      "step": 1000
    },
    {
      "epoch": 0.14093959731543623,
      "grad_norm": 2.9077939987182617,
      "learning_rate": 0.00018590604026845638,
      "loss": 4.0099,
      "step": 1050
    },
    {
      "epoch": 0.1476510067114094,
      "grad_norm": 2.1887967586517334,
      "learning_rate": 0.00018523489932885907,
      "loss": 4.0124,
      "step": 1100
    },
    {
      "epoch": 0.15436241610738255,
      "grad_norm": 2.6332523822784424,
      "learning_rate": 0.00018456375838926174,
      "loss": 4.0078,
      "step": 1150
    },
    {
      "epoch": 0.1610738255033557,
      "grad_norm": 2.3427298069000244,
      "learning_rate": 0.00018389261744966443,
      "loss": 4.0036,
      "step": 1200
    },
    {
      "epoch": 0.16778523489932887,
      "grad_norm": 2.49576997756958,
      "learning_rate": 0.00018322147651006712,
      "loss": 4.0017,
      "step": 1250
    },
    {
      "epoch": 0.174496644295302,
      "grad_norm": 2.27463436126709,
      "learning_rate": 0.00018255033557046981,
      "loss": 3.9747,
      "step": 1300
    },
    {
      "epoch": 0.18120805369127516,
      "grad_norm": 2.0607120990753174,
      "learning_rate": 0.00018187919463087248,
      "loss": 4.009,
      "step": 1350
    },
    {
      "epoch": 0.18791946308724833,
      "grad_norm": 2.1656742095947266,
      "learning_rate": 0.00018120805369127517,
      "loss": 3.9373,
      "step": 1400
    },
    {
      "epoch": 0.19463087248322147,
      "grad_norm": 2.2813687324523926,
      "learning_rate": 0.00018053691275167786,
      "loss": 3.9853,
      "step": 1450
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 2.083205223083496,
      "learning_rate": 0.00017986577181208056,
      "loss": 3.9077,
      "step": 1500
    },
    {
      "epoch": 0.2080536912751678,
      "grad_norm": 2.522930145263672,
      "learning_rate": 0.00017919463087248322,
      "loss": 3.944,
      "step": 1550
    },
    {
      "epoch": 0.21476510067114093,
      "grad_norm": 1.9890559911727905,
      "learning_rate": 0.0001785234899328859,
      "loss": 3.9472,
      "step": 1600
    },
    {
      "epoch": 0.2214765100671141,
      "grad_norm": 2.7502059936523438,
      "learning_rate": 0.0001778523489932886,
      "loss": 3.9943,
      "step": 1650
    },
    {
      "epoch": 0.22818791946308725,
      "grad_norm": 2.2661354541778564,
      "learning_rate": 0.0001771812080536913,
      "loss": 3.9344,
      "step": 1700
    },
    {
      "epoch": 0.2348993288590604,
      "grad_norm": 1.9702156782150269,
      "learning_rate": 0.00017651006711409396,
      "loss": 3.9075,
      "step": 1750
    },
    {
      "epoch": 0.24161073825503357,
      "grad_norm": 1.967592716217041,
      "learning_rate": 0.00017583892617449665,
      "loss": 3.9332,
      "step": 1800
    },
    {
      "epoch": 0.2483221476510067,
      "grad_norm": 1.9875054359436035,
      "learning_rate": 0.00017516778523489935,
      "loss": 3.9302,
      "step": 1850
    },
    {
      "epoch": 0.2550335570469799,
      "grad_norm": 2.074561834335327,
      "learning_rate": 0.000174496644295302,
      "loss": 3.8833,
      "step": 1900
    },
    {
      "epoch": 0.26174496644295303,
      "grad_norm": 2.250723123550415,
      "learning_rate": 0.0001738255033557047,
      "loss": 3.9434,
      "step": 1950
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 2.1008102893829346,
      "learning_rate": 0.0001731543624161074,
      "loss": 3.8837,
      "step": 2000
    },
    {
      "epoch": 0.2751677852348993,
      "grad_norm": 2.4409961700439453,
      "learning_rate": 0.0001724832214765101,
      "loss": 3.9174,
      "step": 2050
    },
    {
      "epoch": 0.28187919463087246,
      "grad_norm": 2.200366258621216,
      "learning_rate": 0.00017181208053691275,
      "loss": 3.8413,
      "step": 2100
    },
    {
      "epoch": 0.28859060402684567,
      "grad_norm": 1.953946590423584,
      "learning_rate": 0.00017114093959731544,
      "loss": 3.8873,
      "step": 2150
    },
    {
      "epoch": 0.2953020134228188,
      "grad_norm": 2.2515640258789062,
      "learning_rate": 0.00017046979865771814,
      "loss": 3.8174,
      "step": 2200
    },
    {
      "epoch": 0.30201342281879195,
      "grad_norm": 1.904086947441101,
      "learning_rate": 0.00016979865771812083,
      "loss": 3.8626,
      "step": 2250
    },
    {
      "epoch": 0.3087248322147651,
      "grad_norm": 1.9325824975967407,
      "learning_rate": 0.0001691275167785235,
      "loss": 3.93,
      "step": 2300
    },
    {
      "epoch": 0.31543624161073824,
      "grad_norm": 2.224473476409912,
      "learning_rate": 0.00016845637583892619,
      "loss": 3.8731,
      "step": 2350
    },
    {
      "epoch": 0.3221476510067114,
      "grad_norm": 1.8665449619293213,
      "learning_rate": 0.00016778523489932888,
      "loss": 3.8862,
      "step": 2400
    },
    {
      "epoch": 0.3288590604026846,
      "grad_norm": 2.212570905685425,
      "learning_rate": 0.00016711409395973154,
      "loss": 3.9278,
      "step": 2450
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 2.3300766944885254,
      "learning_rate": 0.00016644295302013423,
      "loss": 3.8391,
      "step": 2500
    },
    {
      "epoch": 0.3422818791946309,
      "grad_norm": 2.237867832183838,
      "learning_rate": 0.00016577181208053693,
      "loss": 3.8496,
      "step": 2550
    },
    {
      "epoch": 0.348993288590604,
      "grad_norm": 2.131697654724121,
      "learning_rate": 0.00016510067114093962,
      "loss": 3.8246,
      "step": 2600
    },
    {
      "epoch": 0.35570469798657717,
      "grad_norm": 2.215324878692627,
      "learning_rate": 0.00016442953020134228,
      "loss": 3.866,
      "step": 2650
    },
    {
      "epoch": 0.3624161073825503,
      "grad_norm": 2.4978621006011963,
      "learning_rate": 0.00016375838926174498,
      "loss": 3.838,
      "step": 2700
    },
    {
      "epoch": 0.3691275167785235,
      "grad_norm": 1.9225152730941772,
      "learning_rate": 0.00016308724832214767,
      "loss": 3.8938,
      "step": 2750
    },
    {
      "epoch": 0.37583892617449666,
      "grad_norm": 2.0942625999450684,
      "learning_rate": 0.00016241610738255036,
      "loss": 3.8599,
      "step": 2800
    },
    {
      "epoch": 0.3825503355704698,
      "grad_norm": 2.0202109813690186,
      "learning_rate": 0.00016174496644295302,
      "loss": 3.7686,
      "step": 2850
    },
    {
      "epoch": 0.38926174496644295,
      "grad_norm": 1.8244993686676025,
      "learning_rate": 0.0001610738255033557,
      "loss": 3.8712,
      "step": 2900
    },
    {
      "epoch": 0.3959731543624161,
      "grad_norm": 1.7788095474243164,
      "learning_rate": 0.0001604026845637584,
      "loss": 3.8156,
      "step": 2950
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 2.077664375305176,
      "learning_rate": 0.00015973154362416107,
      "loss": 3.7651,
      "step": 3000
    },
    {
      "epoch": 0.40939597315436244,
      "grad_norm": 1.8456828594207764,
      "learning_rate": 0.00015906040268456377,
      "loss": 3.8081,
      "step": 3050
    },
    {
      "epoch": 0.4161073825503356,
      "grad_norm": 1.816679835319519,
      "learning_rate": 0.00015838926174496643,
      "loss": 3.841,
      "step": 3100
    },
    {
      "epoch": 0.4228187919463087,
      "grad_norm": 2.017704963684082,
      "learning_rate": 0.00015771812080536915,
      "loss": 3.839,
      "step": 3150
    },
    {
      "epoch": 0.42953020134228187,
      "grad_norm": 1.7677229642868042,
      "learning_rate": 0.00015704697986577181,
      "loss": 3.8112,
      "step": 3200
    },
    {
      "epoch": 0.436241610738255,
      "grad_norm": 2.0390050411224365,
      "learning_rate": 0.0001563758389261745,
      "loss": 3.8483,
      "step": 3250
    },
    {
      "epoch": 0.4429530201342282,
      "grad_norm": 1.9494991302490234,
      "learning_rate": 0.0001557046979865772,
      "loss": 3.8103,
      "step": 3300
    },
    {
      "epoch": 0.44966442953020136,
      "grad_norm": 2.0421464443206787,
      "learning_rate": 0.0001550335570469799,
      "loss": 3.8279,
      "step": 3350
    },
    {
      "epoch": 0.4563758389261745,
      "grad_norm": 1.8679497241973877,
      "learning_rate": 0.00015436241610738256,
      "loss": 3.8216,
      "step": 3400
    },
    {
      "epoch": 0.46308724832214765,
      "grad_norm": 1.9072271585464478,
      "learning_rate": 0.00015369127516778522,
      "loss": 3.8172,
      "step": 3450
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 2.9276883602142334,
      "learning_rate": 0.00015302013422818794,
      "loss": 3.7794,
      "step": 3500
    },
    {
      "epoch": 0.4697986577181208,
      "eval_loss": 3.819903612136841,
      "eval_runtime": 157.4728,
      "eval_samples_per_second": 46.954,
      "eval_steps_per_second": 5.874,
      "step": 3500
    },
    {
      "epoch": 0.47651006711409394,
      "grad_norm": 1.9343053102493286,
      "learning_rate": 0.0001523489932885906,
      "loss": 3.8335,
      "step": 3550
    },
    {
      "epoch": 0.48322147651006714,
      "grad_norm": 1.9856635332107544,
      "learning_rate": 0.0001516778523489933,
      "loss": 3.7995,
      "step": 3600
    },
    {
      "epoch": 0.4899328859060403,
      "grad_norm": 1.7829244136810303,
      "learning_rate": 0.00015100671140939596,
      "loss": 3.7461,
      "step": 3650
    },
    {
      "epoch": 0.4966442953020134,
      "grad_norm": 2.0373854637145996,
      "learning_rate": 0.00015033557046979868,
      "loss": 3.7912,
      "step": 3700
    },
    {
      "epoch": 0.5033557046979866,
      "grad_norm": 2.3217556476593018,
      "learning_rate": 0.00014966442953020135,
      "loss": 3.7968,
      "step": 3750
    },
    {
      "epoch": 0.5100671140939598,
      "grad_norm": 1.9560930728912354,
      "learning_rate": 0.00014899328859060404,
      "loss": 3.7598,
      "step": 3800
    },
    {
      "epoch": 0.5167785234899329,
      "grad_norm": 1.9263492822647095,
      "learning_rate": 0.0001483221476510067,
      "loss": 3.7048,
      "step": 3850
    },
    {
      "epoch": 0.5234899328859061,
      "grad_norm": 2.1479504108428955,
      "learning_rate": 0.00014765100671140942,
      "loss": 3.7715,
      "step": 3900
    },
    {
      "epoch": 0.5302013422818792,
      "grad_norm": 1.899453043937683,
      "learning_rate": 0.0001469798657718121,
      "loss": 3.7414,
      "step": 3950
    },
    {
      "epoch": 0.5369127516778524,
      "grad_norm": 1.9471064805984497,
      "learning_rate": 0.00014630872483221478,
      "loss": 3.7737,
      "step": 4000
    },
    {
      "epoch": 0.5436241610738255,
      "grad_norm": 2.07108473777771,
      "learning_rate": 0.00014563758389261744,
      "loss": 3.8278,
      "step": 4050
    },
    {
      "epoch": 0.5503355704697986,
      "grad_norm": 2.0954501628875732,
      "learning_rate": 0.00014496644295302014,
      "loss": 3.7693,
      "step": 4100
    },
    {
      "epoch": 0.5570469798657718,
      "grad_norm": 1.6807000637054443,
      "learning_rate": 0.00014429530201342283,
      "loss": 3.7349,
      "step": 4150
    },
    {
      "epoch": 0.5637583892617449,
      "grad_norm": 1.8638263940811157,
      "learning_rate": 0.0001436241610738255,
      "loss": 3.8546,
      "step": 4200
    },
    {
      "epoch": 0.5704697986577181,
      "grad_norm": 2.1892662048339844,
      "learning_rate": 0.00014295302013422819,
      "loss": 3.7528,
      "step": 4250
    },
    {
      "epoch": 0.5771812080536913,
      "grad_norm": 2.7254104614257812,
      "learning_rate": 0.00014228187919463088,
      "loss": 3.7879,
      "step": 4300
    },
    {
      "epoch": 0.5838926174496645,
      "grad_norm": 1.7408696413040161,
      "learning_rate": 0.00014161073825503357,
      "loss": 3.7179,
      "step": 4350
    },
    {
      "epoch": 0.5906040268456376,
      "grad_norm": 1.9033234119415283,
      "learning_rate": 0.00014093959731543624,
      "loss": 3.7662,
      "step": 4400
    },
    {
      "epoch": 0.5973154362416108,
      "grad_norm": 2.1683738231658936,
      "learning_rate": 0.00014026845637583895,
      "loss": 3.7583,
      "step": 4450
    },
    {
      "epoch": 0.6040268456375839,
      "grad_norm": 1.9414604902267456,
      "learning_rate": 0.00013959731543624162,
      "loss": 3.728,
      "step": 4500
    },
    {
      "epoch": 0.610738255033557,
      "grad_norm": 1.6500420570373535,
      "learning_rate": 0.0001389261744966443,
      "loss": 3.7271,
      "step": 4550
    },
    {
      "epoch": 0.6174496644295302,
      "grad_norm": 1.9738832712173462,
      "learning_rate": 0.00013825503355704698,
      "loss": 3.7536,
      "step": 4600
    },
    {
      "epoch": 0.6241610738255033,
      "grad_norm": 1.9532337188720703,
      "learning_rate": 0.00013758389261744967,
      "loss": 3.7965,
      "step": 4650
    },
    {
      "epoch": 0.6308724832214765,
      "grad_norm": 1.915177822113037,
      "learning_rate": 0.00013691275167785236,
      "loss": 3.7685,
      "step": 4700
    },
    {
      "epoch": 0.6375838926174496,
      "grad_norm": 1.591180682182312,
      "learning_rate": 0.00013624161073825503,
      "loss": 3.7537,
      "step": 4750
    },
    {
      "epoch": 0.6442953020134228,
      "grad_norm": 1.8513423204421997,
      "learning_rate": 0.00013557046979865772,
      "loss": 3.7878,
      "step": 4800
    },
    {
      "epoch": 0.6510067114093959,
      "grad_norm": 2.123608350753784,
      "learning_rate": 0.0001348993288590604,
      "loss": 3.7452,
      "step": 4850
    },
    {
      "epoch": 0.6577181208053692,
      "grad_norm": 1.9018489122390747,
      "learning_rate": 0.0001342281879194631,
      "loss": 3.7639,
      "step": 4900
    },
    {
      "epoch": 0.6644295302013423,
      "grad_norm": 2.0423192977905273,
      "learning_rate": 0.00013355704697986577,
      "loss": 3.7226,
      "step": 4950
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 1.7277112007141113,
      "learning_rate": 0.00013288590604026846,
      "loss": 3.738,
      "step": 5000
    },
    {
      "epoch": 0.6778523489932886,
      "grad_norm": 1.8612254858016968,
      "learning_rate": 0.00013221476510067115,
      "loss": 3.7708,
      "step": 5050
    },
    {
      "epoch": 0.6845637583892618,
      "grad_norm": 2.0442166328430176,
      "learning_rate": 0.00013154362416107384,
      "loss": 3.7755,
      "step": 5100
    },
    {
      "epoch": 0.6912751677852349,
      "grad_norm": 1.8707512617111206,
      "learning_rate": 0.0001308724832214765,
      "loss": 3.7641,
      "step": 5150
    },
    {
      "epoch": 0.697986577181208,
      "grad_norm": 1.8773969411849976,
      "learning_rate": 0.0001302013422818792,
      "loss": 3.7439,
      "step": 5200
    },
    {
      "epoch": 0.7046979865771812,
      "grad_norm": 1.8990896940231323,
      "learning_rate": 0.0001295302013422819,
      "loss": 3.7345,
      "step": 5250
    },
    {
      "epoch": 0.7114093959731543,
      "grad_norm": 1.754605770111084,
      "learning_rate": 0.00012885906040268456,
      "loss": 3.7968,
      "step": 5300
    },
    {
      "epoch": 0.7181208053691275,
      "grad_norm": 1.8087323904037476,
      "learning_rate": 0.00012818791946308725,
      "loss": 3.6892,
      "step": 5350
    },
    {
      "epoch": 0.7248322147651006,
      "grad_norm": 1.7670347690582275,
      "learning_rate": 0.00012751677852348994,
      "loss": 3.7949,
      "step": 5400
    },
    {
      "epoch": 0.7315436241610739,
      "grad_norm": 1.7536414861679077,
      "learning_rate": 0.00012684563758389263,
      "loss": 3.7019,
      "step": 5450
    },
    {
      "epoch": 0.738255033557047,
      "grad_norm": 1.8770837783813477,
      "learning_rate": 0.0001261744966442953,
      "loss": 3.7809,
      "step": 5500
    },
    {
      "epoch": 0.7449664429530202,
      "grad_norm": 1.7503114938735962,
      "learning_rate": 0.000125503355704698,
      "loss": 3.7593,
      "step": 5550
    },
    {
      "epoch": 0.7516778523489933,
      "grad_norm": 1.8459446430206299,
      "learning_rate": 0.00012483221476510068,
      "loss": 3.7182,
      "step": 5600
    },
    {
      "epoch": 0.7583892617449665,
      "grad_norm": 1.7982112169265747,
      "learning_rate": 0.00012416107382550337,
      "loss": 3.7317,
      "step": 5650
    },
    {
      "epoch": 0.7651006711409396,
      "grad_norm": 1.994579792022705,
      "learning_rate": 0.00012348993288590604,
      "loss": 3.7181,
      "step": 5700
    },
    {
      "epoch": 0.7718120805369127,
      "grad_norm": 1.7562785148620605,
      "learning_rate": 0.00012281879194630873,
      "loss": 3.7201,
      "step": 5750
    },
    {
      "epoch": 0.7785234899328859,
      "grad_norm": 1.8677860498428345,
      "learning_rate": 0.00012214765100671142,
      "loss": 3.7205,
      "step": 5800
    },
    {
      "epoch": 0.785234899328859,
      "grad_norm": 1.5928508043289185,
      "learning_rate": 0.00012147651006711409,
      "loss": 3.6837,
      "step": 5850
    },
    {
      "epoch": 0.7919463087248322,
      "grad_norm": 1.8394744396209717,
      "learning_rate": 0.0001208053691275168,
      "loss": 3.738,
      "step": 5900
    },
    {
      "epoch": 0.7986577181208053,
      "grad_norm": 1.6827718019485474,
      "learning_rate": 0.00012013422818791946,
      "loss": 3.7434,
      "step": 5950
    },
    {
      "epoch": 0.8053691275167785,
      "grad_norm": 1.8026622533798218,
      "learning_rate": 0.00011946308724832216,
      "loss": 3.7436,
      "step": 6000
    },
    {
      "epoch": 0.8120805369127517,
      "grad_norm": 1.93060302734375,
      "learning_rate": 0.00011879194630872483,
      "loss": 3.6676,
      "step": 6050
    },
    {
      "epoch": 0.8187919463087249,
      "grad_norm": 1.832889199256897,
      "learning_rate": 0.00011812080536912754,
      "loss": 3.7323,
      "step": 6100
    },
    {
      "epoch": 0.825503355704698,
      "grad_norm": 1.95548415184021,
      "learning_rate": 0.0001174496644295302,
      "loss": 3.6927,
      "step": 6150
    },
    {
      "epoch": 0.8322147651006712,
      "grad_norm": 1.7701032161712646,
      "learning_rate": 0.0001167785234899329,
      "loss": 3.6612,
      "step": 6200
    },
    {
      "epoch": 0.8389261744966443,
      "grad_norm": 1.810914158821106,
      "learning_rate": 0.00011610738255033557,
      "loss": 3.6991,
      "step": 6250
    },
    {
      "epoch": 0.8456375838926175,
      "grad_norm": 1.7224668264389038,
      "learning_rate": 0.00011543624161073828,
      "loss": 3.6973,
      "step": 6300
    },
    {
      "epoch": 0.8523489932885906,
      "grad_norm": 1.694105863571167,
      "learning_rate": 0.00011476510067114094,
      "loss": 3.7326,
      "step": 6350
    },
    {
      "epoch": 0.8590604026845637,
      "grad_norm": 1.7891417741775513,
      "learning_rate": 0.00011409395973154362,
      "loss": 3.6984,
      "step": 6400
    },
    {
      "epoch": 0.8657718120805369,
      "grad_norm": 1.8742501735687256,
      "learning_rate": 0.00011342281879194631,
      "loss": 3.6438,
      "step": 6450
    },
    {
      "epoch": 0.87248322147651,
      "grad_norm": 1.6098870038986206,
      "learning_rate": 0.00011275167785234899,
      "loss": 3.7155,
      "step": 6500
    },
    {
      "epoch": 0.8791946308724832,
      "grad_norm": 1.648972749710083,
      "learning_rate": 0.00011208053691275168,
      "loss": 3.7358,
      "step": 6550
    },
    {
      "epoch": 0.8859060402684564,
      "grad_norm": 1.773345708847046,
      "learning_rate": 0.00011140939597315436,
      "loss": 3.6344,
      "step": 6600
    },
    {
      "epoch": 0.8926174496644296,
      "grad_norm": 1.688447117805481,
      "learning_rate": 0.00011073825503355705,
      "loss": 3.6634,
      "step": 6650
    },
    {
      "epoch": 0.8993288590604027,
      "grad_norm": 1.6986675262451172,
      "learning_rate": 0.00011006711409395973,
      "loss": 3.6835,
      "step": 6700
    },
    {
      "epoch": 0.9060402684563759,
      "grad_norm": 1.9521842002868652,
      "learning_rate": 0.00010939597315436242,
      "loss": 3.6643,
      "step": 6750
    },
    {
      "epoch": 0.912751677852349,
      "grad_norm": 1.933103084564209,
      "learning_rate": 0.0001087248322147651,
      "loss": 3.704,
      "step": 6800
    },
    {
      "epoch": 0.9194630872483222,
      "grad_norm": 1.788414716720581,
      "learning_rate": 0.0001080536912751678,
      "loss": 3.6997,
      "step": 6850
    },
    {
      "epoch": 0.9261744966442953,
      "grad_norm": 1.689279556274414,
      "learning_rate": 0.00010738255033557047,
      "loss": 3.6812,
      "step": 6900
    },
    {
      "epoch": 0.9328859060402684,
      "grad_norm": 1.7318956851959229,
      "learning_rate": 0.00010671140939597315,
      "loss": 3.6802,
      "step": 6950
    },
    {
      "epoch": 0.9395973154362416,
      "grad_norm": 1.7405134439468384,
      "learning_rate": 0.00010604026845637584,
      "loss": 3.7215,
      "step": 7000
    },
    {
      "epoch": 0.9395973154362416,
      "eval_loss": 3.7047133445739746,
      "eval_runtime": 157.6355,
      "eval_samples_per_second": 46.906,
      "eval_steps_per_second": 5.868,
      "step": 7000
    }
  ],
  "logging_steps": 50,
  "max_steps": 14900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 3500,
  "total_flos": 3.8151401766912e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
