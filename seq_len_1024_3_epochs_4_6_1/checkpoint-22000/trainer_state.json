{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.953020134228188,
  "eval_steps": 2000,
  "global_step": 22000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006711409395973154,
      "grad_norm": 10.790489196777344,
      "learning_rate": 0.00019955257270693513,
      "loss": 7.4933,
      "step": 50
    },
    {
      "epoch": 0.013422818791946308,
      "grad_norm": 8.378059387207031,
      "learning_rate": 0.00019910514541387027,
      "loss": 6.1798,
      "step": 100
    },
    {
      "epoch": 0.020134228187919462,
      "grad_norm": 7.472206115722656,
      "learning_rate": 0.0001986577181208054,
      "loss": 5.8782,
      "step": 150
    },
    {
      "epoch": 0.026845637583892617,
      "grad_norm": 4.47011137008667,
      "learning_rate": 0.00019821029082774049,
      "loss": 5.5895,
      "step": 200
    },
    {
      "epoch": 0.03355704697986577,
      "grad_norm": 5.513430118560791,
      "learning_rate": 0.00019776286353467563,
      "loss": 5.3806,
      "step": 250
    },
    {
      "epoch": 0.040268456375838924,
      "grad_norm": 3.5382018089294434,
      "learning_rate": 0.00019731543624161075,
      "loss": 5.2315,
      "step": 300
    },
    {
      "epoch": 0.04697986577181208,
      "grad_norm": 3.360107183456421,
      "learning_rate": 0.00019686800894854587,
      "loss": 5.1051,
      "step": 350
    },
    {
      "epoch": 0.053691275167785234,
      "grad_norm": 3.428941488265991,
      "learning_rate": 0.000196420581655481,
      "loss": 5.0233,
      "step": 400
    },
    {
      "epoch": 0.06040268456375839,
      "grad_norm": 3.571742534637451,
      "learning_rate": 0.00019597315436241613,
      "loss": 4.9713,
      "step": 450
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 3.269148588180542,
      "learning_rate": 0.00019552572706935123,
      "loss": 4.9633,
      "step": 500
    },
    {
      "epoch": 0.0738255033557047,
      "grad_norm": 3.473794460296631,
      "learning_rate": 0.00019507829977628635,
      "loss": 4.8632,
      "step": 550
    },
    {
      "epoch": 0.08053691275167785,
      "grad_norm": 4.577197551727295,
      "learning_rate": 0.0001946308724832215,
      "loss": 4.8383,
      "step": 600
    },
    {
      "epoch": 0.087248322147651,
      "grad_norm": 3.5890700817108154,
      "learning_rate": 0.0001941834451901566,
      "loss": 4.8429,
      "step": 650
    },
    {
      "epoch": 0.09395973154362416,
      "grad_norm": 3.7943320274353027,
      "learning_rate": 0.00019373601789709173,
      "loss": 4.7996,
      "step": 700
    },
    {
      "epoch": 0.10067114093959731,
      "grad_norm": 3.3145439624786377,
      "learning_rate": 0.00019328859060402688,
      "loss": 4.796,
      "step": 750
    },
    {
      "epoch": 0.10738255033557047,
      "grad_norm": 3.5548324584960938,
      "learning_rate": 0.00019284116331096197,
      "loss": 4.7535,
      "step": 800
    },
    {
      "epoch": 0.11409395973154363,
      "grad_norm": 2.7853317260742188,
      "learning_rate": 0.0001923937360178971,
      "loss": 4.7199,
      "step": 850
    },
    {
      "epoch": 0.12080536912751678,
      "grad_norm": 3.2372257709503174,
      "learning_rate": 0.0001919463087248322,
      "loss": 4.7018,
      "step": 900
    },
    {
      "epoch": 0.12751677852348994,
      "grad_norm": 2.6051738262176514,
      "learning_rate": 0.00019149888143176735,
      "loss": 4.6857,
      "step": 950
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 3.034135103225708,
      "learning_rate": 0.00019105145413870247,
      "loss": 4.6841,
      "step": 1000
    },
    {
      "epoch": 0.14093959731543623,
      "grad_norm": 4.43419885635376,
      "learning_rate": 0.0001906040268456376,
      "loss": 4.6356,
      "step": 1050
    },
    {
      "epoch": 0.1476510067114094,
      "grad_norm": 3.0248541831970215,
      "learning_rate": 0.0001901565995525727,
      "loss": 4.6134,
      "step": 1100
    },
    {
      "epoch": 0.15436241610738255,
      "grad_norm": 3.251465320587158,
      "learning_rate": 0.00018970917225950783,
      "loss": 4.5901,
      "step": 1150
    },
    {
      "epoch": 0.1610738255033557,
      "grad_norm": 2.927469253540039,
      "learning_rate": 0.00018926174496644295,
      "loss": 4.5563,
      "step": 1200
    },
    {
      "epoch": 0.16778523489932887,
      "grad_norm": 3.0689961910247803,
      "learning_rate": 0.0001888143176733781,
      "loss": 4.5613,
      "step": 1250
    },
    {
      "epoch": 0.174496644295302,
      "grad_norm": 3.1747705936431885,
      "learning_rate": 0.0001883668903803132,
      "loss": 4.5068,
      "step": 1300
    },
    {
      "epoch": 0.18120805369127516,
      "grad_norm": 2.507188558578491,
      "learning_rate": 0.00018791946308724833,
      "loss": 4.5451,
      "step": 1350
    },
    {
      "epoch": 0.18791946308724833,
      "grad_norm": 2.8694608211517334,
      "learning_rate": 0.00018747203579418348,
      "loss": 4.4815,
      "step": 1400
    },
    {
      "epoch": 0.19463087248322147,
      "grad_norm": 2.7909154891967773,
      "learning_rate": 0.00018702460850111857,
      "loss": 4.4981,
      "step": 1450
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 2.4281070232391357,
      "learning_rate": 0.0001865771812080537,
      "loss": 4.4654,
      "step": 1500
    },
    {
      "epoch": 0.2080536912751678,
      "grad_norm": 3.202984571456909,
      "learning_rate": 0.0001861297539149888,
      "loss": 4.4372,
      "step": 1550
    },
    {
      "epoch": 0.21476510067114093,
      "grad_norm": 2.8308982849121094,
      "learning_rate": 0.00018568232662192395,
      "loss": 4.4179,
      "step": 1600
    },
    {
      "epoch": 0.2214765100671141,
      "grad_norm": 3.8489437103271484,
      "learning_rate": 0.00018523489932885907,
      "loss": 4.4521,
      "step": 1650
    },
    {
      "epoch": 0.22818791946308725,
      "grad_norm": 3.314119338989258,
      "learning_rate": 0.0001847874720357942,
      "loss": 4.4137,
      "step": 1700
    },
    {
      "epoch": 0.2348993288590604,
      "grad_norm": 2.8175625801086426,
      "learning_rate": 0.0001843400447427293,
      "loss": 4.3651,
      "step": 1750
    },
    {
      "epoch": 0.24161073825503357,
      "grad_norm": 2.4016165733337402,
      "learning_rate": 0.00018389261744966443,
      "loss": 4.386,
      "step": 1800
    },
    {
      "epoch": 0.2483221476510067,
      "grad_norm": 2.6427876949310303,
      "learning_rate": 0.00018344519015659955,
      "loss": 4.3652,
      "step": 1850
    },
    {
      "epoch": 0.2550335570469799,
      "grad_norm": 2.370260715484619,
      "learning_rate": 0.0001829977628635347,
      "loss": 4.3351,
      "step": 1900
    },
    {
      "epoch": 0.26174496644295303,
      "grad_norm": 2.8850646018981934,
      "learning_rate": 0.00018255033557046981,
      "loss": 4.3631,
      "step": 1950
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 3.290013313293457,
      "learning_rate": 0.00018210290827740493,
      "loss": 4.3041,
      "step": 2000
    },
    {
      "epoch": 0.2684563758389262,
      "eval_loss": 4.340182781219482,
      "eval_runtime": 162.5609,
      "eval_samples_per_second": 45.485,
      "eval_steps_per_second": 5.69,
      "step": 2000
    },
    {
      "epoch": 0.2751677852348993,
      "grad_norm": 2.546400547027588,
      "learning_rate": 0.00018165548098434005,
      "loss": 4.3004,
      "step": 2050
    },
    {
      "epoch": 0.28187919463087246,
      "grad_norm": 2.7902262210845947,
      "learning_rate": 0.00018120805369127517,
      "loss": 4.2362,
      "step": 2100
    },
    {
      "epoch": 0.28859060402684567,
      "grad_norm": 2.3196280002593994,
      "learning_rate": 0.0001807606263982103,
      "loss": 4.2644,
      "step": 2150
    },
    {
      "epoch": 0.2953020134228188,
      "grad_norm": 2.56771183013916,
      "learning_rate": 0.0001803131991051454,
      "loss": 4.2014,
      "step": 2200
    },
    {
      "epoch": 0.30201342281879195,
      "grad_norm": 2.438952922821045,
      "learning_rate": 0.00017986577181208056,
      "loss": 4.2418,
      "step": 2250
    },
    {
      "epoch": 0.3087248322147651,
      "grad_norm": 2.456835985183716,
      "learning_rate": 0.00017941834451901567,
      "loss": 4.2988,
      "step": 2300
    },
    {
      "epoch": 0.31543624161073824,
      "grad_norm": 2.4694225788116455,
      "learning_rate": 0.0001789709172259508,
      "loss": 4.2423,
      "step": 2350
    },
    {
      "epoch": 0.3221476510067114,
      "grad_norm": 2.3424112796783447,
      "learning_rate": 0.0001785234899328859,
      "loss": 4.2621,
      "step": 2400
    },
    {
      "epoch": 0.3288590604026846,
      "grad_norm": 2.9227726459503174,
      "learning_rate": 0.00017807606263982103,
      "loss": 4.2822,
      "step": 2450
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 2.609145164489746,
      "learning_rate": 0.00017762863534675615,
      "loss": 4.1947,
      "step": 2500
    },
    {
      "epoch": 0.3422818791946309,
      "grad_norm": 2.8602592945098877,
      "learning_rate": 0.0001771812080536913,
      "loss": 4.2104,
      "step": 2550
    },
    {
      "epoch": 0.348993288590604,
      "grad_norm": 2.5163161754608154,
      "learning_rate": 0.00017673378076062642,
      "loss": 4.1803,
      "step": 2600
    },
    {
      "epoch": 0.35570469798657717,
      "grad_norm": 2.9439642429351807,
      "learning_rate": 0.00017628635346756153,
      "loss": 4.2077,
      "step": 2650
    },
    {
      "epoch": 0.3624161073825503,
      "grad_norm": 2.8177683353424072,
      "learning_rate": 0.00017583892617449665,
      "loss": 4.1933,
      "step": 2700
    },
    {
      "epoch": 0.3691275167785235,
      "grad_norm": 2.2104084491729736,
      "learning_rate": 0.00017539149888143177,
      "loss": 4.2405,
      "step": 2750
    },
    {
      "epoch": 0.37583892617449666,
      "grad_norm": 2.3873324394226074,
      "learning_rate": 0.0001749440715883669,
      "loss": 4.1803,
      "step": 2800
    },
    {
      "epoch": 0.3825503355704698,
      "grad_norm": 2.3182456493377686,
      "learning_rate": 0.000174496644295302,
      "loss": 4.1144,
      "step": 2850
    },
    {
      "epoch": 0.38926174496644295,
      "grad_norm": 1.999471664428711,
      "learning_rate": 0.00017404921700223716,
      "loss": 4.2037,
      "step": 2900
    },
    {
      "epoch": 0.3959731543624161,
      "grad_norm": 2.2083094120025635,
      "learning_rate": 0.00017360178970917228,
      "loss": 4.1472,
      "step": 2950
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 2.4949729442596436,
      "learning_rate": 0.0001731543624161074,
      "loss": 4.1067,
      "step": 3000
    },
    {
      "epoch": 0.40939597315436244,
      "grad_norm": 2.041747570037842,
      "learning_rate": 0.00017270693512304251,
      "loss": 4.14,
      "step": 3050
    },
    {
      "epoch": 0.4161073825503356,
      "grad_norm": 2.006192922592163,
      "learning_rate": 0.00017225950782997763,
      "loss": 4.1524,
      "step": 3100
    },
    {
      "epoch": 0.4228187919463087,
      "grad_norm": 2.3533244132995605,
      "learning_rate": 0.00017181208053691275,
      "loss": 4.151,
      "step": 3150
    },
    {
      "epoch": 0.42953020134228187,
      "grad_norm": 2.2018721103668213,
      "learning_rate": 0.00017136465324384787,
      "loss": 4.125,
      "step": 3200
    },
    {
      "epoch": 0.436241610738255,
      "grad_norm": 2.3891186714172363,
      "learning_rate": 0.00017091722595078302,
      "loss": 4.1551,
      "step": 3250
    },
    {
      "epoch": 0.4429530201342282,
      "grad_norm": 2.1365983486175537,
      "learning_rate": 0.00017046979865771814,
      "loss": 4.1247,
      "step": 3300
    },
    {
      "epoch": 0.44966442953020136,
      "grad_norm": 3.380601644515991,
      "learning_rate": 0.00017002237136465325,
      "loss": 4.1396,
      "step": 3350
    },
    {
      "epoch": 0.4563758389261745,
      "grad_norm": 2.271970272064209,
      "learning_rate": 0.00016957494407158837,
      "loss": 4.1353,
      "step": 3400
    },
    {
      "epoch": 0.46308724832214765,
      "grad_norm": 2.328897476196289,
      "learning_rate": 0.0001691275167785235,
      "loss": 4.1206,
      "step": 3450
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 2.701540470123291,
      "learning_rate": 0.0001686800894854586,
      "loss": 4.0771,
      "step": 3500
    },
    {
      "epoch": 0.47651006711409394,
      "grad_norm": 2.3224728107452393,
      "learning_rate": 0.00016823266219239376,
      "loss": 4.1363,
      "step": 3550
    },
    {
      "epoch": 0.48322147651006714,
      "grad_norm": 2.234788656234741,
      "learning_rate": 0.00016778523489932888,
      "loss": 4.1029,
      "step": 3600
    },
    {
      "epoch": 0.4899328859060403,
      "grad_norm": 2.018167018890381,
      "learning_rate": 0.000167337807606264,
      "loss": 4.0552,
      "step": 3650
    },
    {
      "epoch": 0.4966442953020134,
      "grad_norm": 2.490947723388672,
      "learning_rate": 0.00016689038031319912,
      "loss": 4.0923,
      "step": 3700
    },
    {
      "epoch": 0.5033557046979866,
      "grad_norm": 3.020853281021118,
      "learning_rate": 0.00016644295302013423,
      "loss": 4.0856,
      "step": 3750
    },
    {
      "epoch": 0.5100671140939598,
      "grad_norm": 2.341273307800293,
      "learning_rate": 0.00016599552572706935,
      "loss": 4.0556,
      "step": 3800
    },
    {
      "epoch": 0.5167785234899329,
      "grad_norm": 2.249079942703247,
      "learning_rate": 0.00016554809843400447,
      "loss": 4.0154,
      "step": 3850
    },
    {
      "epoch": 0.5234899328859061,
      "grad_norm": 2.3283233642578125,
      "learning_rate": 0.00016510067114093962,
      "loss": 4.0575,
      "step": 3900
    },
    {
      "epoch": 0.5302013422818792,
      "grad_norm": 2.081382989883423,
      "learning_rate": 0.00016465324384787474,
      "loss": 4.0373,
      "step": 3950
    },
    {
      "epoch": 0.5369127516778524,
      "grad_norm": 2.3708956241607666,
      "learning_rate": 0.00016420581655480986,
      "loss": 4.0685,
      "step": 4000
    },
    {
      "epoch": 0.5369127516778524,
      "eval_loss": 4.093254566192627,
      "eval_runtime": 162.4249,
      "eval_samples_per_second": 45.523,
      "eval_steps_per_second": 5.695,
      "step": 4000
    },
    {
      "epoch": 0.5436241610738255,
      "grad_norm": 2.6542136669158936,
      "learning_rate": 0.00016375838926174498,
      "loss": 4.102,
      "step": 4050
    },
    {
      "epoch": 0.5503355704697986,
      "grad_norm": 2.4046437740325928,
      "learning_rate": 0.0001633109619686801,
      "loss": 4.0461,
      "step": 4100
    },
    {
      "epoch": 0.5570469798657718,
      "grad_norm": 2.031710624694824,
      "learning_rate": 0.0001628635346756152,
      "loss": 4.0279,
      "step": 4150
    },
    {
      "epoch": 0.5637583892617449,
      "grad_norm": 2.17460298538208,
      "learning_rate": 0.00016241610738255036,
      "loss": 4.1368,
      "step": 4200
    },
    {
      "epoch": 0.5704697986577181,
      "grad_norm": 2.3074185848236084,
      "learning_rate": 0.00016196868008948548,
      "loss": 4.0357,
      "step": 4250
    },
    {
      "epoch": 0.5771812080536913,
      "grad_norm": 3.606067419052124,
      "learning_rate": 0.0001615212527964206,
      "loss": 4.0731,
      "step": 4300
    },
    {
      "epoch": 0.5838926174496645,
      "grad_norm": 2.155182123184204,
      "learning_rate": 0.0001610738255033557,
      "loss": 3.9928,
      "step": 4350
    },
    {
      "epoch": 0.5906040268456376,
      "grad_norm": 2.0736520290374756,
      "learning_rate": 0.00016062639821029084,
      "loss": 4.0568,
      "step": 4400
    },
    {
      "epoch": 0.5973154362416108,
      "grad_norm": 2.227292060852051,
      "learning_rate": 0.00016017897091722595,
      "loss": 4.0244,
      "step": 4450
    },
    {
      "epoch": 0.6040268456375839,
      "grad_norm": 2.3559179306030273,
      "learning_rate": 0.00015973154362416107,
      "loss": 3.9957,
      "step": 4500
    },
    {
      "epoch": 0.610738255033557,
      "grad_norm": 1.8822554349899292,
      "learning_rate": 0.00015928411633109622,
      "loss": 4.0034,
      "step": 4550
    },
    {
      "epoch": 0.6174496644295302,
      "grad_norm": 2.22510027885437,
      "learning_rate": 0.00015883668903803134,
      "loss": 4.0264,
      "step": 4600
    },
    {
      "epoch": 0.6241610738255033,
      "grad_norm": 2.1918015480041504,
      "learning_rate": 0.00015838926174496643,
      "loss": 4.0695,
      "step": 4650
    },
    {
      "epoch": 0.6308724832214765,
      "grad_norm": 2.6183977127075195,
      "learning_rate": 0.00015794183445190158,
      "loss": 4.0423,
      "step": 4700
    },
    {
      "epoch": 0.6375838926174496,
      "grad_norm": 1.954463243484497,
      "learning_rate": 0.0001574944071588367,
      "loss": 4.0223,
      "step": 4750
    },
    {
      "epoch": 0.6442953020134228,
      "grad_norm": 2.0809178352355957,
      "learning_rate": 0.00015704697986577181,
      "loss": 4.0583,
      "step": 4800
    },
    {
      "epoch": 0.6510067114093959,
      "grad_norm": 2.3275105953216553,
      "learning_rate": 0.00015659955257270696,
      "loss": 4.0022,
      "step": 4850
    },
    {
      "epoch": 0.6577181208053692,
      "grad_norm": 2.1101841926574707,
      "learning_rate": 0.00015615212527964208,
      "loss": 4.0344,
      "step": 4900
    },
    {
      "epoch": 0.6644295302013423,
      "grad_norm": 2.25433349609375,
      "learning_rate": 0.0001557046979865772,
      "loss": 3.9917,
      "step": 4950
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 1.895716905593872,
      "learning_rate": 0.0001552572706935123,
      "loss": 3.997,
      "step": 5000
    },
    {
      "epoch": 0.6778523489932886,
      "grad_norm": 2.0908238887786865,
      "learning_rate": 0.00015480984340044744,
      "loss": 4.034,
      "step": 5050
    },
    {
      "epoch": 0.6845637583892618,
      "grad_norm": 2.230741500854492,
      "learning_rate": 0.00015436241610738256,
      "loss": 4.0349,
      "step": 5100
    },
    {
      "epoch": 0.6912751677852349,
      "grad_norm": 1.9905948638916016,
      "learning_rate": 0.00015391498881431768,
      "loss": 4.0222,
      "step": 5150
    },
    {
      "epoch": 0.697986577181208,
      "grad_norm": 2.1055192947387695,
      "learning_rate": 0.00015346756152125282,
      "loss": 4.0062,
      "step": 5200
    },
    {
      "epoch": 0.7046979865771812,
      "grad_norm": 2.117342948913574,
      "learning_rate": 0.00015302013422818794,
      "loss": 3.9944,
      "step": 5250
    },
    {
      "epoch": 0.7114093959731543,
      "grad_norm": 1.8675481081008911,
      "learning_rate": 0.00015257270693512303,
      "loss": 4.0592,
      "step": 5300
    },
    {
      "epoch": 0.7181208053691275,
      "grad_norm": 1.947300910949707,
      "learning_rate": 0.00015212527964205818,
      "loss": 3.9503,
      "step": 5350
    },
    {
      "epoch": 0.7248322147651006,
      "grad_norm": 1.8780542612075806,
      "learning_rate": 0.0001516778523489933,
      "loss": 4.0422,
      "step": 5400
    },
    {
      "epoch": 0.7315436241610739,
      "grad_norm": 2.1013174057006836,
      "learning_rate": 0.00015123042505592842,
      "loss": 3.9645,
      "step": 5450
    },
    {
      "epoch": 0.738255033557047,
      "grad_norm": 1.8844223022460938,
      "learning_rate": 0.00015078299776286354,
      "loss": 4.0321,
      "step": 5500
    },
    {
      "epoch": 0.7449664429530202,
      "grad_norm": 1.8203758001327515,
      "learning_rate": 0.00015033557046979868,
      "loss": 4.0199,
      "step": 5550
    },
    {
      "epoch": 0.7516778523489933,
      "grad_norm": 2.0477988719940186,
      "learning_rate": 0.00014988814317673377,
      "loss": 3.9696,
      "step": 5600
    },
    {
      "epoch": 0.7583892617449665,
      "grad_norm": 2.198514461517334,
      "learning_rate": 0.0001494407158836689,
      "loss": 3.9802,
      "step": 5650
    },
    {
      "epoch": 0.7651006711409396,
      "grad_norm": 2.373324155807495,
      "learning_rate": 0.00014899328859060404,
      "loss": 3.9704,
      "step": 5700
    },
    {
      "epoch": 0.7718120805369127,
      "grad_norm": 1.9061485528945923,
      "learning_rate": 0.00014854586129753916,
      "loss": 3.9764,
      "step": 5750
    },
    {
      "epoch": 0.7785234899328859,
      "grad_norm": 2.4947032928466797,
      "learning_rate": 0.00014809843400447428,
      "loss": 3.9643,
      "step": 5800
    },
    {
      "epoch": 0.785234899328859,
      "grad_norm": 1.7593334913253784,
      "learning_rate": 0.00014765100671140942,
      "loss": 3.938,
      "step": 5850
    },
    {
      "epoch": 0.7919463087248322,
      "grad_norm": 2.0924270153045654,
      "learning_rate": 0.00014720357941834454,
      "loss": 3.9889,
      "step": 5900
    },
    {
      "epoch": 0.7986577181208053,
      "grad_norm": 1.7368900775909424,
      "learning_rate": 0.00014675615212527963,
      "loss": 3.984,
      "step": 5950
    },
    {
      "epoch": 0.8053691275167785,
      "grad_norm": 2.297349214553833,
      "learning_rate": 0.00014630872483221478,
      "loss": 3.996,
      "step": 6000
    },
    {
      "epoch": 0.8053691275167785,
      "eval_loss": 3.9835445880889893,
      "eval_runtime": 162.5825,
      "eval_samples_per_second": 45.478,
      "eval_steps_per_second": 5.689,
      "step": 6000
    },
    {
      "epoch": 0.8120805369127517,
      "grad_norm": 1.9397445917129517,
      "learning_rate": 0.0001458612975391499,
      "loss": 3.9147,
      "step": 6050
    },
    {
      "epoch": 0.8187919463087249,
      "grad_norm": 2.169581651687622,
      "learning_rate": 0.00014541387024608502,
      "loss": 3.9704,
      "step": 6100
    },
    {
      "epoch": 0.825503355704698,
      "grad_norm": 2.4411067962646484,
      "learning_rate": 0.00014496644295302014,
      "loss": 3.9452,
      "step": 6150
    },
    {
      "epoch": 0.8322147651006712,
      "grad_norm": 1.904971718788147,
      "learning_rate": 0.00014451901565995528,
      "loss": 3.9115,
      "step": 6200
    },
    {
      "epoch": 0.8389261744966443,
      "grad_norm": 1.962564468383789,
      "learning_rate": 0.00014407158836689037,
      "loss": 3.9439,
      "step": 6250
    },
    {
      "epoch": 0.8456375838926175,
      "grad_norm": 1.9230397939682007,
      "learning_rate": 0.0001436241610738255,
      "loss": 3.94,
      "step": 6300
    },
    {
      "epoch": 0.8523489932885906,
      "grad_norm": 1.9584341049194336,
      "learning_rate": 0.00014317673378076064,
      "loss": 3.971,
      "step": 6350
    },
    {
      "epoch": 0.8590604026845637,
      "grad_norm": 1.9793425798416138,
      "learning_rate": 0.00014272930648769576,
      "loss": 3.9391,
      "step": 6400
    },
    {
      "epoch": 0.8657718120805369,
      "grad_norm": 2.034069538116455,
      "learning_rate": 0.00014228187919463088,
      "loss": 3.8817,
      "step": 6450
    },
    {
      "epoch": 0.87248322147651,
      "grad_norm": 1.8274506330490112,
      "learning_rate": 0.00014183445190156602,
      "loss": 3.9433,
      "step": 6500
    },
    {
      "epoch": 0.8791946308724832,
      "grad_norm": 1.7879055738449097,
      "learning_rate": 0.00014138702460850112,
      "loss": 3.9738,
      "step": 6550
    },
    {
      "epoch": 0.8859060402684564,
      "grad_norm": 2.0719285011291504,
      "learning_rate": 0.00014093959731543624,
      "loss": 3.8795,
      "step": 6600
    },
    {
      "epoch": 0.8926174496644296,
      "grad_norm": 1.9361950159072876,
      "learning_rate": 0.00014049217002237135,
      "loss": 3.8915,
      "step": 6650
    },
    {
      "epoch": 0.8993288590604027,
      "grad_norm": 1.8328882455825806,
      "learning_rate": 0.0001400447427293065,
      "loss": 3.9084,
      "step": 6700
    },
    {
      "epoch": 0.9060402684563759,
      "grad_norm": 2.2745120525360107,
      "learning_rate": 0.00013959731543624162,
      "loss": 3.8969,
      "step": 6750
    },
    {
      "epoch": 0.912751677852349,
      "grad_norm": 2.6110641956329346,
      "learning_rate": 0.00013914988814317674,
      "loss": 3.9377,
      "step": 6800
    },
    {
      "epoch": 0.9194630872483222,
      "grad_norm": 2.115372896194458,
      "learning_rate": 0.00013870246085011186,
      "loss": 3.9321,
      "step": 6850
    },
    {
      "epoch": 0.9261744966442953,
      "grad_norm": 1.8986639976501465,
      "learning_rate": 0.00013825503355704698,
      "loss": 3.9146,
      "step": 6900
    },
    {
      "epoch": 0.9328859060402684,
      "grad_norm": 1.887952208518982,
      "learning_rate": 0.0001378076062639821,
      "loss": 3.9109,
      "step": 6950
    },
    {
      "epoch": 0.9395973154362416,
      "grad_norm": 1.9198431968688965,
      "learning_rate": 0.00013736017897091724,
      "loss": 3.9526,
      "step": 7000
    },
    {
      "epoch": 0.9463087248322147,
      "grad_norm": 1.9584773778915405,
      "learning_rate": 0.00013691275167785236,
      "loss": 3.8625,
      "step": 7050
    },
    {
      "epoch": 0.9530201342281879,
      "grad_norm": 1.8673774003982544,
      "learning_rate": 0.00013646532438478748,
      "loss": 3.9077,
      "step": 7100
    },
    {
      "epoch": 0.959731543624161,
      "grad_norm": 2.0675199031829834,
      "learning_rate": 0.00013601789709172263,
      "loss": 3.8698,
      "step": 7150
    },
    {
      "epoch": 0.9664429530201343,
      "grad_norm": 1.6207541227340698,
      "learning_rate": 0.00013557046979865772,
      "loss": 3.9141,
      "step": 7200
    },
    {
      "epoch": 0.9731543624161074,
      "grad_norm": 2.8088505268096924,
      "learning_rate": 0.00013512304250559284,
      "loss": 3.938,
      "step": 7250
    },
    {
      "epoch": 0.9798657718120806,
      "grad_norm": 1.8355159759521484,
      "learning_rate": 0.00013467561521252796,
      "loss": 3.8562,
      "step": 7300
    },
    {
      "epoch": 0.9865771812080537,
      "grad_norm": 1.9756077527999878,
      "learning_rate": 0.0001342281879194631,
      "loss": 3.886,
      "step": 7350
    },
    {
      "epoch": 0.9932885906040269,
      "grad_norm": 1.9274468421936035,
      "learning_rate": 0.00013378076062639822,
      "loss": 3.8929,
      "step": 7400
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.1064250469207764,
      "learning_rate": 0.00013333333333333334,
      "loss": 3.8994,
      "step": 7450
    },
    {
      "epoch": 1.0067114093959733,
      "grad_norm": 1.8564592599868774,
      "learning_rate": 0.00013288590604026846,
      "loss": 3.7252,
      "step": 7500
    },
    {
      "epoch": 1.0134228187919463,
      "grad_norm": 1.6410019397735596,
      "learning_rate": 0.00013243847874720358,
      "loss": 3.7495,
      "step": 7550
    },
    {
      "epoch": 1.0201342281879195,
      "grad_norm": 1.8422561883926392,
      "learning_rate": 0.0001319910514541387,
      "loss": 3.7363,
      "step": 7600
    },
    {
      "epoch": 1.0268456375838926,
      "grad_norm": 2.3126742839813232,
      "learning_rate": 0.00013154362416107384,
      "loss": 3.7488,
      "step": 7650
    },
    {
      "epoch": 1.0335570469798658,
      "grad_norm": 1.7981934547424316,
      "learning_rate": 0.00013109619686800896,
      "loss": 3.7191,
      "step": 7700
    },
    {
      "epoch": 1.0402684563758389,
      "grad_norm": 1.9690864086151123,
      "learning_rate": 0.00013064876957494408,
      "loss": 3.7122,
      "step": 7750
    },
    {
      "epoch": 1.0469798657718121,
      "grad_norm": 1.6626468896865845,
      "learning_rate": 0.0001302013422818792,
      "loss": 3.7722,
      "step": 7800
    },
    {
      "epoch": 1.0536912751677852,
      "grad_norm": 1.9286785125732422,
      "learning_rate": 0.00012975391498881432,
      "loss": 3.7702,
      "step": 7850
    },
    {
      "epoch": 1.0604026845637584,
      "grad_norm": 1.8496731519699097,
      "learning_rate": 0.00012930648769574944,
      "loss": 3.7674,
      "step": 7900
    },
    {
      "epoch": 1.0671140939597314,
      "grad_norm": 1.8163055181503296,
      "learning_rate": 0.00012885906040268456,
      "loss": 3.7534,
      "step": 7950
    },
    {
      "epoch": 1.0738255033557047,
      "grad_norm": 1.7587890625,
      "learning_rate": 0.0001284116331096197,
      "loss": 3.7602,
      "step": 8000
    },
    {
      "epoch": 1.0738255033557047,
      "eval_loss": 3.9196174144744873,
      "eval_runtime": 162.6088,
      "eval_samples_per_second": 45.471,
      "eval_steps_per_second": 5.688,
      "step": 8000
    },
    {
      "epoch": 1.0805369127516777,
      "grad_norm": 1.718888521194458,
      "learning_rate": 0.00012796420581655482,
      "loss": 3.7464,
      "step": 8050
    },
    {
      "epoch": 1.087248322147651,
      "grad_norm": 1.777680516242981,
      "learning_rate": 0.00012751677852348994,
      "loss": 3.7201,
      "step": 8100
    },
    {
      "epoch": 1.0939597315436242,
      "grad_norm": 1.7772389650344849,
      "learning_rate": 0.00012706935123042506,
      "loss": 3.7305,
      "step": 8150
    },
    {
      "epoch": 1.1006711409395973,
      "grad_norm": 1.6960409879684448,
      "learning_rate": 0.00012662192393736018,
      "loss": 3.7109,
      "step": 8200
    },
    {
      "epoch": 1.1073825503355705,
      "grad_norm": 1.7087211608886719,
      "learning_rate": 0.0001261744966442953,
      "loss": 3.7316,
      "step": 8250
    },
    {
      "epoch": 1.1140939597315436,
      "grad_norm": 1.9681346416473389,
      "learning_rate": 0.00012572706935123044,
      "loss": 3.7966,
      "step": 8300
    },
    {
      "epoch": 1.1208053691275168,
      "grad_norm": 1.6378687620162964,
      "learning_rate": 0.00012527964205816556,
      "loss": 3.7642,
      "step": 8350
    },
    {
      "epoch": 1.1275167785234899,
      "grad_norm": 2.2363264560699463,
      "learning_rate": 0.00012483221476510068,
      "loss": 3.7221,
      "step": 8400
    },
    {
      "epoch": 1.1342281879194631,
      "grad_norm": 1.6552705764770508,
      "learning_rate": 0.0001243847874720358,
      "loss": 3.7198,
      "step": 8450
    },
    {
      "epoch": 1.1409395973154361,
      "grad_norm": 1.8389724493026733,
      "learning_rate": 0.00012393736017897092,
      "loss": 3.7954,
      "step": 8500
    },
    {
      "epoch": 1.1476510067114094,
      "grad_norm": 1.698006272315979,
      "learning_rate": 0.00012348993288590604,
      "loss": 3.7331,
      "step": 8550
    },
    {
      "epoch": 1.1543624161073827,
      "grad_norm": 1.8203381299972534,
      "learning_rate": 0.00012304250559284116,
      "loss": 3.7365,
      "step": 8600
    },
    {
      "epoch": 1.1610738255033557,
      "grad_norm": 1.7478222846984863,
      "learning_rate": 0.0001225950782997763,
      "loss": 3.7326,
      "step": 8650
    },
    {
      "epoch": 1.167785234899329,
      "grad_norm": 2.002506971359253,
      "learning_rate": 0.00012214765100671142,
      "loss": 3.7031,
      "step": 8700
    },
    {
      "epoch": 1.174496644295302,
      "grad_norm": 1.8073633909225464,
      "learning_rate": 0.00012170022371364653,
      "loss": 3.7017,
      "step": 8750
    },
    {
      "epoch": 1.1812080536912752,
      "grad_norm": 2.2271525859832764,
      "learning_rate": 0.00012125279642058168,
      "loss": 3.7121,
      "step": 8800
    },
    {
      "epoch": 1.1879194630872483,
      "grad_norm": 1.829747200012207,
      "learning_rate": 0.0001208053691275168,
      "loss": 3.7114,
      "step": 8850
    },
    {
      "epoch": 1.1946308724832215,
      "grad_norm": 1.8219759464263916,
      "learning_rate": 0.0001203579418344519,
      "loss": 3.6834,
      "step": 8900
    },
    {
      "epoch": 1.2013422818791946,
      "grad_norm": 1.9637089967727661,
      "learning_rate": 0.00011991051454138702,
      "loss": 3.7485,
      "step": 8950
    },
    {
      "epoch": 1.2080536912751678,
      "grad_norm": 1.68866765499115,
      "learning_rate": 0.00011946308724832216,
      "loss": 3.7345,
      "step": 9000
    },
    {
      "epoch": 1.2147651006711409,
      "grad_norm": 1.9192544221878052,
      "learning_rate": 0.00011901565995525727,
      "loss": 3.6988,
      "step": 9050
    },
    {
      "epoch": 1.221476510067114,
      "grad_norm": 1.9349634647369385,
      "learning_rate": 0.00011856823266219239,
      "loss": 3.7087,
      "step": 9100
    },
    {
      "epoch": 1.2281879194630871,
      "grad_norm": 1.8326870203018188,
      "learning_rate": 0.00011812080536912754,
      "loss": 3.7328,
      "step": 9150
    },
    {
      "epoch": 1.2348993288590604,
      "grad_norm": 1.720355749130249,
      "learning_rate": 0.00011767337807606264,
      "loss": 3.6889,
      "step": 9200
    },
    {
      "epoch": 1.2416107382550337,
      "grad_norm": 1.7372373342514038,
      "learning_rate": 0.00011722595078299776,
      "loss": 3.7,
      "step": 9250
    },
    {
      "epoch": 1.2483221476510067,
      "grad_norm": 1.736511468887329,
      "learning_rate": 0.0001167785234899329,
      "loss": 3.7377,
      "step": 9300
    },
    {
      "epoch": 1.25503355704698,
      "grad_norm": 1.7328877449035645,
      "learning_rate": 0.00011633109619686801,
      "loss": 3.7335,
      "step": 9350
    },
    {
      "epoch": 1.261744966442953,
      "grad_norm": 1.8427703380584717,
      "learning_rate": 0.00011588366890380313,
      "loss": 3.7374,
      "step": 9400
    },
    {
      "epoch": 1.2684563758389262,
      "grad_norm": 1.885656476020813,
      "learning_rate": 0.00011543624161073828,
      "loss": 3.7156,
      "step": 9450
    },
    {
      "epoch": 1.2751677852348993,
      "grad_norm": 2.031139850616455,
      "learning_rate": 0.00011498881431767338,
      "loss": 3.729,
      "step": 9500
    },
    {
      "epoch": 1.2818791946308725,
      "grad_norm": 1.8556535243988037,
      "learning_rate": 0.0001145413870246085,
      "loss": 3.6989,
      "step": 9550
    },
    {
      "epoch": 1.2885906040268456,
      "grad_norm": 1.6632606983184814,
      "learning_rate": 0.00011409395973154362,
      "loss": 3.7026,
      "step": 9600
    },
    {
      "epoch": 1.2953020134228188,
      "grad_norm": 1.7672059535980225,
      "learning_rate": 0.00011364653243847875,
      "loss": 3.7165,
      "step": 9650
    },
    {
      "epoch": 1.302013422818792,
      "grad_norm": 1.7998037338256836,
      "learning_rate": 0.00011319910514541387,
      "loss": 3.6672,
      "step": 9700
    },
    {
      "epoch": 1.308724832214765,
      "grad_norm": 1.8024260997772217,
      "learning_rate": 0.00011275167785234899,
      "loss": 3.7492,
      "step": 9750
    },
    {
      "epoch": 1.3154362416107381,
      "grad_norm": 2.016787528991699,
      "learning_rate": 0.00011230425055928412,
      "loss": 3.6563,
      "step": 9800
    },
    {
      "epoch": 1.3221476510067114,
      "grad_norm": 1.7714214324951172,
      "learning_rate": 0.00011185682326621924,
      "loss": 3.727,
      "step": 9850
    },
    {
      "epoch": 1.3288590604026846,
      "grad_norm": 1.8688411712646484,
      "learning_rate": 0.00011140939597315436,
      "loss": 3.6975,
      "step": 9900
    },
    {
      "epoch": 1.3355704697986577,
      "grad_norm": 1.7632691860198975,
      "learning_rate": 0.00011096196868008951,
      "loss": 3.6857,
      "step": 9950
    },
    {
      "epoch": 1.342281879194631,
      "grad_norm": 1.5825245380401611,
      "learning_rate": 0.00011051454138702461,
      "loss": 3.6732,
      "step": 10000
    },
    {
      "epoch": 1.342281879194631,
      "eval_loss": 3.870502233505249,
      "eval_runtime": 162.7469,
      "eval_samples_per_second": 45.433,
      "eval_steps_per_second": 5.684,
      "step": 10000
    },
    {
      "epoch": 1.348993288590604,
      "grad_norm": 1.973941683769226,
      "learning_rate": 0.00011006711409395973,
      "loss": 3.6828,
      "step": 10050
    },
    {
      "epoch": 1.3557046979865772,
      "grad_norm": 1.8064035177230835,
      "learning_rate": 0.00010961968680089485,
      "loss": 3.7094,
      "step": 10100
    },
    {
      "epoch": 1.3624161073825503,
      "grad_norm": 1.8067954778671265,
      "learning_rate": 0.00010917225950782998,
      "loss": 3.6927,
      "step": 10150
    },
    {
      "epoch": 1.3691275167785235,
      "grad_norm": 1.7133145332336426,
      "learning_rate": 0.0001087248322147651,
      "loss": 3.6851,
      "step": 10200
    },
    {
      "epoch": 1.3758389261744965,
      "grad_norm": 1.6184312105178833,
      "learning_rate": 0.00010827740492170022,
      "loss": 3.7714,
      "step": 10250
    },
    {
      "epoch": 1.3825503355704698,
      "grad_norm": 2.0730485916137695,
      "learning_rate": 0.00010782997762863535,
      "loss": 3.6928,
      "step": 10300
    },
    {
      "epoch": 1.389261744966443,
      "grad_norm": 1.7251218557357788,
      "learning_rate": 0.00010738255033557047,
      "loss": 3.7445,
      "step": 10350
    },
    {
      "epoch": 1.395973154362416,
      "grad_norm": 1.840207815170288,
      "learning_rate": 0.00010693512304250559,
      "loss": 3.6749,
      "step": 10400
    },
    {
      "epoch": 1.4026845637583891,
      "grad_norm": 1.7138358354568481,
      "learning_rate": 0.00010648769574944072,
      "loss": 3.662,
      "step": 10450
    },
    {
      "epoch": 1.4093959731543624,
      "grad_norm": 1.9387127161026,
      "learning_rate": 0.00010604026845637584,
      "loss": 3.7218,
      "step": 10500
    },
    {
      "epoch": 1.4161073825503356,
      "grad_norm": 1.9715064764022827,
      "learning_rate": 0.00010559284116331096,
      "loss": 3.687,
      "step": 10550
    },
    {
      "epoch": 1.4228187919463087,
      "grad_norm": 2.122396469116211,
      "learning_rate": 0.0001051454138702461,
      "loss": 3.6728,
      "step": 10600
    },
    {
      "epoch": 1.429530201342282,
      "grad_norm": 1.6888189315795898,
      "learning_rate": 0.00010469798657718121,
      "loss": 3.6824,
      "step": 10650
    },
    {
      "epoch": 1.436241610738255,
      "grad_norm": 1.891233205795288,
      "learning_rate": 0.00010425055928411633,
      "loss": 3.6543,
      "step": 10700
    },
    {
      "epoch": 1.4429530201342282,
      "grad_norm": 1.5387130975723267,
      "learning_rate": 0.00010380313199105145,
      "loss": 3.6133,
      "step": 10750
    },
    {
      "epoch": 1.4496644295302015,
      "grad_norm": 1.8442648649215698,
      "learning_rate": 0.00010335570469798659,
      "loss": 3.6602,
      "step": 10800
    },
    {
      "epoch": 1.4563758389261745,
      "grad_norm": 1.8325936794281006,
      "learning_rate": 0.0001029082774049217,
      "loss": 3.7247,
      "step": 10850
    },
    {
      "epoch": 1.4630872483221475,
      "grad_norm": 1.7375953197479248,
      "learning_rate": 0.00010246085011185682,
      "loss": 3.7467,
      "step": 10900
    },
    {
      "epoch": 1.4697986577181208,
      "grad_norm": 1.663519024848938,
      "learning_rate": 0.00010201342281879196,
      "loss": 3.7239,
      "step": 10950
    },
    {
      "epoch": 1.476510067114094,
      "grad_norm": 1.7236326932907104,
      "learning_rate": 0.00010156599552572707,
      "loss": 3.655,
      "step": 11000
    },
    {
      "epoch": 1.483221476510067,
      "grad_norm": 1.7892532348632812,
      "learning_rate": 0.0001011185682326622,
      "loss": 3.7258,
      "step": 11050
    },
    {
      "epoch": 1.4899328859060403,
      "grad_norm": 1.9019808769226074,
      "learning_rate": 0.00010067114093959733,
      "loss": 3.7026,
      "step": 11100
    },
    {
      "epoch": 1.4966442953020134,
      "grad_norm": 1.933449149131775,
      "learning_rate": 0.00010022371364653245,
      "loss": 3.6954,
      "step": 11150
    },
    {
      "epoch": 1.5033557046979866,
      "grad_norm": 1.970953345298767,
      "learning_rate": 9.977628635346756e-05,
      "loss": 3.7412,
      "step": 11200
    },
    {
      "epoch": 1.5100671140939599,
      "grad_norm": 1.4818817377090454,
      "learning_rate": 9.93288590604027e-05,
      "loss": 3.6745,
      "step": 11250
    },
    {
      "epoch": 1.516778523489933,
      "grad_norm": 1.9372941255569458,
      "learning_rate": 9.888143176733782e-05,
      "loss": 3.6684,
      "step": 11300
    },
    {
      "epoch": 1.523489932885906,
      "grad_norm": 1.8963295221328735,
      "learning_rate": 9.843400447427293e-05,
      "loss": 3.6778,
      "step": 11350
    },
    {
      "epoch": 1.5302013422818792,
      "grad_norm": 1.7811622619628906,
      "learning_rate": 9.798657718120807e-05,
      "loss": 3.678,
      "step": 11400
    },
    {
      "epoch": 1.5369127516778525,
      "grad_norm": 1.8568273782730103,
      "learning_rate": 9.753914988814317e-05,
      "loss": 3.7113,
      "step": 11450
    },
    {
      "epoch": 1.5436241610738255,
      "grad_norm": 1.7511708736419678,
      "learning_rate": 9.70917225950783e-05,
      "loss": 3.7339,
      "step": 11500
    },
    {
      "epoch": 1.5503355704697985,
      "grad_norm": 1.5993133783340454,
      "learning_rate": 9.664429530201344e-05,
      "loss": 3.728,
      "step": 11550
    },
    {
      "epoch": 1.5570469798657718,
      "grad_norm": 1.808392882347107,
      "learning_rate": 9.619686800894854e-05,
      "loss": 3.6402,
      "step": 11600
    },
    {
      "epoch": 1.563758389261745,
      "grad_norm": 1.5712970495224,
      "learning_rate": 9.574944071588368e-05,
      "loss": 3.6582,
      "step": 11650
    },
    {
      "epoch": 1.570469798657718,
      "grad_norm": 1.6577106714248657,
      "learning_rate": 9.53020134228188e-05,
      "loss": 3.6858,
      "step": 11700
    },
    {
      "epoch": 1.5771812080536913,
      "grad_norm": 1.6533180475234985,
      "learning_rate": 9.485458612975391e-05,
      "loss": 3.7046,
      "step": 11750
    },
    {
      "epoch": 1.5838926174496644,
      "grad_norm": 1.7732304334640503,
      "learning_rate": 9.440715883668905e-05,
      "loss": 3.7131,
      "step": 11800
    },
    {
      "epoch": 1.5906040268456376,
      "grad_norm": 1.8663212060928345,
      "learning_rate": 9.395973154362417e-05,
      "loss": 3.6858,
      "step": 11850
    },
    {
      "epoch": 1.5973154362416109,
      "grad_norm": 1.673304796218872,
      "learning_rate": 9.351230425055928e-05,
      "loss": 3.6952,
      "step": 11900
    },
    {
      "epoch": 1.604026845637584,
      "grad_norm": 1.920793890953064,
      "learning_rate": 9.30648769574944e-05,
      "loss": 3.642,
      "step": 11950
    },
    {
      "epoch": 1.610738255033557,
      "grad_norm": 2.0271005630493164,
      "learning_rate": 9.261744966442954e-05,
      "loss": 3.6081,
      "step": 12000
    },
    {
      "epoch": 1.610738255033557,
      "eval_loss": 3.822552442550659,
      "eval_runtime": 162.6172,
      "eval_samples_per_second": 45.469,
      "eval_steps_per_second": 5.688,
      "step": 12000
    },
    {
      "epoch": 1.6174496644295302,
      "grad_norm": 1.6307083368301392,
      "learning_rate": 9.217002237136466e-05,
      "loss": 3.7122,
      "step": 12050
    },
    {
      "epoch": 1.6241610738255035,
      "grad_norm": 1.5601937770843506,
      "learning_rate": 9.172259507829977e-05,
      "loss": 3.6442,
      "step": 12100
    },
    {
      "epoch": 1.6308724832214765,
      "grad_norm": 1.8363960981369019,
      "learning_rate": 9.127516778523491e-05,
      "loss": 3.6617,
      "step": 12150
    },
    {
      "epoch": 1.6375838926174495,
      "grad_norm": 1.5406157970428467,
      "learning_rate": 9.082774049217003e-05,
      "loss": 3.7093,
      "step": 12200
    },
    {
      "epoch": 1.6442953020134228,
      "grad_norm": 1.7960492372512817,
      "learning_rate": 9.038031319910515e-05,
      "loss": 3.6446,
      "step": 12250
    },
    {
      "epoch": 1.651006711409396,
      "grad_norm": 1.685287594795227,
      "learning_rate": 8.993288590604028e-05,
      "loss": 3.673,
      "step": 12300
    },
    {
      "epoch": 1.6577181208053693,
      "grad_norm": 1.593563437461853,
      "learning_rate": 8.94854586129754e-05,
      "loss": 3.7044,
      "step": 12350
    },
    {
      "epoch": 1.6644295302013423,
      "grad_norm": 1.6507277488708496,
      "learning_rate": 8.903803131991052e-05,
      "loss": 3.6949,
      "step": 12400
    },
    {
      "epoch": 1.6711409395973154,
      "grad_norm": 1.7030646800994873,
      "learning_rate": 8.859060402684565e-05,
      "loss": 3.6925,
      "step": 12450
    },
    {
      "epoch": 1.6778523489932886,
      "grad_norm": 1.756118893623352,
      "learning_rate": 8.814317673378077e-05,
      "loss": 3.7414,
      "step": 12500
    },
    {
      "epoch": 1.6845637583892619,
      "grad_norm": 1.616539478302002,
      "learning_rate": 8.769574944071589e-05,
      "loss": 3.659,
      "step": 12550
    },
    {
      "epoch": 1.691275167785235,
      "grad_norm": 1.7860844135284424,
      "learning_rate": 8.7248322147651e-05,
      "loss": 3.6469,
      "step": 12600
    },
    {
      "epoch": 1.697986577181208,
      "grad_norm": 1.8660767078399658,
      "learning_rate": 8.680089485458614e-05,
      "loss": 3.6532,
      "step": 12650
    },
    {
      "epoch": 1.7046979865771812,
      "grad_norm": 1.7671188116073608,
      "learning_rate": 8.635346756152126e-05,
      "loss": 3.6823,
      "step": 12700
    },
    {
      "epoch": 1.7114093959731544,
      "grad_norm": 1.835741400718689,
      "learning_rate": 8.590604026845638e-05,
      "loss": 3.6804,
      "step": 12750
    },
    {
      "epoch": 1.7181208053691275,
      "grad_norm": 1.5555942058563232,
      "learning_rate": 8.545861297539151e-05,
      "loss": 3.6214,
      "step": 12800
    },
    {
      "epoch": 1.7248322147651005,
      "grad_norm": 1.5993740558624268,
      "learning_rate": 8.501118568232663e-05,
      "loss": 3.65,
      "step": 12850
    },
    {
      "epoch": 1.7315436241610738,
      "grad_norm": 1.7887495756149292,
      "learning_rate": 8.456375838926175e-05,
      "loss": 3.7014,
      "step": 12900
    },
    {
      "epoch": 1.738255033557047,
      "grad_norm": 1.8236172199249268,
      "learning_rate": 8.411633109619688e-05,
      "loss": 3.6489,
      "step": 12950
    },
    {
      "epoch": 1.7449664429530203,
      "grad_norm": 1.877402424812317,
      "learning_rate": 8.3668903803132e-05,
      "loss": 3.7053,
      "step": 13000
    },
    {
      "epoch": 1.7516778523489933,
      "grad_norm": 1.808396816253662,
      "learning_rate": 8.322147651006712e-05,
      "loss": 3.6533,
      "step": 13050
    },
    {
      "epoch": 1.7583892617449663,
      "grad_norm": 1.7000905275344849,
      "learning_rate": 8.277404921700224e-05,
      "loss": 3.6348,
      "step": 13100
    },
    {
      "epoch": 1.7651006711409396,
      "grad_norm": 1.7329148054122925,
      "learning_rate": 8.232662192393737e-05,
      "loss": 3.6474,
      "step": 13150
    },
    {
      "epoch": 1.7718120805369129,
      "grad_norm": 1.6307921409606934,
      "learning_rate": 8.187919463087249e-05,
      "loss": 3.6084,
      "step": 13200
    },
    {
      "epoch": 1.778523489932886,
      "grad_norm": 1.677521824836731,
      "learning_rate": 8.14317673378076e-05,
      "loss": 3.6621,
      "step": 13250
    },
    {
      "epoch": 1.785234899328859,
      "grad_norm": 1.6124249696731567,
      "learning_rate": 8.098434004474274e-05,
      "loss": 3.6939,
      "step": 13300
    },
    {
      "epoch": 1.7919463087248322,
      "grad_norm": 1.678755521774292,
      "learning_rate": 8.053691275167784e-05,
      "loss": 3.7108,
      "step": 13350
    },
    {
      "epoch": 1.7986577181208054,
      "grad_norm": 1.6039682626724243,
      "learning_rate": 8.008948545861298e-05,
      "loss": 3.6731,
      "step": 13400
    },
    {
      "epoch": 1.8053691275167785,
      "grad_norm": 1.6984403133392334,
      "learning_rate": 7.964205816554811e-05,
      "loss": 3.6498,
      "step": 13450
    },
    {
      "epoch": 1.8120805369127517,
      "grad_norm": 1.4636286497116089,
      "learning_rate": 7.919463087248322e-05,
      "loss": 3.6459,
      "step": 13500
    },
    {
      "epoch": 1.8187919463087248,
      "grad_norm": 1.7094327211380005,
      "learning_rate": 7.874720357941835e-05,
      "loss": 3.6658,
      "step": 13550
    },
    {
      "epoch": 1.825503355704698,
      "grad_norm": 1.9971048831939697,
      "learning_rate": 7.829977628635348e-05,
      "loss": 3.6355,
      "step": 13600
    },
    {
      "epoch": 1.8322147651006713,
      "grad_norm": 1.6042828559875488,
      "learning_rate": 7.78523489932886e-05,
      "loss": 3.6541,
      "step": 13650
    },
    {
      "epoch": 1.8389261744966443,
      "grad_norm": 1.7191697359085083,
      "learning_rate": 7.740492170022372e-05,
      "loss": 3.6805,
      "step": 13700
    },
    {
      "epoch": 1.8456375838926173,
      "grad_norm": 1.8752992153167725,
      "learning_rate": 7.695749440715884e-05,
      "loss": 3.7073,
      "step": 13750
    },
    {
      "epoch": 1.8523489932885906,
      "grad_norm": 1.7412216663360596,
      "learning_rate": 7.651006711409397e-05,
      "loss": 3.6344,
      "step": 13800
    },
    {
      "epoch": 1.8590604026845639,
      "grad_norm": 1.7621570825576782,
      "learning_rate": 7.606263982102909e-05,
      "loss": 3.6407,
      "step": 13850
    },
    {
      "epoch": 1.8657718120805369,
      "grad_norm": 1.540213942527771,
      "learning_rate": 7.561521252796421e-05,
      "loss": 3.6157,
      "step": 13900
    },
    {
      "epoch": 1.87248322147651,
      "grad_norm": 1.5980356931686401,
      "learning_rate": 7.516778523489934e-05,
      "loss": 3.6388,
      "step": 13950
    },
    {
      "epoch": 1.8791946308724832,
      "grad_norm": 1.6501256227493286,
      "learning_rate": 7.472035794183445e-05,
      "loss": 3.6571,
      "step": 14000
    },
    {
      "epoch": 1.8791946308724832,
      "eval_loss": 3.7856876850128174,
      "eval_runtime": 162.4502,
      "eval_samples_per_second": 45.515,
      "eval_steps_per_second": 5.694,
      "step": 14000
    },
    {
      "epoch": 1.8859060402684564,
      "grad_norm": 1.6996926069259644,
      "learning_rate": 7.427293064876958e-05,
      "loss": 3.6078,
      "step": 14050
    },
    {
      "epoch": 1.8926174496644297,
      "grad_norm": 1.689079999923706,
      "learning_rate": 7.382550335570471e-05,
      "loss": 3.6457,
      "step": 14100
    },
    {
      "epoch": 1.8993288590604027,
      "grad_norm": 1.7803497314453125,
      "learning_rate": 7.337807606263982e-05,
      "loss": 3.6222,
      "step": 14150
    },
    {
      "epoch": 1.9060402684563758,
      "grad_norm": 1.7981234788894653,
      "learning_rate": 7.293064876957495e-05,
      "loss": 3.6534,
      "step": 14200
    },
    {
      "epoch": 1.912751677852349,
      "grad_norm": 1.6933602094650269,
      "learning_rate": 7.248322147651007e-05,
      "loss": 3.654,
      "step": 14250
    },
    {
      "epoch": 1.9194630872483223,
      "grad_norm": 1.811556339263916,
      "learning_rate": 7.203579418344519e-05,
      "loss": 3.6178,
      "step": 14300
    },
    {
      "epoch": 1.9261744966442953,
      "grad_norm": 1.6319540739059448,
      "learning_rate": 7.158836689038032e-05,
      "loss": 3.6696,
      "step": 14350
    },
    {
      "epoch": 1.9328859060402683,
      "grad_norm": 1.61830472946167,
      "learning_rate": 7.114093959731544e-05,
      "loss": 3.6302,
      "step": 14400
    },
    {
      "epoch": 1.9395973154362416,
      "grad_norm": 1.8217264413833618,
      "learning_rate": 7.069351230425056e-05,
      "loss": 3.636,
      "step": 14450
    },
    {
      "epoch": 1.9463087248322148,
      "grad_norm": 1.721028208732605,
      "learning_rate": 7.024608501118568e-05,
      "loss": 3.5942,
      "step": 14500
    },
    {
      "epoch": 1.9530201342281879,
      "grad_norm": 1.6130499839782715,
      "learning_rate": 6.979865771812081e-05,
      "loss": 3.5936,
      "step": 14550
    },
    {
      "epoch": 1.959731543624161,
      "grad_norm": 1.597703456878662,
      "learning_rate": 6.935123042505593e-05,
      "loss": 3.6211,
      "step": 14600
    },
    {
      "epoch": 1.9664429530201342,
      "grad_norm": 1.7599424123764038,
      "learning_rate": 6.890380313199105e-05,
      "loss": 3.6005,
      "step": 14650
    },
    {
      "epoch": 1.9731543624161074,
      "grad_norm": 1.6251449584960938,
      "learning_rate": 6.845637583892618e-05,
      "loss": 3.6744,
      "step": 14700
    },
    {
      "epoch": 1.9798657718120807,
      "grad_norm": 1.670011281967163,
      "learning_rate": 6.800894854586131e-05,
      "loss": 3.6694,
      "step": 14750
    },
    {
      "epoch": 1.9865771812080537,
      "grad_norm": 1.7179476022720337,
      "learning_rate": 6.756152125279642e-05,
      "loss": 3.6555,
      "step": 14800
    },
    {
      "epoch": 1.9932885906040267,
      "grad_norm": 1.7108198404312134,
      "learning_rate": 6.711409395973155e-05,
      "loss": 3.6812,
      "step": 14850
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.0470712184906006,
      "learning_rate": 6.666666666666667e-05,
      "loss": 3.6148,
      "step": 14900
    },
    {
      "epoch": 2.0067114093959733,
      "grad_norm": 1.7152677774429321,
      "learning_rate": 6.621923937360179e-05,
      "loss": 3.4536,
      "step": 14950
    },
    {
      "epoch": 2.0134228187919465,
      "grad_norm": 1.5394121408462524,
      "learning_rate": 6.577181208053692e-05,
      "loss": 3.4314,
      "step": 15000
    },
    {
      "epoch": 2.0201342281879193,
      "grad_norm": 1.673532247543335,
      "learning_rate": 6.532438478747204e-05,
      "loss": 3.4127,
      "step": 15050
    },
    {
      "epoch": 2.0268456375838926,
      "grad_norm": 1.6651633977890015,
      "learning_rate": 6.487695749440716e-05,
      "loss": 3.4199,
      "step": 15100
    },
    {
      "epoch": 2.033557046979866,
      "grad_norm": 1.629820466041565,
      "learning_rate": 6.442953020134228e-05,
      "loss": 3.4982,
      "step": 15150
    },
    {
      "epoch": 2.040268456375839,
      "grad_norm": 2.960078477859497,
      "learning_rate": 6.398210290827741e-05,
      "loss": 3.4175,
      "step": 15200
    },
    {
      "epoch": 2.046979865771812,
      "grad_norm": 1.6331849098205566,
      "learning_rate": 6.353467561521253e-05,
      "loss": 3.4476,
      "step": 15250
    },
    {
      "epoch": 2.053691275167785,
      "grad_norm": 1.5889371633529663,
      "learning_rate": 6.308724832214765e-05,
      "loss": 3.3979,
      "step": 15300
    },
    {
      "epoch": 2.0604026845637584,
      "grad_norm": 1.589768886566162,
      "learning_rate": 6.263982102908278e-05,
      "loss": 3.4025,
      "step": 15350
    },
    {
      "epoch": 2.0671140939597317,
      "grad_norm": 1.6449745893478394,
      "learning_rate": 6.21923937360179e-05,
      "loss": 3.4763,
      "step": 15400
    },
    {
      "epoch": 2.073825503355705,
      "grad_norm": 1.6844086647033691,
      "learning_rate": 6.174496644295302e-05,
      "loss": 3.4511,
      "step": 15450
    },
    {
      "epoch": 2.0805369127516777,
      "grad_norm": 1.7127331495285034,
      "learning_rate": 6.129753914988815e-05,
      "loss": 3.483,
      "step": 15500
    },
    {
      "epoch": 2.087248322147651,
      "grad_norm": 1.6126748323440552,
      "learning_rate": 6.0850111856823265e-05,
      "loss": 3.5017,
      "step": 15550
    },
    {
      "epoch": 2.0939597315436242,
      "grad_norm": 1.5719211101531982,
      "learning_rate": 6.04026845637584e-05,
      "loss": 3.434,
      "step": 15600
    },
    {
      "epoch": 2.1006711409395975,
      "grad_norm": 1.5330615043640137,
      "learning_rate": 5.995525727069351e-05,
      "loss": 3.4468,
      "step": 15650
    },
    {
      "epoch": 2.1073825503355703,
      "grad_norm": 1.6528611183166504,
      "learning_rate": 5.9507829977628635e-05,
      "loss": 3.41,
      "step": 15700
    },
    {
      "epoch": 2.1140939597315436,
      "grad_norm": 1.7769107818603516,
      "learning_rate": 5.906040268456377e-05,
      "loss": 3.4692,
      "step": 15750
    },
    {
      "epoch": 2.120805369127517,
      "grad_norm": 1.5827597379684448,
      "learning_rate": 5.861297539149888e-05,
      "loss": 3.4831,
      "step": 15800
    },
    {
      "epoch": 2.12751677852349,
      "grad_norm": 1.587511658668518,
      "learning_rate": 5.8165548098434006e-05,
      "loss": 3.4688,
      "step": 15850
    },
    {
      "epoch": 2.134228187919463,
      "grad_norm": 1.6929033994674683,
      "learning_rate": 5.771812080536914e-05,
      "loss": 3.4436,
      "step": 15900
    },
    {
      "epoch": 2.140939597315436,
      "grad_norm": 1.7121696472167969,
      "learning_rate": 5.727069351230425e-05,
      "loss": 3.4663,
      "step": 15950
    },
    {
      "epoch": 2.1476510067114094,
      "grad_norm": 1.6225776672363281,
      "learning_rate": 5.6823266219239376e-05,
      "loss": 3.468,
      "step": 16000
    },
    {
      "epoch": 2.1476510067114094,
      "eval_loss": 3.788213014602661,
      "eval_runtime": 162.7311,
      "eval_samples_per_second": 45.437,
      "eval_steps_per_second": 5.684,
      "step": 16000
    },
    {
      "epoch": 2.1543624161073827,
      "grad_norm": 1.529729962348938,
      "learning_rate": 5.6375838926174495e-05,
      "loss": 3.4246,
      "step": 16050
    },
    {
      "epoch": 2.1610738255033555,
      "grad_norm": 1.7807292938232422,
      "learning_rate": 5.592841163310962e-05,
      "loss": 3.4828,
      "step": 16100
    },
    {
      "epoch": 2.1677852348993287,
      "grad_norm": 1.9697442054748535,
      "learning_rate": 5.5480984340044754e-05,
      "loss": 3.4609,
      "step": 16150
    },
    {
      "epoch": 2.174496644295302,
      "grad_norm": 1.782840371131897,
      "learning_rate": 5.5033557046979866e-05,
      "loss": 3.4763,
      "step": 16200
    },
    {
      "epoch": 2.1812080536912752,
      "grad_norm": 1.5498623847961426,
      "learning_rate": 5.458612975391499e-05,
      "loss": 3.4877,
      "step": 16250
    },
    {
      "epoch": 2.1879194630872485,
      "grad_norm": 1.5690759420394897,
      "learning_rate": 5.413870246085011e-05,
      "loss": 3.4339,
      "step": 16300
    },
    {
      "epoch": 2.1946308724832213,
      "grad_norm": 1.5514875650405884,
      "learning_rate": 5.3691275167785237e-05,
      "loss": 3.4508,
      "step": 16350
    },
    {
      "epoch": 2.2013422818791946,
      "grad_norm": 1.5700316429138184,
      "learning_rate": 5.324384787472036e-05,
      "loss": 3.4262,
      "step": 16400
    },
    {
      "epoch": 2.208053691275168,
      "grad_norm": 1.5077303647994995,
      "learning_rate": 5.279642058165548e-05,
      "loss": 3.4676,
      "step": 16450
    },
    {
      "epoch": 2.214765100671141,
      "grad_norm": 1.652395486831665,
      "learning_rate": 5.234899328859061e-05,
      "loss": 3.403,
      "step": 16500
    },
    {
      "epoch": 2.221476510067114,
      "grad_norm": 1.607506513595581,
      "learning_rate": 5.1901565995525726e-05,
      "loss": 3.4521,
      "step": 16550
    },
    {
      "epoch": 2.228187919463087,
      "grad_norm": 1.6635923385620117,
      "learning_rate": 5.145413870246085e-05,
      "loss": 3.4888,
      "step": 16600
    },
    {
      "epoch": 2.2348993288590604,
      "grad_norm": 1.6008780002593994,
      "learning_rate": 5.100671140939598e-05,
      "loss": 3.4322,
      "step": 16650
    },
    {
      "epoch": 2.2416107382550337,
      "grad_norm": 1.7262181043624878,
      "learning_rate": 5.05592841163311e-05,
      "loss": 3.4781,
      "step": 16700
    },
    {
      "epoch": 2.248322147651007,
      "grad_norm": 1.6073312759399414,
      "learning_rate": 5.011185682326622e-05,
      "loss": 3.5089,
      "step": 16750
    },
    {
      "epoch": 2.2550335570469797,
      "grad_norm": 1.5951563119888306,
      "learning_rate": 4.966442953020135e-05,
      "loss": 3.4928,
      "step": 16800
    },
    {
      "epoch": 2.261744966442953,
      "grad_norm": 1.5302022695541382,
      "learning_rate": 4.921700223713647e-05,
      "loss": 3.4224,
      "step": 16850
    },
    {
      "epoch": 2.2684563758389262,
      "grad_norm": 1.6186580657958984,
      "learning_rate": 4.8769574944071586e-05,
      "loss": 3.4583,
      "step": 16900
    },
    {
      "epoch": 2.2751677852348995,
      "grad_norm": 1.6184650659561157,
      "learning_rate": 4.832214765100672e-05,
      "loss": 3.4228,
      "step": 16950
    },
    {
      "epoch": 2.2818791946308723,
      "grad_norm": 1.575856328010559,
      "learning_rate": 4.787472035794184e-05,
      "loss": 3.4507,
      "step": 17000
    },
    {
      "epoch": 2.2885906040268456,
      "grad_norm": 1.6886470317840576,
      "learning_rate": 4.742729306487696e-05,
      "loss": 3.4326,
      "step": 17050
    },
    {
      "epoch": 2.295302013422819,
      "grad_norm": 1.8477067947387695,
      "learning_rate": 4.697986577181208e-05,
      "loss": 3.4302,
      "step": 17100
    },
    {
      "epoch": 2.302013422818792,
      "grad_norm": 1.520548939704895,
      "learning_rate": 4.65324384787472e-05,
      "loss": 3.4222,
      "step": 17150
    },
    {
      "epoch": 2.3087248322147653,
      "grad_norm": 1.6176061630249023,
      "learning_rate": 4.608501118568233e-05,
      "loss": 3.5113,
      "step": 17200
    },
    {
      "epoch": 2.315436241610738,
      "grad_norm": 1.6444754600524902,
      "learning_rate": 4.5637583892617453e-05,
      "loss": 3.4433,
      "step": 17250
    },
    {
      "epoch": 2.3221476510067114,
      "grad_norm": 1.6425626277923584,
      "learning_rate": 4.519015659955257e-05,
      "loss": 3.4661,
      "step": 17300
    },
    {
      "epoch": 2.3288590604026846,
      "grad_norm": 1.8422555923461914,
      "learning_rate": 4.47427293064877e-05,
      "loss": 3.4834,
      "step": 17350
    },
    {
      "epoch": 2.335570469798658,
      "grad_norm": 1.553962230682373,
      "learning_rate": 4.4295302013422824e-05,
      "loss": 3.4972,
      "step": 17400
    },
    {
      "epoch": 2.3422818791946307,
      "grad_norm": 1.6390997171401978,
      "learning_rate": 4.384787472035794e-05,
      "loss": 3.4109,
      "step": 17450
    },
    {
      "epoch": 2.348993288590604,
      "grad_norm": 1.7110148668289185,
      "learning_rate": 4.340044742729307e-05,
      "loss": 3.4265,
      "step": 17500
    },
    {
      "epoch": 2.3557046979865772,
      "grad_norm": 1.5789573192596436,
      "learning_rate": 4.295302013422819e-05,
      "loss": 3.4387,
      "step": 17550
    },
    {
      "epoch": 2.3624161073825505,
      "grad_norm": 1.6281596422195435,
      "learning_rate": 4.2505592841163314e-05,
      "loss": 3.5434,
      "step": 17600
    },
    {
      "epoch": 2.3691275167785237,
      "grad_norm": 1.5776898860931396,
      "learning_rate": 4.205816554809844e-05,
      "loss": 3.4649,
      "step": 17650
    },
    {
      "epoch": 2.3758389261744965,
      "grad_norm": 1.605911374092102,
      "learning_rate": 4.161073825503356e-05,
      "loss": 3.4117,
      "step": 17700
    },
    {
      "epoch": 2.38255033557047,
      "grad_norm": 1.5826613903045654,
      "learning_rate": 4.1163310961968684e-05,
      "loss": 3.4539,
      "step": 17750
    },
    {
      "epoch": 2.389261744966443,
      "grad_norm": 1.5055994987487793,
      "learning_rate": 4.07158836689038e-05,
      "loss": 3.4135,
      "step": 17800
    },
    {
      "epoch": 2.395973154362416,
      "grad_norm": 1.594423770904541,
      "learning_rate": 4.026845637583892e-05,
      "loss": 3.4433,
      "step": 17850
    },
    {
      "epoch": 2.402684563758389,
      "grad_norm": 1.693912148475647,
      "learning_rate": 3.9821029082774055e-05,
      "loss": 3.4438,
      "step": 17900
    },
    {
      "epoch": 2.4093959731543624,
      "grad_norm": 1.5657391548156738,
      "learning_rate": 3.9373601789709174e-05,
      "loss": 3.4151,
      "step": 17950
    },
    {
      "epoch": 2.4161073825503356,
      "grad_norm": 1.5241135358810425,
      "learning_rate": 3.89261744966443e-05,
      "loss": 3.4503,
      "step": 18000
    },
    {
      "epoch": 2.4161073825503356,
      "eval_loss": 3.773228883743286,
      "eval_runtime": 162.602,
      "eval_samples_per_second": 45.473,
      "eval_steps_per_second": 5.689,
      "step": 18000
    },
    {
      "epoch": 2.422818791946309,
      "grad_norm": 1.6786565780639648,
      "learning_rate": 3.847874720357942e-05,
      "loss": 3.468,
      "step": 18050
    },
    {
      "epoch": 2.4295302013422817,
      "grad_norm": 1.6283726692199707,
      "learning_rate": 3.8031319910514545e-05,
      "loss": 3.397,
      "step": 18100
    },
    {
      "epoch": 2.436241610738255,
      "grad_norm": 1.5895943641662598,
      "learning_rate": 3.758389261744967e-05,
      "loss": 3.4047,
      "step": 18150
    },
    {
      "epoch": 2.442953020134228,
      "grad_norm": 1.5851837396621704,
      "learning_rate": 3.713646532438479e-05,
      "loss": 3.4549,
      "step": 18200
    },
    {
      "epoch": 2.4496644295302015,
      "grad_norm": 1.6233174800872803,
      "learning_rate": 3.668903803131991e-05,
      "loss": 3.4419,
      "step": 18250
    },
    {
      "epoch": 2.4563758389261743,
      "grad_norm": 1.6169599294662476,
      "learning_rate": 3.6241610738255034e-05,
      "loss": 3.474,
      "step": 18300
    },
    {
      "epoch": 2.4630872483221475,
      "grad_norm": 1.7274724245071411,
      "learning_rate": 3.579418344519016e-05,
      "loss": 3.4309,
      "step": 18350
    },
    {
      "epoch": 2.469798657718121,
      "grad_norm": 1.6607664823532104,
      "learning_rate": 3.534675615212528e-05,
      "loss": 3.4676,
      "step": 18400
    },
    {
      "epoch": 2.476510067114094,
      "grad_norm": 1.514392375946045,
      "learning_rate": 3.4899328859060405e-05,
      "loss": 3.435,
      "step": 18450
    },
    {
      "epoch": 2.4832214765100673,
      "grad_norm": 1.5482730865478516,
      "learning_rate": 3.4451901565995524e-05,
      "loss": 3.485,
      "step": 18500
    },
    {
      "epoch": 2.48993288590604,
      "grad_norm": 1.5969749689102173,
      "learning_rate": 3.4004474272930656e-05,
      "loss": 3.4709,
      "step": 18550
    },
    {
      "epoch": 2.4966442953020134,
      "grad_norm": 1.502331018447876,
      "learning_rate": 3.3557046979865775e-05,
      "loss": 3.4425,
      "step": 18600
    },
    {
      "epoch": 2.5033557046979866,
      "grad_norm": 1.5289604663848877,
      "learning_rate": 3.3109619686800894e-05,
      "loss": 3.4591,
      "step": 18650
    },
    {
      "epoch": 2.51006711409396,
      "grad_norm": 1.5513089895248413,
      "learning_rate": 3.266219239373602e-05,
      "loss": 3.4585,
      "step": 18700
    },
    {
      "epoch": 2.5167785234899327,
      "grad_norm": 1.5721323490142822,
      "learning_rate": 3.221476510067114e-05,
      "loss": 3.4646,
      "step": 18750
    },
    {
      "epoch": 2.523489932885906,
      "grad_norm": 1.5612685680389404,
      "learning_rate": 3.1767337807606265e-05,
      "loss": 3.4497,
      "step": 18800
    },
    {
      "epoch": 2.530201342281879,
      "grad_norm": 1.6491388082504272,
      "learning_rate": 3.131991051454139e-05,
      "loss": 3.4237,
      "step": 18850
    },
    {
      "epoch": 2.5369127516778525,
      "grad_norm": 1.6106172800064087,
      "learning_rate": 3.087248322147651e-05,
      "loss": 3.4517,
      "step": 18900
    },
    {
      "epoch": 2.5436241610738257,
      "grad_norm": 1.6333776712417603,
      "learning_rate": 3.0425055928411632e-05,
      "loss": 3.4274,
      "step": 18950
    },
    {
      "epoch": 2.5503355704697985,
      "grad_norm": 1.4931169748306274,
      "learning_rate": 2.9977628635346755e-05,
      "loss": 3.4685,
      "step": 19000
    },
    {
      "epoch": 2.557046979865772,
      "grad_norm": 1.6241871118545532,
      "learning_rate": 2.9530201342281884e-05,
      "loss": 3.4007,
      "step": 19050
    },
    {
      "epoch": 2.563758389261745,
      "grad_norm": 1.466904640197754,
      "learning_rate": 2.9082774049217003e-05,
      "loss": 3.3871,
      "step": 19100
    },
    {
      "epoch": 2.570469798657718,
      "grad_norm": 1.596106767654419,
      "learning_rate": 2.8635346756152125e-05,
      "loss": 3.4836,
      "step": 19150
    },
    {
      "epoch": 2.577181208053691,
      "grad_norm": 1.5772963762283325,
      "learning_rate": 2.8187919463087248e-05,
      "loss": 3.4484,
      "step": 19200
    },
    {
      "epoch": 2.5838926174496644,
      "grad_norm": 1.5828404426574707,
      "learning_rate": 2.7740492170022377e-05,
      "loss": 3.4501,
      "step": 19250
    },
    {
      "epoch": 2.5906040268456376,
      "grad_norm": 1.4994250535964966,
      "learning_rate": 2.7293064876957496e-05,
      "loss": 3.4699,
      "step": 19300
    },
    {
      "epoch": 2.597315436241611,
      "grad_norm": 1.608881950378418,
      "learning_rate": 2.6845637583892618e-05,
      "loss": 3.427,
      "step": 19350
    },
    {
      "epoch": 2.604026845637584,
      "grad_norm": 1.5887093544006348,
      "learning_rate": 2.639821029082774e-05,
      "loss": 3.4844,
      "step": 19400
    },
    {
      "epoch": 2.610738255033557,
      "grad_norm": 1.6723344326019287,
      "learning_rate": 2.5950782997762863e-05,
      "loss": 3.4268,
      "step": 19450
    },
    {
      "epoch": 2.61744966442953,
      "grad_norm": 1.550860047340393,
      "learning_rate": 2.550335570469799e-05,
      "loss": 3.4517,
      "step": 19500
    },
    {
      "epoch": 2.6241610738255035,
      "grad_norm": 1.5336228609085083,
      "learning_rate": 2.505592841163311e-05,
      "loss": 3.4018,
      "step": 19550
    },
    {
      "epoch": 2.6308724832214763,
      "grad_norm": 1.7155981063842773,
      "learning_rate": 2.4608501118568234e-05,
      "loss": 3.4273,
      "step": 19600
    },
    {
      "epoch": 2.6375838926174495,
      "grad_norm": 1.744408369064331,
      "learning_rate": 2.416107382550336e-05,
      "loss": 3.4772,
      "step": 19650
    },
    {
      "epoch": 2.6442953020134228,
      "grad_norm": 1.555572748184204,
      "learning_rate": 2.371364653243848e-05,
      "loss": 3.4466,
      "step": 19700
    },
    {
      "epoch": 2.651006711409396,
      "grad_norm": 1.4938950538635254,
      "learning_rate": 2.32662192393736e-05,
      "loss": 3.4577,
      "step": 19750
    },
    {
      "epoch": 2.6577181208053693,
      "grad_norm": 1.7314934730529785,
      "learning_rate": 2.2818791946308727e-05,
      "loss": 3.4513,
      "step": 19800
    },
    {
      "epoch": 2.6644295302013425,
      "grad_norm": 1.6318755149841309,
      "learning_rate": 2.237136465324385e-05,
      "loss": 3.4468,
      "step": 19850
    },
    {
      "epoch": 2.6711409395973154,
      "grad_norm": 1.5827444791793823,
      "learning_rate": 2.192393736017897e-05,
      "loss": 3.4914,
      "step": 19900
    },
    {
      "epoch": 2.6778523489932886,
      "grad_norm": 1.6052618026733398,
      "learning_rate": 2.1476510067114094e-05,
      "loss": 3.4663,
      "step": 19950
    },
    {
      "epoch": 2.684563758389262,
      "grad_norm": 1.555431604385376,
      "learning_rate": 2.102908277404922e-05,
      "loss": 3.4472,
      "step": 20000
    },
    {
      "epoch": 2.684563758389262,
      "eval_loss": 3.7593932151794434,
      "eval_runtime": 162.4436,
      "eval_samples_per_second": 45.517,
      "eval_steps_per_second": 5.694,
      "step": 20000
    },
    {
      "epoch": 2.6912751677852347,
      "grad_norm": 1.579399585723877,
      "learning_rate": 2.0581655480984342e-05,
      "loss": 3.447,
      "step": 20050
    },
    {
      "epoch": 2.697986577181208,
      "grad_norm": 1.671927809715271,
      "learning_rate": 2.013422818791946e-05,
      "loss": 3.4362,
      "step": 20100
    },
    {
      "epoch": 2.704697986577181,
      "grad_norm": 1.696120023727417,
      "learning_rate": 1.9686800894854587e-05,
      "loss": 3.4051,
      "step": 20150
    },
    {
      "epoch": 2.7114093959731544,
      "grad_norm": 1.5659016370773315,
      "learning_rate": 1.923937360178971e-05,
      "loss": 3.4608,
      "step": 20200
    },
    {
      "epoch": 2.7181208053691277,
      "grad_norm": 1.6513797044754028,
      "learning_rate": 1.8791946308724835e-05,
      "loss": 3.4814,
      "step": 20250
    },
    {
      "epoch": 2.7248322147651005,
      "grad_norm": 1.5817255973815918,
      "learning_rate": 1.8344519015659954e-05,
      "loss": 3.4024,
      "step": 20300
    },
    {
      "epoch": 2.7315436241610738,
      "grad_norm": 1.6191551685333252,
      "learning_rate": 1.789709172259508e-05,
      "loss": 3.4089,
      "step": 20350
    },
    {
      "epoch": 2.738255033557047,
      "grad_norm": 1.5731521844863892,
      "learning_rate": 1.7449664429530202e-05,
      "loss": 3.397,
      "step": 20400
    },
    {
      "epoch": 2.7449664429530203,
      "grad_norm": 1.499017357826233,
      "learning_rate": 1.7002237136465328e-05,
      "loss": 3.4508,
      "step": 20450
    },
    {
      "epoch": 2.751677852348993,
      "grad_norm": 1.6638922691345215,
      "learning_rate": 1.6554809843400447e-05,
      "loss": 3.4265,
      "step": 20500
    },
    {
      "epoch": 2.7583892617449663,
      "grad_norm": 1.6168420314788818,
      "learning_rate": 1.610738255033557e-05,
      "loss": 3.4395,
      "step": 20550
    },
    {
      "epoch": 2.7651006711409396,
      "grad_norm": 1.6166397333145142,
      "learning_rate": 1.5659955257270695e-05,
      "loss": 3.46,
      "step": 20600
    },
    {
      "epoch": 2.771812080536913,
      "grad_norm": 1.4933935403823853,
      "learning_rate": 1.5212527964205816e-05,
      "loss": 3.4349,
      "step": 20650
    },
    {
      "epoch": 2.778523489932886,
      "grad_norm": 1.5886409282684326,
      "learning_rate": 1.4765100671140942e-05,
      "loss": 3.3948,
      "step": 20700
    },
    {
      "epoch": 2.785234899328859,
      "grad_norm": 3.101917028427124,
      "learning_rate": 1.4317673378076063e-05,
      "loss": 3.4245,
      "step": 20750
    },
    {
      "epoch": 2.791946308724832,
      "grad_norm": 1.5998289585113525,
      "learning_rate": 1.3870246085011188e-05,
      "loss": 3.4106,
      "step": 20800
    },
    {
      "epoch": 2.7986577181208054,
      "grad_norm": 1.6779838800430298,
      "learning_rate": 1.3422818791946309e-05,
      "loss": 3.4171,
      "step": 20850
    },
    {
      "epoch": 2.8053691275167782,
      "grad_norm": 1.6282153129577637,
      "learning_rate": 1.2975391498881432e-05,
      "loss": 3.4224,
      "step": 20900
    },
    {
      "epoch": 2.8120805369127515,
      "grad_norm": 1.541927456855774,
      "learning_rate": 1.2527964205816556e-05,
      "loss": 3.4039,
      "step": 20950
    },
    {
      "epoch": 2.8187919463087248,
      "grad_norm": 1.5708619356155396,
      "learning_rate": 1.208053691275168e-05,
      "loss": 3.4469,
      "step": 21000
    },
    {
      "epoch": 2.825503355704698,
      "grad_norm": 1.6038545370101929,
      "learning_rate": 1.16331096196868e-05,
      "loss": 3.468,
      "step": 21050
    },
    {
      "epoch": 2.8322147651006713,
      "grad_norm": 1.539315938949585,
      "learning_rate": 1.1185682326621925e-05,
      "loss": 3.3866,
      "step": 21100
    },
    {
      "epoch": 2.8389261744966445,
      "grad_norm": 1.518175482749939,
      "learning_rate": 1.0738255033557047e-05,
      "loss": 3.4368,
      "step": 21150
    },
    {
      "epoch": 2.8456375838926173,
      "grad_norm": 2.106003522872925,
      "learning_rate": 1.0290827740492171e-05,
      "loss": 3.4241,
      "step": 21200
    },
    {
      "epoch": 2.8523489932885906,
      "grad_norm": 1.5160446166992188,
      "learning_rate": 9.843400447427293e-06,
      "loss": 3.4731,
      "step": 21250
    },
    {
      "epoch": 2.859060402684564,
      "grad_norm": 1.65804922580719,
      "learning_rate": 9.395973154362418e-06,
      "loss": 3.4375,
      "step": 21300
    },
    {
      "epoch": 2.8657718120805367,
      "grad_norm": 1.6830915212631226,
      "learning_rate": 8.94854586129754e-06,
      "loss": 3.4878,
      "step": 21350
    },
    {
      "epoch": 2.87248322147651,
      "grad_norm": 1.6542431116104126,
      "learning_rate": 8.501118568232664e-06,
      "loss": 3.48,
      "step": 21400
    },
    {
      "epoch": 2.879194630872483,
      "grad_norm": 1.5780009031295776,
      "learning_rate": 8.053691275167785e-06,
      "loss": 3.4214,
      "step": 21450
    },
    {
      "epoch": 2.8859060402684564,
      "grad_norm": 1.6395678520202637,
      "learning_rate": 7.606263982102908e-06,
      "loss": 3.3872,
      "step": 21500
    },
    {
      "epoch": 2.8926174496644297,
      "grad_norm": 1.583823800086975,
      "learning_rate": 7.158836689038031e-06,
      "loss": 3.4613,
      "step": 21550
    },
    {
      "epoch": 2.899328859060403,
      "grad_norm": 1.4331480264663696,
      "learning_rate": 6.7114093959731546e-06,
      "loss": 3.4209,
      "step": 21600
    },
    {
      "epoch": 2.9060402684563758,
      "grad_norm": 1.6454474925994873,
      "learning_rate": 6.263982102908278e-06,
      "loss": 3.4884,
      "step": 21650
    },
    {
      "epoch": 2.912751677852349,
      "grad_norm": 1.6268033981323242,
      "learning_rate": 5.8165548098434e-06,
      "loss": 3.4139,
      "step": 21700
    },
    {
      "epoch": 2.9194630872483223,
      "grad_norm": 1.5776199102401733,
      "learning_rate": 5.3691275167785235e-06,
      "loss": 3.403,
      "step": 21750
    },
    {
      "epoch": 2.926174496644295,
      "grad_norm": 1.6831140518188477,
      "learning_rate": 4.921700223713647e-06,
      "loss": 3.427,
      "step": 21800
    },
    {
      "epoch": 2.9328859060402683,
      "grad_norm": 1.55259108543396,
      "learning_rate": 4.47427293064877e-06,
      "loss": 3.431,
      "step": 21850
    },
    {
      "epoch": 2.9395973154362416,
      "grad_norm": 1.5940346717834473,
      "learning_rate": 4.026845637583892e-06,
      "loss": 3.394,
      "step": 21900
    },
    {
      "epoch": 2.946308724832215,
      "grad_norm": 1.572155237197876,
      "learning_rate": 3.5794183445190157e-06,
      "loss": 3.3623,
      "step": 21950
    },
    {
      "epoch": 2.953020134228188,
      "grad_norm": 1.5112075805664062,
      "learning_rate": 3.131991051454139e-06,
      "loss": 3.4978,
      "step": 22000
    },
    {
      "epoch": 2.953020134228188,
      "eval_loss": 3.748018980026245,
      "eval_runtime": 162.779,
      "eval_samples_per_second": 45.424,
      "eval_steps_per_second": 5.683,
      "step": 22000
    }
  ],
  "logging_steps": 50,
  "max_steps": 22350,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 2000,
  "total_flos": 1.2088991990100787e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
