{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9306261664199755,
  "eval_steps": 5000,
  "global_step": 30000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003217710277366626,
      "grad_norm": 5.708463191986084,
      "learning_rate": 0.00019967822897226333,
      "loss": 5.648,
      "step": 50
    },
    {
      "epoch": 0.006435420554733252,
      "grad_norm": 6.245403289794922,
      "learning_rate": 0.00019935645794452668,
      "loss": 5.0192,
      "step": 100
    },
    {
      "epoch": 0.009653130832099878,
      "grad_norm": 4.880184650421143,
      "learning_rate": 0.00019903468691679,
      "loss": 4.8404,
      "step": 150
    },
    {
      "epoch": 0.012870841109466504,
      "grad_norm": 3.2033851146698,
      "learning_rate": 0.00019871291588905335,
      "loss": 4.7183,
      "step": 200
    },
    {
      "epoch": 0.01608855138683313,
      "grad_norm": 3.4466590881347656,
      "learning_rate": 0.0001983911448613167,
      "loss": 4.4931,
      "step": 250
    },
    {
      "epoch": 0.019306261664199756,
      "grad_norm": 2.6892502307891846,
      "learning_rate": 0.00019806937383358005,
      "loss": 4.4088,
      "step": 300
    },
    {
      "epoch": 0.02252397194156638,
      "grad_norm": 2.3479394912719727,
      "learning_rate": 0.00019774760280584337,
      "loss": 4.3427,
      "step": 350
    },
    {
      "epoch": 0.02574168221893301,
      "grad_norm": 2.749128818511963,
      "learning_rate": 0.00019742583177810672,
      "loss": 4.2612,
      "step": 400
    },
    {
      "epoch": 0.028959392496299634,
      "grad_norm": 2.075979709625244,
      "learning_rate": 0.00019710406075037004,
      "loss": 4.2452,
      "step": 450
    },
    {
      "epoch": 0.03217710277366626,
      "grad_norm": 2.22853422164917,
      "learning_rate": 0.00019678228972263336,
      "loss": 4.2312,
      "step": 500
    },
    {
      "epoch": 0.03539481305103288,
      "grad_norm": 2.2646589279174805,
      "learning_rate": 0.0001964605186948967,
      "loss": 4.1847,
      "step": 550
    },
    {
      "epoch": 0.03861252332839951,
      "grad_norm": 2.3271327018737793,
      "learning_rate": 0.00019613874766716006,
      "loss": 4.1753,
      "step": 600
    },
    {
      "epoch": 0.04183023360576614,
      "grad_norm": 2.0853607654571533,
      "learning_rate": 0.0001958169766394234,
      "loss": 4.1847,
      "step": 650
    },
    {
      "epoch": 0.04504794388313276,
      "grad_norm": 2.506178379058838,
      "learning_rate": 0.00019549520561168673,
      "loss": 4.1378,
      "step": 700
    },
    {
      "epoch": 0.04826565416049939,
      "grad_norm": 2.1629042625427246,
      "learning_rate": 0.00019517343458395008,
      "loss": 4.1284,
      "step": 750
    },
    {
      "epoch": 0.05148336443786602,
      "grad_norm": 2.848813772201538,
      "learning_rate": 0.0001948516635562134,
      "loss": 4.1119,
      "step": 800
    },
    {
      "epoch": 0.05470107471523264,
      "grad_norm": 1.9626644849777222,
      "learning_rate": 0.00019452989252847675,
      "loss": 4.093,
      "step": 850
    },
    {
      "epoch": 0.05791878499259927,
      "grad_norm": 1.8613470792770386,
      "learning_rate": 0.00019420812150074007,
      "loss": 4.1022,
      "step": 900
    },
    {
      "epoch": 0.06113649526996589,
      "grad_norm": 2.019782543182373,
      "learning_rate": 0.00019388635047300342,
      "loss": 4.0882,
      "step": 950
    },
    {
      "epoch": 0.06435420554733252,
      "grad_norm": 1.9964349269866943,
      "learning_rate": 0.00019356457944526677,
      "loss": 4.09,
      "step": 1000
    },
    {
      "epoch": 0.06757191582469914,
      "grad_norm": 2.5144734382629395,
      "learning_rate": 0.0001932428084175301,
      "loss": 4.0592,
      "step": 1050
    },
    {
      "epoch": 0.07078962610206577,
      "grad_norm": 1.8884730339050293,
      "learning_rate": 0.00019292103738979344,
      "loss": 4.0382,
      "step": 1100
    },
    {
      "epoch": 0.0740073363794324,
      "grad_norm": 1.7712385654449463,
      "learning_rate": 0.00019259926636205676,
      "loss": 4.0303,
      "step": 1150
    },
    {
      "epoch": 0.07722504665679902,
      "grad_norm": 1.9121333360671997,
      "learning_rate": 0.0001922774953343201,
      "loss": 4.0458,
      "step": 1200
    },
    {
      "epoch": 0.08044275693416565,
      "grad_norm": 2.1626369953155518,
      "learning_rate": 0.00019195572430658343,
      "loss": 4.0293,
      "step": 1250
    },
    {
      "epoch": 0.08366046721153228,
      "grad_norm": 1.9044933319091797,
      "learning_rate": 0.00019163395327884678,
      "loss": 4.0346,
      "step": 1300
    },
    {
      "epoch": 0.0868781774888989,
      "grad_norm": 1.9383076429367065,
      "learning_rate": 0.0001913121822511101,
      "loss": 4.0039,
      "step": 1350
    },
    {
      "epoch": 0.09009588776626552,
      "grad_norm": 1.9362597465515137,
      "learning_rate": 0.00019099041122337345,
      "loss": 4.0196,
      "step": 1400
    },
    {
      "epoch": 0.09331359804363215,
      "grad_norm": 1.686428189277649,
      "learning_rate": 0.0001906686401956368,
      "loss": 4.0125,
      "step": 1450
    },
    {
      "epoch": 0.09653130832099878,
      "grad_norm": 1.8570588827133179,
      "learning_rate": 0.00019034686916790012,
      "loss": 4.0342,
      "step": 1500
    },
    {
      "epoch": 0.09974901859836541,
      "grad_norm": 1.9268720149993896,
      "learning_rate": 0.00019002509814016347,
      "loss": 4.0024,
      "step": 1550
    },
    {
      "epoch": 0.10296672887573204,
      "grad_norm": 1.8554301261901855,
      "learning_rate": 0.0001897033271124268,
      "loss": 3.9815,
      "step": 1600
    },
    {
      "epoch": 0.10618443915309865,
      "grad_norm": 2.0899128913879395,
      "learning_rate": 0.00018938155608469014,
      "loss": 3.9819,
      "step": 1650
    },
    {
      "epoch": 0.10940214943046528,
      "grad_norm": 1.8528681993484497,
      "learning_rate": 0.00018905978505695347,
      "loss": 3.9889,
      "step": 1700
    },
    {
      "epoch": 0.1126198597078319,
      "grad_norm": 1.9715712070465088,
      "learning_rate": 0.00018873801402921682,
      "loss": 4.0032,
      "step": 1750
    },
    {
      "epoch": 0.11583756998519854,
      "grad_norm": 2.3190982341766357,
      "learning_rate": 0.00018841624300148016,
      "loss": 3.9674,
      "step": 1800
    },
    {
      "epoch": 0.11905528026256516,
      "grad_norm": 1.9937533140182495,
      "learning_rate": 0.0001880944719737435,
      "loss": 3.955,
      "step": 1850
    },
    {
      "epoch": 0.12227299053993178,
      "grad_norm": 1.937264323234558,
      "learning_rate": 0.00018777270094600684,
      "loss": 3.9695,
      "step": 1900
    },
    {
      "epoch": 0.1254907008172984,
      "grad_norm": 1.843064546585083,
      "learning_rate": 0.00018745092991827016,
      "loss": 3.9694,
      "step": 1950
    },
    {
      "epoch": 0.12870841109466505,
      "grad_norm": 2.167767286300659,
      "learning_rate": 0.0001871291588905335,
      "loss": 3.9656,
      "step": 2000
    },
    {
      "epoch": 0.13192612137203166,
      "grad_norm": 1.8990890979766846,
      "learning_rate": 0.00018680738786279683,
      "loss": 3.96,
      "step": 2050
    },
    {
      "epoch": 0.13514383164939828,
      "grad_norm": 1.612947940826416,
      "learning_rate": 0.00018648561683506018,
      "loss": 3.938,
      "step": 2100
    },
    {
      "epoch": 0.13836154192676492,
      "grad_norm": 1.8674548864364624,
      "learning_rate": 0.0001861638458073235,
      "loss": 3.9325,
      "step": 2150
    },
    {
      "epoch": 0.14157925220413153,
      "grad_norm": 1.7990913391113281,
      "learning_rate": 0.00018584207477958687,
      "loss": 3.9322,
      "step": 2200
    },
    {
      "epoch": 0.14479696248149818,
      "grad_norm": 2.036912441253662,
      "learning_rate": 0.0001855203037518502,
      "loss": 3.96,
      "step": 2250
    },
    {
      "epoch": 0.1480146727588648,
      "grad_norm": 1.8852088451385498,
      "learning_rate": 0.00018519853272411355,
      "loss": 3.9612,
      "step": 2300
    },
    {
      "epoch": 0.1512323830362314,
      "grad_norm": 1.8201227188110352,
      "learning_rate": 0.00018487676169637687,
      "loss": 3.9407,
      "step": 2350
    },
    {
      "epoch": 0.15445009331359805,
      "grad_norm": 1.7442671060562134,
      "learning_rate": 0.00018455499066864022,
      "loss": 3.9296,
      "step": 2400
    },
    {
      "epoch": 0.15766780359096466,
      "grad_norm": 1.925676703453064,
      "learning_rate": 0.00018423321964090354,
      "loss": 3.9455,
      "step": 2450
    },
    {
      "epoch": 0.1608855138683313,
      "grad_norm": 1.7125753164291382,
      "learning_rate": 0.00018391144861316686,
      "loss": 3.9154,
      "step": 2500
    },
    {
      "epoch": 0.16410322414569792,
      "grad_norm": 1.9763189554214478,
      "learning_rate": 0.0001835896775854302,
      "loss": 3.9175,
      "step": 2550
    },
    {
      "epoch": 0.16732093442306456,
      "grad_norm": 1.892127275466919,
      "learning_rate": 0.00018326790655769356,
      "loss": 3.9281,
      "step": 2600
    },
    {
      "epoch": 0.17053864470043117,
      "grad_norm": 1.8656646013259888,
      "learning_rate": 0.0001829461355299569,
      "loss": 3.9231,
      "step": 2650
    },
    {
      "epoch": 0.1737563549777978,
      "grad_norm": 2.0354743003845215,
      "learning_rate": 0.00018262436450222023,
      "loss": 3.9214,
      "step": 2700
    },
    {
      "epoch": 0.17697406525516443,
      "grad_norm": 1.8047208786010742,
      "learning_rate": 0.00018230259347448358,
      "loss": 3.9069,
      "step": 2750
    },
    {
      "epoch": 0.18019177553253105,
      "grad_norm": 1.8879380226135254,
      "learning_rate": 0.0001819808224467469,
      "loss": 3.9381,
      "step": 2800
    },
    {
      "epoch": 0.1834094858098977,
      "grad_norm": 1.7352334260940552,
      "learning_rate": 0.00018165905141901025,
      "loss": 3.9024,
      "step": 2850
    },
    {
      "epoch": 0.1866271960872643,
      "grad_norm": 2.422841787338257,
      "learning_rate": 0.00018133728039127357,
      "loss": 3.919,
      "step": 2900
    },
    {
      "epoch": 0.18984490636463092,
      "grad_norm": 1.713371992111206,
      "learning_rate": 0.00018101550936353692,
      "loss": 3.9277,
      "step": 2950
    },
    {
      "epoch": 0.19306261664199756,
      "grad_norm": 1.8674311637878418,
      "learning_rate": 0.00018069373833580027,
      "loss": 3.907,
      "step": 3000
    },
    {
      "epoch": 0.19628032691936417,
      "grad_norm": 1.7915334701538086,
      "learning_rate": 0.0001803719673080636,
      "loss": 3.925,
      "step": 3050
    },
    {
      "epoch": 0.19949803719673082,
      "grad_norm": 1.763401746749878,
      "learning_rate": 0.00018005019628032694,
      "loss": 3.9054,
      "step": 3100
    },
    {
      "epoch": 0.20271574747409743,
      "grad_norm": 1.703405499458313,
      "learning_rate": 0.00017972842525259026,
      "loss": 3.8878,
      "step": 3150
    },
    {
      "epoch": 0.20593345775146407,
      "grad_norm": 1.5636786222457886,
      "learning_rate": 0.0001794066542248536,
      "loss": 3.9117,
      "step": 3200
    },
    {
      "epoch": 0.2091511680288307,
      "grad_norm": 2.136035680770874,
      "learning_rate": 0.00017908488319711693,
      "loss": 3.8799,
      "step": 3250
    },
    {
      "epoch": 0.2123688783061973,
      "grad_norm": 1.7485504150390625,
      "learning_rate": 0.00017876311216938028,
      "loss": 3.8868,
      "step": 3300
    },
    {
      "epoch": 0.21558658858356394,
      "grad_norm": 1.657841444015503,
      "learning_rate": 0.0001784413411416436,
      "loss": 3.9092,
      "step": 3350
    },
    {
      "epoch": 0.21880429886093056,
      "grad_norm": 1.8414617776870728,
      "learning_rate": 0.00017811957011390695,
      "loss": 3.8977,
      "step": 3400
    },
    {
      "epoch": 0.2220220091382972,
      "grad_norm": 1.8982664346694946,
      "learning_rate": 0.0001777977990861703,
      "loss": 3.8969,
      "step": 3450
    },
    {
      "epoch": 0.2252397194156638,
      "grad_norm": 1.7462230920791626,
      "learning_rate": 0.00017747602805843362,
      "loss": 3.8592,
      "step": 3500
    },
    {
      "epoch": 0.22845742969303043,
      "grad_norm": 1.9613796472549438,
      "learning_rate": 0.00017715425703069697,
      "loss": 3.8993,
      "step": 3550
    },
    {
      "epoch": 0.23167513997039707,
      "grad_norm": 1.668778896331787,
      "learning_rate": 0.0001768324860029603,
      "loss": 3.908,
      "step": 3600
    },
    {
      "epoch": 0.23489285024776368,
      "grad_norm": 1.8652153015136719,
      "learning_rate": 0.00017651071497522364,
      "loss": 3.8835,
      "step": 3650
    },
    {
      "epoch": 0.23811056052513033,
      "grad_norm": 1.780437707901001,
      "learning_rate": 0.00017618894394748696,
      "loss": 3.8932,
      "step": 3700
    },
    {
      "epoch": 0.24132827080249694,
      "grad_norm": 1.7833822965621948,
      "learning_rate": 0.0001758671729197503,
      "loss": 3.8897,
      "step": 3750
    },
    {
      "epoch": 0.24454598107986356,
      "grad_norm": 1.8179335594177246,
      "learning_rate": 0.00017554540189201366,
      "loss": 3.8537,
      "step": 3800
    },
    {
      "epoch": 0.2477636913572302,
      "grad_norm": 1.55262291431427,
      "learning_rate": 0.000175223630864277,
      "loss": 3.8634,
      "step": 3850
    },
    {
      "epoch": 0.2509814016345968,
      "grad_norm": 1.6948418617248535,
      "learning_rate": 0.00017490185983654033,
      "loss": 3.8658,
      "step": 3900
    },
    {
      "epoch": 0.2541991119119634,
      "grad_norm": 1.5996447801589966,
      "learning_rate": 0.00017458008880880365,
      "loss": 3.8682,
      "step": 3950
    },
    {
      "epoch": 0.2574168221893301,
      "grad_norm": 1.7286579608917236,
      "learning_rate": 0.000174258317781067,
      "loss": 3.8701,
      "step": 4000
    },
    {
      "epoch": 0.2606345324666967,
      "grad_norm": 1.9813194274902344,
      "learning_rate": 0.00017393654675333032,
      "loss": 3.8612,
      "step": 4050
    },
    {
      "epoch": 0.2638522427440633,
      "grad_norm": 1.5580456256866455,
      "learning_rate": 0.00017361477572559367,
      "loss": 3.8762,
      "step": 4100
    },
    {
      "epoch": 0.26706995302142994,
      "grad_norm": 1.6231799125671387,
      "learning_rate": 0.00017329300469785702,
      "loss": 3.8704,
      "step": 4150
    },
    {
      "epoch": 0.27028766329879655,
      "grad_norm": 1.6238138675689697,
      "learning_rate": 0.00017297123367012037,
      "loss": 3.846,
      "step": 4200
    },
    {
      "epoch": 0.2735053735761632,
      "grad_norm": 1.6659433841705322,
      "learning_rate": 0.0001726494626423837,
      "loss": 3.8637,
      "step": 4250
    },
    {
      "epoch": 0.27672308385352984,
      "grad_norm": 1.7193301916122437,
      "learning_rate": 0.00017232769161464704,
      "loss": 3.8581,
      "step": 4300
    },
    {
      "epoch": 0.27994079413089645,
      "grad_norm": 1.5685259103775024,
      "learning_rate": 0.00017200592058691036,
      "loss": 3.8417,
      "step": 4350
    },
    {
      "epoch": 0.28315850440826307,
      "grad_norm": 1.6684436798095703,
      "learning_rate": 0.00017168414955917368,
      "loss": 3.855,
      "step": 4400
    },
    {
      "epoch": 0.2863762146856297,
      "grad_norm": 1.7436463832855225,
      "learning_rate": 0.00017136237853143703,
      "loss": 3.8491,
      "step": 4450
    },
    {
      "epoch": 0.28959392496299635,
      "grad_norm": 1.6485079526901245,
      "learning_rate": 0.00017104060750370036,
      "loss": 3.8701,
      "step": 4500
    },
    {
      "epoch": 0.29281163524036297,
      "grad_norm": 1.510318398475647,
      "learning_rate": 0.0001707188364759637,
      "loss": 3.8294,
      "step": 4550
    },
    {
      "epoch": 0.2960293455177296,
      "grad_norm": 1.6463932991027832,
      "learning_rate": 0.00017039706544822705,
      "loss": 3.8313,
      "step": 4600
    },
    {
      "epoch": 0.2992470557950962,
      "grad_norm": 1.7953572273254395,
      "learning_rate": 0.0001700752944204904,
      "loss": 3.8388,
      "step": 4650
    },
    {
      "epoch": 0.3024647660724628,
      "grad_norm": 1.804811954498291,
      "learning_rate": 0.00016975352339275372,
      "loss": 3.8453,
      "step": 4700
    },
    {
      "epoch": 0.3056824763498295,
      "grad_norm": 1.7344038486480713,
      "learning_rate": 0.00016943175236501707,
      "loss": 3.8408,
      "step": 4750
    },
    {
      "epoch": 0.3089001866271961,
      "grad_norm": 1.7990684509277344,
      "learning_rate": 0.0001691099813372804,
      "loss": 3.8409,
      "step": 4800
    },
    {
      "epoch": 0.3121178969045627,
      "grad_norm": 1.6480028629302979,
      "learning_rate": 0.00016878821030954372,
      "loss": 3.8381,
      "step": 4850
    },
    {
      "epoch": 0.3153356071819293,
      "grad_norm": 1.860586404800415,
      "learning_rate": 0.00016846643928180707,
      "loss": 3.8366,
      "step": 4900
    },
    {
      "epoch": 0.318553317459296,
      "grad_norm": 1.8061760663986206,
      "learning_rate": 0.00016814466825407041,
      "loss": 3.862,
      "step": 4950
    },
    {
      "epoch": 0.3217710277366626,
      "grad_norm": 1.528124213218689,
      "learning_rate": 0.00016782289722633376,
      "loss": 3.8102,
      "step": 5000
    },
    {
      "epoch": 0.3217710277366626,
      "eval_loss": 3.8494200706481934,
      "eval_runtime": 265.4916,
      "eval_samples_per_second": 235.533,
      "eval_steps_per_second": 14.724,
      "step": 5000
    },
    {
      "epoch": 0.3249887380140292,
      "grad_norm": 1.5655710697174072,
      "learning_rate": 0.00016750112619859709,
      "loss": 3.8108,
      "step": 5050
    },
    {
      "epoch": 0.32820644829139584,
      "grad_norm": 1.5868767499923706,
      "learning_rate": 0.00016717935517086043,
      "loss": 3.8279,
      "step": 5100
    },
    {
      "epoch": 0.33142415856876245,
      "grad_norm": 1.730608344078064,
      "learning_rate": 0.00016685758414312376,
      "loss": 3.8276,
      "step": 5150
    },
    {
      "epoch": 0.3346418688461291,
      "grad_norm": 1.5016889572143555,
      "learning_rate": 0.0001665358131153871,
      "loss": 3.8271,
      "step": 5200
    },
    {
      "epoch": 0.33785957912349573,
      "grad_norm": 1.768543004989624,
      "learning_rate": 0.00016621404208765043,
      "loss": 3.8245,
      "step": 5250
    },
    {
      "epoch": 0.34107728940086235,
      "grad_norm": 1.7170417308807373,
      "learning_rate": 0.00016589227105991378,
      "loss": 3.7988,
      "step": 5300
    },
    {
      "epoch": 0.34429499967822896,
      "grad_norm": 1.6304668188095093,
      "learning_rate": 0.00016557050003217712,
      "loss": 3.7977,
      "step": 5350
    },
    {
      "epoch": 0.3475127099555956,
      "grad_norm": 1.7737880945205688,
      "learning_rate": 0.00016524872900444045,
      "loss": 3.8053,
      "step": 5400
    },
    {
      "epoch": 0.35073042023296225,
      "grad_norm": 1.5838203430175781,
      "learning_rate": 0.0001649269579767038,
      "loss": 3.8286,
      "step": 5450
    },
    {
      "epoch": 0.35394813051032886,
      "grad_norm": 1.5951305627822876,
      "learning_rate": 0.00016460518694896712,
      "loss": 3.8375,
      "step": 5500
    },
    {
      "epoch": 0.3571658407876955,
      "grad_norm": 1.4910967350006104,
      "learning_rate": 0.00016428341592123047,
      "loss": 3.8191,
      "step": 5550
    },
    {
      "epoch": 0.3603835510650621,
      "grad_norm": 1.5719431638717651,
      "learning_rate": 0.0001639616448934938,
      "loss": 3.8174,
      "step": 5600
    },
    {
      "epoch": 0.3636012613424287,
      "grad_norm": 1.6584820747375488,
      "learning_rate": 0.00016363987386575714,
      "loss": 3.8514,
      "step": 5650
    },
    {
      "epoch": 0.3668189716197954,
      "grad_norm": 1.5471991300582886,
      "learning_rate": 0.00016331810283802046,
      "loss": 3.8152,
      "step": 5700
    },
    {
      "epoch": 0.370036681897162,
      "grad_norm": 1.71937894821167,
      "learning_rate": 0.0001629963318102838,
      "loss": 3.8221,
      "step": 5750
    },
    {
      "epoch": 0.3732543921745286,
      "grad_norm": 1.5248373746871948,
      "learning_rate": 0.00016267456078254716,
      "loss": 3.8103,
      "step": 5800
    },
    {
      "epoch": 0.3764721024518952,
      "grad_norm": 1.9865561723709106,
      "learning_rate": 0.00016235278975481048,
      "loss": 3.8061,
      "step": 5850
    },
    {
      "epoch": 0.37968981272926183,
      "grad_norm": 1.6441580057144165,
      "learning_rate": 0.00016203101872707383,
      "loss": 3.8397,
      "step": 5900
    },
    {
      "epoch": 0.3829075230066285,
      "grad_norm": 1.6129628419876099,
      "learning_rate": 0.00016170924769933715,
      "loss": 3.8239,
      "step": 5950
    },
    {
      "epoch": 0.3861252332839951,
      "grad_norm": 1.693530797958374,
      "learning_rate": 0.0001613874766716005,
      "loss": 3.8197,
      "step": 6000
    },
    {
      "epoch": 0.38934294356136173,
      "grad_norm": 1.6497831344604492,
      "learning_rate": 0.00016106570564386382,
      "loss": 3.8042,
      "step": 6050
    },
    {
      "epoch": 0.39256065383872835,
      "grad_norm": 1.543588399887085,
      "learning_rate": 0.00016074393461612717,
      "loss": 3.8043,
      "step": 6100
    },
    {
      "epoch": 0.39577836411609496,
      "grad_norm": 1.5462665557861328,
      "learning_rate": 0.00016042216358839052,
      "loss": 3.81,
      "step": 6150
    },
    {
      "epoch": 0.39899607439346163,
      "grad_norm": 1.4841879606246948,
      "learning_rate": 0.00016010039256065387,
      "loss": 3.7991,
      "step": 6200
    },
    {
      "epoch": 0.40221378467082824,
      "grad_norm": 1.612215280532837,
      "learning_rate": 0.0001597786215329172,
      "loss": 3.8027,
      "step": 6250
    },
    {
      "epoch": 0.40543149494819486,
      "grad_norm": 1.7208306789398193,
      "learning_rate": 0.0001594568505051805,
      "loss": 3.7985,
      "step": 6300
    },
    {
      "epoch": 0.4086492052255615,
      "grad_norm": 1.4115861654281616,
      "learning_rate": 0.00015913507947744386,
      "loss": 3.8018,
      "step": 6350
    },
    {
      "epoch": 0.41186691550292814,
      "grad_norm": 1.6343975067138672,
      "learning_rate": 0.00015881330844970718,
      "loss": 3.8365,
      "step": 6400
    },
    {
      "epoch": 0.41508462578029476,
      "grad_norm": 1.8393598794937134,
      "learning_rate": 0.00015849153742197053,
      "loss": 3.8122,
      "step": 6450
    },
    {
      "epoch": 0.4183023360576614,
      "grad_norm": 1.6280580759048462,
      "learning_rate": 0.00015816976639423385,
      "loss": 3.833,
      "step": 6500
    },
    {
      "epoch": 0.421520046335028,
      "grad_norm": 1.706064224243164,
      "learning_rate": 0.00015784799536649723,
      "loss": 3.8062,
      "step": 6550
    },
    {
      "epoch": 0.4247377566123946,
      "grad_norm": 1.5041412115097046,
      "learning_rate": 0.00015752622433876055,
      "loss": 3.7907,
      "step": 6600
    },
    {
      "epoch": 0.42795546688976127,
      "grad_norm": 1.695559024810791,
      "learning_rate": 0.0001572044533110239,
      "loss": 3.7955,
      "step": 6650
    },
    {
      "epoch": 0.4311731771671279,
      "grad_norm": 1.420157551765442,
      "learning_rate": 0.00015688268228328722,
      "loss": 3.8022,
      "step": 6700
    },
    {
      "epoch": 0.4343908874444945,
      "grad_norm": 1.4247632026672363,
      "learning_rate": 0.00015656091125555057,
      "loss": 3.7807,
      "step": 6750
    },
    {
      "epoch": 0.4376085977218611,
      "grad_norm": 1.6030504703521729,
      "learning_rate": 0.0001562391402278139,
      "loss": 3.8111,
      "step": 6800
    },
    {
      "epoch": 0.44082630799922773,
      "grad_norm": 1.5235905647277832,
      "learning_rate": 0.0001559173692000772,
      "loss": 3.7949,
      "step": 6850
    },
    {
      "epoch": 0.4440440182765944,
      "grad_norm": 1.5767970085144043,
      "learning_rate": 0.00015559559817234056,
      "loss": 3.8205,
      "step": 6900
    },
    {
      "epoch": 0.447261728553961,
      "grad_norm": 1.5787184238433838,
      "learning_rate": 0.0001552738271446039,
      "loss": 3.7856,
      "step": 6950
    },
    {
      "epoch": 0.4504794388313276,
      "grad_norm": 1.5897294282913208,
      "learning_rate": 0.00015495205611686726,
      "loss": 3.7828,
      "step": 7000
    },
    {
      "epoch": 0.45369714910869424,
      "grad_norm": 1.568084478378296,
      "learning_rate": 0.00015463028508913058,
      "loss": 3.7875,
      "step": 7050
    },
    {
      "epoch": 0.45691485938606086,
      "grad_norm": 1.693968653678894,
      "learning_rate": 0.00015430851406139393,
      "loss": 3.7734,
      "step": 7100
    },
    {
      "epoch": 0.4601325696634275,
      "grad_norm": 1.5015723705291748,
      "learning_rate": 0.00015398674303365725,
      "loss": 3.768,
      "step": 7150
    },
    {
      "epoch": 0.46335027994079414,
      "grad_norm": 1.435317039489746,
      "learning_rate": 0.0001536649720059206,
      "loss": 3.7828,
      "step": 7200
    },
    {
      "epoch": 0.46656799021816076,
      "grad_norm": 1.6906945705413818,
      "learning_rate": 0.00015334320097818392,
      "loss": 3.7962,
      "step": 7250
    },
    {
      "epoch": 0.46978570049552737,
      "grad_norm": 1.5245842933654785,
      "learning_rate": 0.00015302142995044727,
      "loss": 3.7661,
      "step": 7300
    },
    {
      "epoch": 0.473003410772894,
      "grad_norm": 1.802992820739746,
      "learning_rate": 0.00015269965892271062,
      "loss": 3.7905,
      "step": 7350
    },
    {
      "epoch": 0.47622112105026065,
      "grad_norm": 1.4810494184494019,
      "learning_rate": 0.00015237788789497394,
      "loss": 3.7843,
      "step": 7400
    },
    {
      "epoch": 0.47943883132762727,
      "grad_norm": 1.490763545036316,
      "learning_rate": 0.0001520561168672373,
      "loss": 3.7696,
      "step": 7450
    },
    {
      "epoch": 0.4826565416049939,
      "grad_norm": 1.6670223474502563,
      "learning_rate": 0.0001517343458395006,
      "loss": 3.7817,
      "step": 7500
    },
    {
      "epoch": 0.4858742518823605,
      "grad_norm": 1.5288344621658325,
      "learning_rate": 0.00015141257481176396,
      "loss": 3.7852,
      "step": 7550
    },
    {
      "epoch": 0.4890919621597271,
      "grad_norm": 1.526545763015747,
      "learning_rate": 0.00015109080378402728,
      "loss": 3.7832,
      "step": 7600
    },
    {
      "epoch": 0.4923096724370938,
      "grad_norm": 1.703840732574463,
      "learning_rate": 0.00015076903275629063,
      "loss": 3.7969,
      "step": 7650
    },
    {
      "epoch": 0.4955273827144604,
      "grad_norm": 1.5963698625564575,
      "learning_rate": 0.00015044726172855395,
      "loss": 3.7777,
      "step": 7700
    },
    {
      "epoch": 0.498745092991827,
      "grad_norm": 1.5538502931594849,
      "learning_rate": 0.0001501254907008173,
      "loss": 3.7718,
      "step": 7750
    },
    {
      "epoch": 0.5019628032691936,
      "grad_norm": 1.635689616203308,
      "learning_rate": 0.00014980371967308065,
      "loss": 3.7642,
      "step": 7800
    },
    {
      "epoch": 0.5051805135465602,
      "grad_norm": 1.415955901145935,
      "learning_rate": 0.00014948194864534397,
      "loss": 3.7884,
      "step": 7850
    },
    {
      "epoch": 0.5083982238239269,
      "grad_norm": 1.4858042001724243,
      "learning_rate": 0.00014916017761760732,
      "loss": 3.7673,
      "step": 7900
    },
    {
      "epoch": 0.5116159341012935,
      "grad_norm": 1.6408528089523315,
      "learning_rate": 0.00014883840658987065,
      "loss": 3.7872,
      "step": 7950
    },
    {
      "epoch": 0.5148336443786602,
      "grad_norm": 1.5458942651748657,
      "learning_rate": 0.000148516635562134,
      "loss": 3.7739,
      "step": 8000
    },
    {
      "epoch": 0.5180513546560268,
      "grad_norm": 1.5851566791534424,
      "learning_rate": 0.00014819486453439732,
      "loss": 3.771,
      "step": 8050
    },
    {
      "epoch": 0.5212690649333934,
      "grad_norm": 1.5428862571716309,
      "learning_rate": 0.00014787309350666066,
      "loss": 3.768,
      "step": 8100
    },
    {
      "epoch": 0.52448677521076,
      "grad_norm": 1.609789490699768,
      "learning_rate": 0.00014755132247892401,
      "loss": 3.7607,
      "step": 8150
    },
    {
      "epoch": 0.5277044854881267,
      "grad_norm": 1.5445842742919922,
      "learning_rate": 0.00014722955145118736,
      "loss": 3.7918,
      "step": 8200
    },
    {
      "epoch": 0.5309221957654933,
      "grad_norm": 1.5337051153182983,
      "learning_rate": 0.00014690778042345068,
      "loss": 3.7711,
      "step": 8250
    },
    {
      "epoch": 0.5341399060428599,
      "grad_norm": 1.5095340013504028,
      "learning_rate": 0.000146586009395714,
      "loss": 3.7662,
      "step": 8300
    },
    {
      "epoch": 0.5373576163202265,
      "grad_norm": 1.4930877685546875,
      "learning_rate": 0.00014626423836797736,
      "loss": 3.7603,
      "step": 8350
    },
    {
      "epoch": 0.5405753265975931,
      "grad_norm": 1.5415124893188477,
      "learning_rate": 0.00014594246734024068,
      "loss": 3.7737,
      "step": 8400
    },
    {
      "epoch": 0.5437930368749597,
      "grad_norm": 1.3952467441558838,
      "learning_rate": 0.00014562069631250403,
      "loss": 3.7496,
      "step": 8450
    },
    {
      "epoch": 0.5470107471523264,
      "grad_norm": 1.4307715892791748,
      "learning_rate": 0.00014529892528476738,
      "loss": 3.7555,
      "step": 8500
    },
    {
      "epoch": 0.5502284574296931,
      "grad_norm": 1.5512821674346924,
      "learning_rate": 0.00014497715425703072,
      "loss": 3.7546,
      "step": 8550
    },
    {
      "epoch": 0.5534461677070597,
      "grad_norm": 1.4085383415222168,
      "learning_rate": 0.00014465538322929405,
      "loss": 3.7598,
      "step": 8600
    },
    {
      "epoch": 0.5566638779844263,
      "grad_norm": 1.5264105796813965,
      "learning_rate": 0.0001443336122015574,
      "loss": 3.7888,
      "step": 8650
    },
    {
      "epoch": 0.5598815882617929,
      "grad_norm": 1.6788958311080933,
      "learning_rate": 0.00014401184117382072,
      "loss": 3.7547,
      "step": 8700
    },
    {
      "epoch": 0.5630992985391595,
      "grad_norm": 1.5077401399612427,
      "learning_rate": 0.00014369007014608404,
      "loss": 3.76,
      "step": 8750
    },
    {
      "epoch": 0.5663170088165261,
      "grad_norm": 1.4126062393188477,
      "learning_rate": 0.0001433682991183474,
      "loss": 3.7389,
      "step": 8800
    },
    {
      "epoch": 0.5695347190938927,
      "grad_norm": 1.4041647911071777,
      "learning_rate": 0.0001430465280906107,
      "loss": 3.7823,
      "step": 8850
    },
    {
      "epoch": 0.5727524293712594,
      "grad_norm": 1.487444519996643,
      "learning_rate": 0.00014272475706287406,
      "loss": 3.7539,
      "step": 8900
    },
    {
      "epoch": 0.5759701396486261,
      "grad_norm": 1.514260172843933,
      "learning_rate": 0.0001424029860351374,
      "loss": 3.7738,
      "step": 8950
    },
    {
      "epoch": 0.5791878499259927,
      "grad_norm": 1.5020619630813599,
      "learning_rate": 0.00014208121500740076,
      "loss": 3.7603,
      "step": 9000
    },
    {
      "epoch": 0.5824055602033593,
      "grad_norm": 1.4551719427108765,
      "learning_rate": 0.00014175944397966408,
      "loss": 3.7647,
      "step": 9050
    },
    {
      "epoch": 0.5856232704807259,
      "grad_norm": 1.5674893856048584,
      "learning_rate": 0.00014143767295192743,
      "loss": 3.7466,
      "step": 9100
    },
    {
      "epoch": 0.5888409807580925,
      "grad_norm": 1.5863664150238037,
      "learning_rate": 0.00014111590192419075,
      "loss": 3.7427,
      "step": 9150
    },
    {
      "epoch": 0.5920586910354592,
      "grad_norm": 1.4123094081878662,
      "learning_rate": 0.00014079413089645407,
      "loss": 3.7221,
      "step": 9200
    },
    {
      "epoch": 0.5952764013128258,
      "grad_norm": 1.5162683725357056,
      "learning_rate": 0.00014047235986871742,
      "loss": 3.7688,
      "step": 9250
    },
    {
      "epoch": 0.5984941115901924,
      "grad_norm": 1.4088983535766602,
      "learning_rate": 0.00014015058884098077,
      "loss": 3.7662,
      "step": 9300
    },
    {
      "epoch": 0.601711821867559,
      "grad_norm": 1.582082748413086,
      "learning_rate": 0.00013982881781324412,
      "loss": 3.7678,
      "step": 9350
    },
    {
      "epoch": 0.6049295321449256,
      "grad_norm": 1.650062084197998,
      "learning_rate": 0.00013950704678550744,
      "loss": 3.7322,
      "step": 9400
    },
    {
      "epoch": 0.6081472424222923,
      "grad_norm": 1.618935227394104,
      "learning_rate": 0.0001391852757577708,
      "loss": 3.7449,
      "step": 9450
    },
    {
      "epoch": 0.611364952699659,
      "grad_norm": 1.445117712020874,
      "learning_rate": 0.0001388635047300341,
      "loss": 3.7471,
      "step": 9500
    },
    {
      "epoch": 0.6145826629770256,
      "grad_norm": 1.5350823402404785,
      "learning_rate": 0.00013854173370229746,
      "loss": 3.7391,
      "step": 9550
    },
    {
      "epoch": 0.6178003732543922,
      "grad_norm": 1.5308250188827515,
      "learning_rate": 0.00013821996267456078,
      "loss": 3.7519,
      "step": 9600
    },
    {
      "epoch": 0.6210180835317588,
      "grad_norm": 1.630052924156189,
      "learning_rate": 0.00013789819164682413,
      "loss": 3.7694,
      "step": 9650
    },
    {
      "epoch": 0.6242357938091254,
      "grad_norm": 1.4454141855239868,
      "learning_rate": 0.00013757642061908748,
      "loss": 3.7672,
      "step": 9700
    },
    {
      "epoch": 0.627453504086492,
      "grad_norm": 1.3629132509231567,
      "learning_rate": 0.0001372546495913508,
      "loss": 3.7147,
      "step": 9750
    },
    {
      "epoch": 0.6306712143638586,
      "grad_norm": 1.431198000907898,
      "learning_rate": 0.00013693287856361415,
      "loss": 3.7362,
      "step": 9800
    },
    {
      "epoch": 0.6338889246412253,
      "grad_norm": 1.4634430408477783,
      "learning_rate": 0.00013661110753587747,
      "loss": 3.7349,
      "step": 9850
    },
    {
      "epoch": 0.637106634918592,
      "grad_norm": 1.4025251865386963,
      "learning_rate": 0.00013628933650814082,
      "loss": 3.7584,
      "step": 9900
    },
    {
      "epoch": 0.6403243451959586,
      "grad_norm": 1.5529147386550903,
      "learning_rate": 0.00013596756548040414,
      "loss": 3.7307,
      "step": 9950
    },
    {
      "epoch": 0.6435420554733252,
      "grad_norm": 1.4610544443130493,
      "learning_rate": 0.0001356457944526675,
      "loss": 3.7616,
      "step": 10000
    },
    {
      "epoch": 0.6435420554733252,
      "eval_loss": 3.7617077827453613,
      "eval_runtime": 265.2239,
      "eval_samples_per_second": 235.771,
      "eval_steps_per_second": 14.738,
      "step": 10000
    },
    {
      "epoch": 0.6467597657506918,
      "grad_norm": 1.4213262796401978,
      "learning_rate": 0.0001353240234249308,
      "loss": 3.759,
      "step": 10050
    },
    {
      "epoch": 0.6499774760280584,
      "grad_norm": 1.542510986328125,
      "learning_rate": 0.00013500225239719416,
      "loss": 3.7458,
      "step": 10100
    },
    {
      "epoch": 0.6531951863054251,
      "grad_norm": 1.6064282655715942,
      "learning_rate": 0.0001346804813694575,
      "loss": 3.7184,
      "step": 10150
    },
    {
      "epoch": 0.6564128965827917,
      "grad_norm": 1.514302134513855,
      "learning_rate": 0.00013435871034172083,
      "loss": 3.755,
      "step": 10200
    },
    {
      "epoch": 0.6596306068601583,
      "grad_norm": 1.5046477317810059,
      "learning_rate": 0.00013403693931398418,
      "loss": 3.7595,
      "step": 10250
    },
    {
      "epoch": 0.6628483171375249,
      "grad_norm": 1.461693525314331,
      "learning_rate": 0.0001337151682862475,
      "loss": 3.7359,
      "step": 10300
    },
    {
      "epoch": 0.6660660274148915,
      "grad_norm": 1.457958459854126,
      "learning_rate": 0.00013339339725851085,
      "loss": 3.7513,
      "step": 10350
    },
    {
      "epoch": 0.6692837376922582,
      "grad_norm": 1.6713271141052246,
      "learning_rate": 0.00013307162623077417,
      "loss": 3.7353,
      "step": 10400
    },
    {
      "epoch": 0.6725014479696249,
      "grad_norm": 1.3290150165557861,
      "learning_rate": 0.00013274985520303752,
      "loss": 3.7586,
      "step": 10450
    },
    {
      "epoch": 0.6757191582469915,
      "grad_norm": 1.5137012004852295,
      "learning_rate": 0.00013242808417530087,
      "loss": 3.745,
      "step": 10500
    },
    {
      "epoch": 0.6789368685243581,
      "grad_norm": 1.3896609544754028,
      "learning_rate": 0.00013210631314756422,
      "loss": 3.7468,
      "step": 10550
    },
    {
      "epoch": 0.6821545788017247,
      "grad_norm": 1.526628851890564,
      "learning_rate": 0.00013178454211982754,
      "loss": 3.7187,
      "step": 10600
    },
    {
      "epoch": 0.6853722890790913,
      "grad_norm": 1.5003886222839355,
      "learning_rate": 0.00013146277109209086,
      "loss": 3.7616,
      "step": 10650
    },
    {
      "epoch": 0.6885899993564579,
      "grad_norm": 1.428350806236267,
      "learning_rate": 0.0001311410000643542,
      "loss": 3.7305,
      "step": 10700
    },
    {
      "epoch": 0.6918077096338245,
      "grad_norm": 1.4537230730056763,
      "learning_rate": 0.00013081922903661753,
      "loss": 3.693,
      "step": 10750
    },
    {
      "epoch": 0.6950254199111912,
      "grad_norm": 1.5022156238555908,
      "learning_rate": 0.00013049745800888088,
      "loss": 3.715,
      "step": 10800
    },
    {
      "epoch": 0.6982431301885578,
      "grad_norm": 1.4604668617248535,
      "learning_rate": 0.0001301756869811442,
      "loss": 3.7354,
      "step": 10850
    },
    {
      "epoch": 0.7014608404659245,
      "grad_norm": 1.4215859174728394,
      "learning_rate": 0.00012985391595340758,
      "loss": 3.7292,
      "step": 10900
    },
    {
      "epoch": 0.7046785507432911,
      "grad_norm": 1.5191484689712524,
      "learning_rate": 0.0001295321449256709,
      "loss": 3.6965,
      "step": 10950
    },
    {
      "epoch": 0.7078962610206577,
      "grad_norm": 1.4942009449005127,
      "learning_rate": 0.00012921037389793425,
      "loss": 3.7364,
      "step": 11000
    },
    {
      "epoch": 0.7111139712980243,
      "grad_norm": 1.5531175136566162,
      "learning_rate": 0.00012888860287019757,
      "loss": 3.7132,
      "step": 11050
    },
    {
      "epoch": 0.714331681575391,
      "grad_norm": 1.5190006494522095,
      "learning_rate": 0.00012856683184246092,
      "loss": 3.7264,
      "step": 11100
    },
    {
      "epoch": 0.7175493918527576,
      "grad_norm": 1.3611589670181274,
      "learning_rate": 0.00012824506081472424,
      "loss": 3.7273,
      "step": 11150
    },
    {
      "epoch": 0.7207671021301242,
      "grad_norm": 1.5290502309799194,
      "learning_rate": 0.00012792328978698757,
      "loss": 3.7106,
      "step": 11200
    },
    {
      "epoch": 0.7239848124074908,
      "grad_norm": 1.745439052581787,
      "learning_rate": 0.00012760151875925092,
      "loss": 3.7271,
      "step": 11250
    },
    {
      "epoch": 0.7272025226848574,
      "grad_norm": 1.4779292345046997,
      "learning_rate": 0.00012727974773151426,
      "loss": 3.7057,
      "step": 11300
    },
    {
      "epoch": 0.7304202329622241,
      "grad_norm": 1.514512300491333,
      "learning_rate": 0.0001269579767037776,
      "loss": 3.7329,
      "step": 11350
    },
    {
      "epoch": 0.7336379432395908,
      "grad_norm": 1.3217744827270508,
      "learning_rate": 0.00012663620567604093,
      "loss": 3.7149,
      "step": 11400
    },
    {
      "epoch": 0.7368556535169574,
      "grad_norm": 1.4138789176940918,
      "learning_rate": 0.00012631443464830428,
      "loss": 3.7174,
      "step": 11450
    },
    {
      "epoch": 0.740073363794324,
      "grad_norm": 1.6095556020736694,
      "learning_rate": 0.0001259926636205676,
      "loss": 3.7151,
      "step": 11500
    },
    {
      "epoch": 0.7432910740716906,
      "grad_norm": 1.3526136875152588,
      "learning_rate": 0.00012567089259283095,
      "loss": 3.7233,
      "step": 11550
    },
    {
      "epoch": 0.7465087843490572,
      "grad_norm": 1.3962355852127075,
      "learning_rate": 0.00012534912156509428,
      "loss": 3.7175,
      "step": 11600
    },
    {
      "epoch": 0.7497264946264238,
      "grad_norm": 1.3706499338150024,
      "learning_rate": 0.00012502735053735763,
      "loss": 3.7423,
      "step": 11650
    },
    {
      "epoch": 0.7529442049037904,
      "grad_norm": 1.5154669284820557,
      "learning_rate": 0.00012470557950962097,
      "loss": 3.7109,
      "step": 11700
    },
    {
      "epoch": 0.756161915181157,
      "grad_norm": 1.5177197456359863,
      "learning_rate": 0.0001243838084818843,
      "loss": 3.7181,
      "step": 11750
    },
    {
      "epoch": 0.7593796254585237,
      "grad_norm": 1.544091820716858,
      "learning_rate": 0.00012406203745414765,
      "loss": 3.7037,
      "step": 11800
    },
    {
      "epoch": 0.7625973357358904,
      "grad_norm": 1.4206589460372925,
      "learning_rate": 0.00012374026642641097,
      "loss": 3.7358,
      "step": 11850
    },
    {
      "epoch": 0.765815046013257,
      "grad_norm": 1.394504189491272,
      "learning_rate": 0.00012341849539867432,
      "loss": 3.7373,
      "step": 11900
    },
    {
      "epoch": 0.7690327562906236,
      "grad_norm": 1.4185174703598022,
      "learning_rate": 0.00012309672437093764,
      "loss": 3.7127,
      "step": 11950
    },
    {
      "epoch": 0.7722504665679902,
      "grad_norm": 1.4325945377349854,
      "learning_rate": 0.000122774953343201,
      "loss": 3.6975,
      "step": 12000
    },
    {
      "epoch": 0.7754681768453568,
      "grad_norm": 1.3826154470443726,
      "learning_rate": 0.0001224531823154643,
      "loss": 3.7356,
      "step": 12050
    },
    {
      "epoch": 0.7786858871227235,
      "grad_norm": 1.304455280303955,
      "learning_rate": 0.00012213141128772766,
      "loss": 3.7046,
      "step": 12100
    },
    {
      "epoch": 0.7819035974000901,
      "grad_norm": 1.3714405298233032,
      "learning_rate": 0.00012180964025999099,
      "loss": 3.6891,
      "step": 12150
    },
    {
      "epoch": 0.7851213076774567,
      "grad_norm": 1.4013997316360474,
      "learning_rate": 0.00012148786923225433,
      "loss": 3.7087,
      "step": 12200
    },
    {
      "epoch": 0.7883390179548233,
      "grad_norm": 1.6577954292297363,
      "learning_rate": 0.00012116609820451768,
      "loss": 3.7395,
      "step": 12250
    },
    {
      "epoch": 0.7915567282321899,
      "grad_norm": 1.344954490661621,
      "learning_rate": 0.000120844327176781,
      "loss": 3.7071,
      "step": 12300
    },
    {
      "epoch": 0.7947744385095566,
      "grad_norm": 1.4181835651397705,
      "learning_rate": 0.00012052255614904435,
      "loss": 3.7087,
      "step": 12350
    },
    {
      "epoch": 0.7979921487869233,
      "grad_norm": 1.3909629583358765,
      "learning_rate": 0.00012020078512130768,
      "loss": 3.7253,
      "step": 12400
    },
    {
      "epoch": 0.8012098590642899,
      "grad_norm": 1.4208472967147827,
      "learning_rate": 0.00011987901409357103,
      "loss": 3.6838,
      "step": 12450
    },
    {
      "epoch": 0.8044275693416565,
      "grad_norm": 1.3540873527526855,
      "learning_rate": 0.00011955724306583435,
      "loss": 3.7277,
      "step": 12500
    },
    {
      "epoch": 0.8076452796190231,
      "grad_norm": 1.4144264459609985,
      "learning_rate": 0.0001192354720380977,
      "loss": 3.7101,
      "step": 12550
    },
    {
      "epoch": 0.8108629898963897,
      "grad_norm": 1.3713274002075195,
      "learning_rate": 0.00011891370101036104,
      "loss": 3.7337,
      "step": 12600
    },
    {
      "epoch": 0.8140807001737563,
      "grad_norm": 1.3811883926391602,
      "learning_rate": 0.00011859192998262436,
      "loss": 3.709,
      "step": 12650
    },
    {
      "epoch": 0.817298410451123,
      "grad_norm": 1.2885204553604126,
      "learning_rate": 0.00011827015895488771,
      "loss": 3.722,
      "step": 12700
    },
    {
      "epoch": 0.8205161207284896,
      "grad_norm": 1.3638536930084229,
      "learning_rate": 0.00011794838792715104,
      "loss": 3.7046,
      "step": 12750
    },
    {
      "epoch": 0.8237338310058563,
      "grad_norm": 1.470550775527954,
      "learning_rate": 0.00011762661689941439,
      "loss": 3.7051,
      "step": 12800
    },
    {
      "epoch": 0.8269515412832229,
      "grad_norm": 1.3965017795562744,
      "learning_rate": 0.00011730484587167771,
      "loss": 3.6938,
      "step": 12850
    },
    {
      "epoch": 0.8301692515605895,
      "grad_norm": 1.433742642402649,
      "learning_rate": 0.00011698307484394106,
      "loss": 3.7036,
      "step": 12900
    },
    {
      "epoch": 0.8333869618379561,
      "grad_norm": 1.3865172863006592,
      "learning_rate": 0.0001166613038162044,
      "loss": 3.7161,
      "step": 12950
    },
    {
      "epoch": 0.8366046721153227,
      "grad_norm": 1.3329259157180786,
      "learning_rate": 0.00011633953278846775,
      "loss": 3.6957,
      "step": 13000
    },
    {
      "epoch": 0.8398223823926894,
      "grad_norm": 1.3531204462051392,
      "learning_rate": 0.00011601776176073107,
      "loss": 3.679,
      "step": 13050
    },
    {
      "epoch": 0.843040092670056,
      "grad_norm": 1.444761872291565,
      "learning_rate": 0.00011569599073299439,
      "loss": 3.6865,
      "step": 13100
    },
    {
      "epoch": 0.8462578029474226,
      "grad_norm": 1.289284586906433,
      "learning_rate": 0.00011537421970525774,
      "loss": 3.7248,
      "step": 13150
    },
    {
      "epoch": 0.8494755132247892,
      "grad_norm": 1.3678547143936157,
      "learning_rate": 0.00011505244867752108,
      "loss": 3.7121,
      "step": 13200
    },
    {
      "epoch": 0.8526932235021558,
      "grad_norm": 1.374481201171875,
      "learning_rate": 0.00011473067764978443,
      "loss": 3.69,
      "step": 13250
    },
    {
      "epoch": 0.8559109337795225,
      "grad_norm": 1.378310203552246,
      "learning_rate": 0.00011440890662204775,
      "loss": 3.7298,
      "step": 13300
    },
    {
      "epoch": 0.8591286440568892,
      "grad_norm": 1.431227684020996,
      "learning_rate": 0.0001140871355943111,
      "loss": 3.702,
      "step": 13350
    },
    {
      "epoch": 0.8623463543342558,
      "grad_norm": 1.332476258277893,
      "learning_rate": 0.00011376536456657443,
      "loss": 3.7081,
      "step": 13400
    },
    {
      "epoch": 0.8655640646116224,
      "grad_norm": 1.5192354917526245,
      "learning_rate": 0.00011344359353883778,
      "loss": 3.7,
      "step": 13450
    },
    {
      "epoch": 0.868781774888989,
      "grad_norm": 1.4399982690811157,
      "learning_rate": 0.0001131218225111011,
      "loss": 3.6885,
      "step": 13500
    },
    {
      "epoch": 0.8719994851663556,
      "grad_norm": 1.4089025259017944,
      "learning_rate": 0.00011280005148336444,
      "loss": 3.6893,
      "step": 13550
    },
    {
      "epoch": 0.8752171954437222,
      "grad_norm": 1.3762171268463135,
      "learning_rate": 0.00011247828045562779,
      "loss": 3.6927,
      "step": 13600
    },
    {
      "epoch": 0.8784349057210888,
      "grad_norm": 1.632237195968628,
      "learning_rate": 0.00011215650942789111,
      "loss": 3.6837,
      "step": 13650
    },
    {
      "epoch": 0.8816526159984555,
      "grad_norm": 1.2982897758483887,
      "learning_rate": 0.00011183473840015446,
      "loss": 3.6975,
      "step": 13700
    },
    {
      "epoch": 0.8848703262758221,
      "grad_norm": 1.3807618618011475,
      "learning_rate": 0.00011151296737241779,
      "loss": 3.6985,
      "step": 13750
    },
    {
      "epoch": 0.8880880365531888,
      "grad_norm": 1.3885951042175293,
      "learning_rate": 0.00011119119634468114,
      "loss": 3.6966,
      "step": 13800
    },
    {
      "epoch": 0.8913057468305554,
      "grad_norm": 1.4155813455581665,
      "learning_rate": 0.00011086942531694446,
      "loss": 3.6947,
      "step": 13850
    },
    {
      "epoch": 0.894523457107922,
      "grad_norm": 1.4288203716278076,
      "learning_rate": 0.00011054765428920781,
      "loss": 3.7077,
      "step": 13900
    },
    {
      "epoch": 0.8977411673852886,
      "grad_norm": 1.4406700134277344,
      "learning_rate": 0.00011022588326147115,
      "loss": 3.7156,
      "step": 13950
    },
    {
      "epoch": 0.9009588776626553,
      "grad_norm": 1.343912959098816,
      "learning_rate": 0.0001099041122337345,
      "loss": 3.6916,
      "step": 14000
    },
    {
      "epoch": 0.9041765879400219,
      "grad_norm": 1.373458743095398,
      "learning_rate": 0.00010958234120599782,
      "loss": 3.6794,
      "step": 14050
    },
    {
      "epoch": 0.9073942982173885,
      "grad_norm": 1.5218682289123535,
      "learning_rate": 0.00010926057017826114,
      "loss": 3.6808,
      "step": 14100
    },
    {
      "epoch": 0.9106120084947551,
      "grad_norm": 1.3118118047714233,
      "learning_rate": 0.00010893879915052449,
      "loss": 3.6818,
      "step": 14150
    },
    {
      "epoch": 0.9138297187721217,
      "grad_norm": 1.2833913564682007,
      "learning_rate": 0.00010861702812278782,
      "loss": 3.6812,
      "step": 14200
    },
    {
      "epoch": 0.9170474290494884,
      "grad_norm": 1.4563984870910645,
      "learning_rate": 0.00010829525709505117,
      "loss": 3.71,
      "step": 14250
    },
    {
      "epoch": 0.920265139326855,
      "grad_norm": 1.4481703042984009,
      "learning_rate": 0.0001079734860673145,
      "loss": 3.7153,
      "step": 14300
    },
    {
      "epoch": 0.9234828496042217,
      "grad_norm": 1.3796296119689941,
      "learning_rate": 0.00010765171503957784,
      "loss": 3.6861,
      "step": 14350
    },
    {
      "epoch": 0.9267005598815883,
      "grad_norm": 1.3451817035675049,
      "learning_rate": 0.00010732994401184118,
      "loss": 3.676,
      "step": 14400
    },
    {
      "epoch": 0.9299182701589549,
      "grad_norm": 1.4265419244766235,
      "learning_rate": 0.00010700817298410453,
      "loss": 3.6965,
      "step": 14450
    },
    {
      "epoch": 0.9331359804363215,
      "grad_norm": 1.4312410354614258,
      "learning_rate": 0.00010668640195636785,
      "loss": 3.6793,
      "step": 14500
    },
    {
      "epoch": 0.9363536907136881,
      "grad_norm": 1.27151358127594,
      "learning_rate": 0.00010636463092863119,
      "loss": 3.6923,
      "step": 14550
    },
    {
      "epoch": 0.9395714009910547,
      "grad_norm": 1.3572524785995483,
      "learning_rate": 0.00010604285990089453,
      "loss": 3.6893,
      "step": 14600
    },
    {
      "epoch": 0.9427891112684214,
      "grad_norm": 1.2574596405029297,
      "learning_rate": 0.00010572108887315786,
      "loss": 3.6794,
      "step": 14650
    },
    {
      "epoch": 0.946006821545788,
      "grad_norm": 1.5595601797103882,
      "learning_rate": 0.0001053993178454212,
      "loss": 3.6837,
      "step": 14700
    },
    {
      "epoch": 0.9492245318231547,
      "grad_norm": 1.5511091947555542,
      "learning_rate": 0.00010507754681768454,
      "loss": 3.6608,
      "step": 14750
    },
    {
      "epoch": 0.9524422421005213,
      "grad_norm": 1.4596153497695923,
      "learning_rate": 0.00010475577578994789,
      "loss": 3.6629,
      "step": 14800
    },
    {
      "epoch": 0.9556599523778879,
      "grad_norm": 1.3913320302963257,
      "learning_rate": 0.00010443400476221121,
      "loss": 3.6884,
      "step": 14850
    },
    {
      "epoch": 0.9588776626552545,
      "grad_norm": 1.4841831922531128,
      "learning_rate": 0.00010411223373447456,
      "loss": 3.6652,
      "step": 14900
    },
    {
      "epoch": 0.9620953729326212,
      "grad_norm": 1.3244131803512573,
      "learning_rate": 0.0001037904627067379,
      "loss": 3.6623,
      "step": 14950
    },
    {
      "epoch": 0.9653130832099878,
      "grad_norm": 1.6536582708358765,
      "learning_rate": 0.00010346869167900122,
      "loss": 3.6971,
      "step": 15000
    },
    {
      "epoch": 0.9653130832099878,
      "eval_loss": 3.7068796157836914,
      "eval_runtime": 263.4895,
      "eval_samples_per_second": 237.323,
      "eval_steps_per_second": 14.836,
      "step": 15000
    },
    {
      "epoch": 0.9685307934873544,
      "grad_norm": 1.257914423942566,
      "learning_rate": 0.00010314692065126457,
      "loss": 3.6915,
      "step": 15050
    },
    {
      "epoch": 0.971748503764721,
      "grad_norm": 1.358247995376587,
      "learning_rate": 0.00010282514962352789,
      "loss": 3.6846,
      "step": 15100
    },
    {
      "epoch": 0.9749662140420876,
      "grad_norm": 1.3499046564102173,
      "learning_rate": 0.00010250337859579125,
      "loss": 3.6856,
      "step": 15150
    },
    {
      "epoch": 0.9781839243194542,
      "grad_norm": 1.4130003452301025,
      "learning_rate": 0.00010218160756805457,
      "loss": 3.6608,
      "step": 15200
    },
    {
      "epoch": 0.981401634596821,
      "grad_norm": 1.321494460105896,
      "learning_rate": 0.00010185983654031792,
      "loss": 3.6818,
      "step": 15250
    },
    {
      "epoch": 0.9846193448741876,
      "grad_norm": 1.38776433467865,
      "learning_rate": 0.00010153806551258124,
      "loss": 3.6728,
      "step": 15300
    },
    {
      "epoch": 0.9878370551515542,
      "grad_norm": 1.4059538841247559,
      "learning_rate": 0.00010121629448484459,
      "loss": 3.6836,
      "step": 15350
    },
    {
      "epoch": 0.9910547654289208,
      "grad_norm": 1.3295284509658813,
      "learning_rate": 0.00010089452345710793,
      "loss": 3.6843,
      "step": 15400
    },
    {
      "epoch": 0.9942724757062874,
      "grad_norm": 1.3209415674209595,
      "learning_rate": 0.00010057275242937128,
      "loss": 3.68,
      "step": 15450
    },
    {
      "epoch": 0.997490185983654,
      "grad_norm": 1.281501054763794,
      "learning_rate": 0.0001002509814016346,
      "loss": 3.6762,
      "step": 15500
    },
    {
      "epoch": 1.0007078962610207,
      "grad_norm": 1.371727466583252,
      "learning_rate": 9.992921037389795e-05,
      "loss": 3.6431,
      "step": 15550
    },
    {
      "epoch": 1.0039256065383872,
      "grad_norm": 1.3286274671554565,
      "learning_rate": 9.960743934616128e-05,
      "loss": 3.5694,
      "step": 15600
    },
    {
      "epoch": 1.007143316815754,
      "grad_norm": 1.3124269247055054,
      "learning_rate": 9.928566831842462e-05,
      "loss": 3.5659,
      "step": 15650
    },
    {
      "epoch": 1.0103610270931205,
      "grad_norm": 1.4841876029968262,
      "learning_rate": 9.896389729068794e-05,
      "loss": 3.5772,
      "step": 15700
    },
    {
      "epoch": 1.0135787373704872,
      "grad_norm": 1.3433741331100464,
      "learning_rate": 9.864212626295129e-05,
      "loss": 3.5851,
      "step": 15750
    },
    {
      "epoch": 1.0167964476478537,
      "grad_norm": 1.2452703714370728,
      "learning_rate": 9.832035523521462e-05,
      "loss": 3.5724,
      "step": 15800
    },
    {
      "epoch": 1.0200141579252204,
      "grad_norm": 1.3698406219482422,
      "learning_rate": 9.799858420747796e-05,
      "loss": 3.6017,
      "step": 15850
    },
    {
      "epoch": 1.023231868202587,
      "grad_norm": 1.453417181968689,
      "learning_rate": 9.76768131797413e-05,
      "loss": 3.5625,
      "step": 15900
    },
    {
      "epoch": 1.0264495784799537,
      "grad_norm": 1.2847111225128174,
      "learning_rate": 9.735504215200464e-05,
      "loss": 3.5802,
      "step": 15950
    },
    {
      "epoch": 1.0296672887573204,
      "grad_norm": 1.336992859840393,
      "learning_rate": 9.703327112426798e-05,
      "loss": 3.5746,
      "step": 16000
    },
    {
      "epoch": 1.032884999034687,
      "grad_norm": 1.3536893129348755,
      "learning_rate": 9.671150009653131e-05,
      "loss": 3.5862,
      "step": 16050
    },
    {
      "epoch": 1.0361027093120536,
      "grad_norm": 1.4305367469787598,
      "learning_rate": 9.638972906879465e-05,
      "loss": 3.5866,
      "step": 16100
    },
    {
      "epoch": 1.0393204195894201,
      "grad_norm": 1.3105171918869019,
      "learning_rate": 9.6067958041058e-05,
      "loss": 3.5802,
      "step": 16150
    },
    {
      "epoch": 1.0425381298667868,
      "grad_norm": 1.280097246170044,
      "learning_rate": 9.574618701332132e-05,
      "loss": 3.5541,
      "step": 16200
    },
    {
      "epoch": 1.0457558401441533,
      "grad_norm": 1.3462110757827759,
      "learning_rate": 9.542441598558466e-05,
      "loss": 3.5535,
      "step": 16250
    },
    {
      "epoch": 1.04897355042152,
      "grad_norm": 1.332698941230774,
      "learning_rate": 9.510264495784799e-05,
      "loss": 3.5734,
      "step": 16300
    },
    {
      "epoch": 1.0521912606988866,
      "grad_norm": 1.3461825847625732,
      "learning_rate": 9.478087393011134e-05,
      "loss": 3.5912,
      "step": 16350
    },
    {
      "epoch": 1.0554089709762533,
      "grad_norm": 1.2803709506988525,
      "learning_rate": 9.445910290237468e-05,
      "loss": 3.5854,
      "step": 16400
    },
    {
      "epoch": 1.05862668125362,
      "grad_norm": 1.2886099815368652,
      "learning_rate": 9.413733187463801e-05,
      "loss": 3.5376,
      "step": 16450
    },
    {
      "epoch": 1.0618443915309865,
      "grad_norm": 1.3081588745117188,
      "learning_rate": 9.381556084690135e-05,
      "loss": 3.5603,
      "step": 16500
    },
    {
      "epoch": 1.0650621018083533,
      "grad_norm": 1.375091552734375,
      "learning_rate": 9.34937898191647e-05,
      "loss": 3.6073,
      "step": 16550
    },
    {
      "epoch": 1.0682798120857198,
      "grad_norm": 1.3548369407653809,
      "learning_rate": 9.317201879142803e-05,
      "loss": 3.5777,
      "step": 16600
    },
    {
      "epoch": 1.0714975223630865,
      "grad_norm": 1.3117871284484863,
      "learning_rate": 9.285024776369137e-05,
      "loss": 3.5796,
      "step": 16650
    },
    {
      "epoch": 1.074715232640453,
      "grad_norm": 1.354841709136963,
      "learning_rate": 9.25284767359547e-05,
      "loss": 3.5652,
      "step": 16700
    },
    {
      "epoch": 1.0779329429178197,
      "grad_norm": 1.2852262258529663,
      "learning_rate": 9.220670570821804e-05,
      "loss": 3.5794,
      "step": 16750
    },
    {
      "epoch": 1.0811506531951862,
      "grad_norm": 1.3593242168426514,
      "learning_rate": 9.188493468048137e-05,
      "loss": 3.5881,
      "step": 16800
    },
    {
      "epoch": 1.084368363472553,
      "grad_norm": 1.318591594696045,
      "learning_rate": 9.156316365274471e-05,
      "loss": 3.5731,
      "step": 16850
    },
    {
      "epoch": 1.0875860737499194,
      "grad_norm": 1.3135958909988403,
      "learning_rate": 9.124139262500804e-05,
      "loss": 3.5616,
      "step": 16900
    },
    {
      "epoch": 1.0908037840272862,
      "grad_norm": 1.3136214017868042,
      "learning_rate": 9.091962159727139e-05,
      "loss": 3.558,
      "step": 16950
    },
    {
      "epoch": 1.094021494304653,
      "grad_norm": 1.3504694700241089,
      "learning_rate": 9.059785056953473e-05,
      "loss": 3.5833,
      "step": 17000
    },
    {
      "epoch": 1.0972392045820194,
      "grad_norm": 1.3666808605194092,
      "learning_rate": 9.027607954179806e-05,
      "loss": 3.5707,
      "step": 17050
    },
    {
      "epoch": 1.1004569148593861,
      "grad_norm": 1.3129650354385376,
      "learning_rate": 8.99543085140614e-05,
      "loss": 3.6015,
      "step": 17100
    },
    {
      "epoch": 1.1036746251367526,
      "grad_norm": 1.3991070985794067,
      "learning_rate": 8.963253748632473e-05,
      "loss": 3.5786,
      "step": 17150
    },
    {
      "epoch": 1.1068923354141194,
      "grad_norm": 1.2787703275680542,
      "learning_rate": 8.931076645858807e-05,
      "loss": 3.5607,
      "step": 17200
    },
    {
      "epoch": 1.1101100456914859,
      "grad_norm": 1.3472989797592163,
      "learning_rate": 8.89889954308514e-05,
      "loss": 3.5542,
      "step": 17250
    },
    {
      "epoch": 1.1133277559688526,
      "grad_norm": 1.4285074472427368,
      "learning_rate": 8.866722440311474e-05,
      "loss": 3.5939,
      "step": 17300
    },
    {
      "epoch": 1.116545466246219,
      "grad_norm": 1.3628560304641724,
      "learning_rate": 8.834545337537809e-05,
      "loss": 3.567,
      "step": 17350
    },
    {
      "epoch": 1.1197631765235858,
      "grad_norm": 1.2474770545959473,
      "learning_rate": 8.802368234764142e-05,
      "loss": 3.5981,
      "step": 17400
    },
    {
      "epoch": 1.1229808868009525,
      "grad_norm": 1.3459198474884033,
      "learning_rate": 8.770191131990476e-05,
      "loss": 3.564,
      "step": 17450
    },
    {
      "epoch": 1.126198597078319,
      "grad_norm": 1.3066281080245972,
      "learning_rate": 8.73801402921681e-05,
      "loss": 3.5737,
      "step": 17500
    },
    {
      "epoch": 1.1294163073556858,
      "grad_norm": 1.3470265865325928,
      "learning_rate": 8.705836926443144e-05,
      "loss": 3.5519,
      "step": 17550
    },
    {
      "epoch": 1.1326340176330523,
      "grad_norm": 1.325677752494812,
      "learning_rate": 8.673659823669478e-05,
      "loss": 3.5822,
      "step": 17600
    },
    {
      "epoch": 1.135851727910419,
      "grad_norm": 1.251651406288147,
      "learning_rate": 8.64148272089581e-05,
      "loss": 3.5757,
      "step": 17650
    },
    {
      "epoch": 1.1390694381877855,
      "grad_norm": 1.3146425485610962,
      "learning_rate": 8.609305618122145e-05,
      "loss": 3.5839,
      "step": 17700
    },
    {
      "epoch": 1.1422871484651522,
      "grad_norm": 1.41942298412323,
      "learning_rate": 8.577128515348478e-05,
      "loss": 3.5656,
      "step": 17750
    },
    {
      "epoch": 1.1455048587425187,
      "grad_norm": 1.360817313194275,
      "learning_rate": 8.544951412574812e-05,
      "loss": 3.5814,
      "step": 17800
    },
    {
      "epoch": 1.1487225690198855,
      "grad_norm": 1.3837175369262695,
      "learning_rate": 8.512774309801146e-05,
      "loss": 3.562,
      "step": 17850
    },
    {
      "epoch": 1.1519402792972522,
      "grad_norm": 1.3302637338638306,
      "learning_rate": 8.480597207027479e-05,
      "loss": 3.5787,
      "step": 17900
    },
    {
      "epoch": 1.1551579895746187,
      "grad_norm": 1.3273913860321045,
      "learning_rate": 8.448420104253814e-05,
      "loss": 3.5784,
      "step": 17950
    },
    {
      "epoch": 1.1583756998519854,
      "grad_norm": 1.3345680236816406,
      "learning_rate": 8.416243001480148e-05,
      "loss": 3.5862,
      "step": 18000
    },
    {
      "epoch": 1.161593410129352,
      "grad_norm": 1.5223904848098755,
      "learning_rate": 8.384065898706481e-05,
      "loss": 3.5794,
      "step": 18050
    },
    {
      "epoch": 1.1648111204067186,
      "grad_norm": 1.3170835971832275,
      "learning_rate": 8.351888795932815e-05,
      "loss": 3.5806,
      "step": 18100
    },
    {
      "epoch": 1.1680288306840851,
      "grad_norm": 1.295703411102295,
      "learning_rate": 8.319711693159148e-05,
      "loss": 3.5684,
      "step": 18150
    },
    {
      "epoch": 1.1712465409614519,
      "grad_norm": 1.2782163619995117,
      "learning_rate": 8.287534590385482e-05,
      "loss": 3.5739,
      "step": 18200
    },
    {
      "epoch": 1.1744642512388184,
      "grad_norm": 1.429962396621704,
      "learning_rate": 8.255357487611815e-05,
      "loss": 3.5635,
      "step": 18250
    },
    {
      "epoch": 1.177681961516185,
      "grad_norm": 1.2697548866271973,
      "learning_rate": 8.22318038483815e-05,
      "loss": 3.5634,
      "step": 18300
    },
    {
      "epoch": 1.1808996717935516,
      "grad_norm": 1.4323211908340454,
      "learning_rate": 8.191003282064484e-05,
      "loss": 3.5885,
      "step": 18350
    },
    {
      "epoch": 1.1841173820709183,
      "grad_norm": 1.4669663906097412,
      "learning_rate": 8.158826179290817e-05,
      "loss": 3.5988,
      "step": 18400
    },
    {
      "epoch": 1.187335092348285,
      "grad_norm": 1.3237757682800293,
      "learning_rate": 8.126649076517151e-05,
      "loss": 3.5764,
      "step": 18450
    },
    {
      "epoch": 1.1905528026256516,
      "grad_norm": 1.3396570682525635,
      "learning_rate": 8.094471973743484e-05,
      "loss": 3.5583,
      "step": 18500
    },
    {
      "epoch": 1.1937705129030183,
      "grad_norm": 1.3677898645401,
      "learning_rate": 8.062294870969819e-05,
      "loss": 3.565,
      "step": 18550
    },
    {
      "epoch": 1.1969882231803848,
      "grad_norm": 1.2402373552322388,
      "learning_rate": 8.030117768196151e-05,
      "loss": 3.5822,
      "step": 18600
    },
    {
      "epoch": 1.2002059334577515,
      "grad_norm": 1.2478113174438477,
      "learning_rate": 7.997940665422485e-05,
      "loss": 3.5809,
      "step": 18650
    },
    {
      "epoch": 1.203423643735118,
      "grad_norm": 1.3786866664886475,
      "learning_rate": 7.96576356264882e-05,
      "loss": 3.5842,
      "step": 18700
    },
    {
      "epoch": 1.2066413540124847,
      "grad_norm": 1.2402065992355347,
      "learning_rate": 7.933586459875153e-05,
      "loss": 3.5665,
      "step": 18750
    },
    {
      "epoch": 1.2098590642898515,
      "grad_norm": 1.3640503883361816,
      "learning_rate": 7.901409357101487e-05,
      "loss": 3.5838,
      "step": 18800
    },
    {
      "epoch": 1.213076774567218,
      "grad_norm": 1.3896487951278687,
      "learning_rate": 7.86923225432782e-05,
      "loss": 3.6025,
      "step": 18850
    },
    {
      "epoch": 1.2162944848445847,
      "grad_norm": 1.2686352729797363,
      "learning_rate": 7.837055151554155e-05,
      "loss": 3.5561,
      "step": 18900
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 1.3675435781478882,
      "learning_rate": 7.804878048780489e-05,
      "loss": 3.5543,
      "step": 18950
    },
    {
      "epoch": 1.222729905399318,
      "grad_norm": 1.4386305809020996,
      "learning_rate": 7.772700946006822e-05,
      "loss": 3.5723,
      "step": 19000
    },
    {
      "epoch": 1.2259476156766844,
      "grad_norm": 1.3953163623809814,
      "learning_rate": 7.740523843233156e-05,
      "loss": 3.5987,
      "step": 19050
    },
    {
      "epoch": 1.2291653259540511,
      "grad_norm": 1.3075175285339355,
      "learning_rate": 7.70834674045949e-05,
      "loss": 3.5879,
      "step": 19100
    },
    {
      "epoch": 1.2323830362314177,
      "grad_norm": 1.3377676010131836,
      "learning_rate": 7.676169637685823e-05,
      "loss": 3.5568,
      "step": 19150
    },
    {
      "epoch": 1.2356007465087844,
      "grad_norm": 1.310508131980896,
      "learning_rate": 7.643992534912156e-05,
      "loss": 3.5767,
      "step": 19200
    },
    {
      "epoch": 1.2388184567861509,
      "grad_norm": 1.2935187816619873,
      "learning_rate": 7.61181543213849e-05,
      "loss": 3.5534,
      "step": 19250
    },
    {
      "epoch": 1.2420361670635176,
      "grad_norm": 1.346254587173462,
      "learning_rate": 7.579638329364825e-05,
      "loss": 3.5823,
      "step": 19300
    },
    {
      "epoch": 1.2452538773408843,
      "grad_norm": 1.3792812824249268,
      "learning_rate": 7.547461226591158e-05,
      "loss": 3.5854,
      "step": 19350
    },
    {
      "epoch": 1.2484715876182508,
      "grad_norm": 1.3770166635513306,
      "learning_rate": 7.515284123817492e-05,
      "loss": 3.5776,
      "step": 19400
    },
    {
      "epoch": 1.2516892978956176,
      "grad_norm": 1.3417412042617798,
      "learning_rate": 7.483107021043826e-05,
      "loss": 3.5503,
      "step": 19450
    },
    {
      "epoch": 1.254907008172984,
      "grad_norm": 1.2102118730545044,
      "learning_rate": 7.45092991827016e-05,
      "loss": 3.5675,
      "step": 19500
    },
    {
      "epoch": 1.2581247184503508,
      "grad_norm": 1.2482776641845703,
      "learning_rate": 7.418752815496494e-05,
      "loss": 3.5658,
      "step": 19550
    },
    {
      "epoch": 1.2613424287277173,
      "grad_norm": 1.279727816581726,
      "learning_rate": 7.386575712722826e-05,
      "loss": 3.6214,
      "step": 19600
    },
    {
      "epoch": 1.264560139005084,
      "grad_norm": 1.3243595361709595,
      "learning_rate": 7.35439860994916e-05,
      "loss": 3.5814,
      "step": 19650
    },
    {
      "epoch": 1.2677778492824507,
      "grad_norm": 1.3281787633895874,
      "learning_rate": 7.322221507175495e-05,
      "loss": 3.5811,
      "step": 19700
    },
    {
      "epoch": 1.2709955595598172,
      "grad_norm": 1.3438292741775513,
      "learning_rate": 7.290044404401828e-05,
      "loss": 3.5731,
      "step": 19750
    },
    {
      "epoch": 1.2742132698371837,
      "grad_norm": 1.2914618253707886,
      "learning_rate": 7.257867301628162e-05,
      "loss": 3.5682,
      "step": 19800
    },
    {
      "epoch": 1.2774309801145505,
      "grad_norm": 1.5457500219345093,
      "learning_rate": 7.225690198854495e-05,
      "loss": 3.5605,
      "step": 19850
    },
    {
      "epoch": 1.2806486903919172,
      "grad_norm": 1.4512808322906494,
      "learning_rate": 7.19351309608083e-05,
      "loss": 3.5697,
      "step": 19900
    },
    {
      "epoch": 1.2838664006692837,
      "grad_norm": 1.4141628742218018,
      "learning_rate": 7.161335993307164e-05,
      "loss": 3.5702,
      "step": 19950
    },
    {
      "epoch": 1.2870841109466504,
      "grad_norm": 1.3025624752044678,
      "learning_rate": 7.129158890533497e-05,
      "loss": 3.5689,
      "step": 20000
    },
    {
      "epoch": 1.2870841109466504,
      "eval_loss": 3.683199167251587,
      "eval_runtime": 263.9171,
      "eval_samples_per_second": 236.938,
      "eval_steps_per_second": 14.811,
      "step": 20000
    },
    {
      "epoch": 1.290301821224017,
      "grad_norm": 1.2821990251541138,
      "learning_rate": 7.096981787759829e-05,
      "loss": 3.5843,
      "step": 20050
    },
    {
      "epoch": 1.2935195315013837,
      "grad_norm": 1.253565788269043,
      "learning_rate": 7.064804684986164e-05,
      "loss": 3.5501,
      "step": 20100
    },
    {
      "epoch": 1.2967372417787502,
      "grad_norm": 1.4058635234832764,
      "learning_rate": 7.032627582212498e-05,
      "loss": 3.5608,
      "step": 20150
    },
    {
      "epoch": 1.2999549520561169,
      "grad_norm": 1.3863412141799927,
      "learning_rate": 7.000450479438831e-05,
      "loss": 3.5788,
      "step": 20200
    },
    {
      "epoch": 1.3031726623334836,
      "grad_norm": 1.4177054166793823,
      "learning_rate": 6.968273376665165e-05,
      "loss": 3.5781,
      "step": 20250
    },
    {
      "epoch": 1.3063903726108501,
      "grad_norm": 1.341414451599121,
      "learning_rate": 6.9360962738915e-05,
      "loss": 3.5664,
      "step": 20300
    },
    {
      "epoch": 1.3096080828882166,
      "grad_norm": 1.214327335357666,
      "learning_rate": 6.903919171117833e-05,
      "loss": 3.553,
      "step": 20350
    },
    {
      "epoch": 1.3128257931655833,
      "grad_norm": 1.3007310628890991,
      "learning_rate": 6.871742068344167e-05,
      "loss": 3.5618,
      "step": 20400
    },
    {
      "epoch": 1.31604350344295,
      "grad_norm": 1.322880744934082,
      "learning_rate": 6.8395649655705e-05,
      "loss": 3.577,
      "step": 20450
    },
    {
      "epoch": 1.3192612137203166,
      "grad_norm": 1.3201282024383545,
      "learning_rate": 6.807387862796835e-05,
      "loss": 3.5798,
      "step": 20500
    },
    {
      "epoch": 1.3224789239976833,
      "grad_norm": 1.3690431118011475,
      "learning_rate": 6.775210760023167e-05,
      "loss": 3.5661,
      "step": 20550
    },
    {
      "epoch": 1.3256966342750498,
      "grad_norm": 1.327406644821167,
      "learning_rate": 6.743033657249501e-05,
      "loss": 3.5775,
      "step": 20600
    },
    {
      "epoch": 1.3289143445524165,
      "grad_norm": 1.300678014755249,
      "learning_rate": 6.710856554475834e-05,
      "loss": 3.5466,
      "step": 20650
    },
    {
      "epoch": 1.332132054829783,
      "grad_norm": 1.341482400894165,
      "learning_rate": 6.67867945170217e-05,
      "loss": 3.5904,
      "step": 20700
    },
    {
      "epoch": 1.3353497651071498,
      "grad_norm": 1.4333558082580566,
      "learning_rate": 6.646502348928503e-05,
      "loss": 3.555,
      "step": 20750
    },
    {
      "epoch": 1.3385674753845165,
      "grad_norm": 1.2995190620422363,
      "learning_rate": 6.614325246154836e-05,
      "loss": 3.5663,
      "step": 20800
    },
    {
      "epoch": 1.341785185661883,
      "grad_norm": 1.2702720165252686,
      "learning_rate": 6.58214814338117e-05,
      "loss": 3.5683,
      "step": 20850
    },
    {
      "epoch": 1.3450028959392497,
      "grad_norm": 1.2509737014770508,
      "learning_rate": 6.549971040607505e-05,
      "loss": 3.5892,
      "step": 20900
    },
    {
      "epoch": 1.3482206062166162,
      "grad_norm": 1.3745512962341309,
      "learning_rate": 6.517793937833838e-05,
      "loss": 3.5888,
      "step": 20950
    },
    {
      "epoch": 1.351438316493983,
      "grad_norm": 1.35554838180542,
      "learning_rate": 6.485616835060172e-05,
      "loss": 3.5568,
      "step": 21000
    },
    {
      "epoch": 1.3546560267713494,
      "grad_norm": 1.2529220581054688,
      "learning_rate": 6.453439732286504e-05,
      "loss": 3.5656,
      "step": 21050
    },
    {
      "epoch": 1.3578737370487162,
      "grad_norm": 1.2773915529251099,
      "learning_rate": 6.421262629512839e-05,
      "loss": 3.5612,
      "step": 21100
    },
    {
      "epoch": 1.361091447326083,
      "grad_norm": 1.2728307247161865,
      "learning_rate": 6.389085526739173e-05,
      "loss": 3.5703,
      "step": 21150
    },
    {
      "epoch": 1.3643091576034494,
      "grad_norm": 1.3883274793624878,
      "learning_rate": 6.356908423965506e-05,
      "loss": 3.5583,
      "step": 21200
    },
    {
      "epoch": 1.367526867880816,
      "grad_norm": 1.3251320123672485,
      "learning_rate": 6.32473132119184e-05,
      "loss": 3.5639,
      "step": 21250
    },
    {
      "epoch": 1.3707445781581826,
      "grad_norm": 1.306999921798706,
      "learning_rate": 6.292554218418175e-05,
      "loss": 3.5706,
      "step": 21300
    },
    {
      "epoch": 1.3739622884355494,
      "grad_norm": 1.4314274787902832,
      "learning_rate": 6.260377115644508e-05,
      "loss": 3.5767,
      "step": 21350
    },
    {
      "epoch": 1.3771799987129159,
      "grad_norm": 1.264668583869934,
      "learning_rate": 6.228200012870842e-05,
      "loss": 3.5656,
      "step": 21400
    },
    {
      "epoch": 1.3803977089902826,
      "grad_norm": 1.2963719367980957,
      "learning_rate": 6.196022910097175e-05,
      "loss": 3.5589,
      "step": 21450
    },
    {
      "epoch": 1.383615419267649,
      "grad_norm": 1.2145180702209473,
      "learning_rate": 6.163845807323509e-05,
      "loss": 3.5693,
      "step": 21500
    },
    {
      "epoch": 1.3868331295450158,
      "grad_norm": 1.3015607595443726,
      "learning_rate": 6.131668704549842e-05,
      "loss": 3.5341,
      "step": 21550
    },
    {
      "epoch": 1.3900508398223823,
      "grad_norm": 1.2085806131362915,
      "learning_rate": 6.0994916017761764e-05,
      "loss": 3.5797,
      "step": 21600
    },
    {
      "epoch": 1.393268550099749,
      "grad_norm": 1.2589747905731201,
      "learning_rate": 6.06731449900251e-05,
      "loss": 3.5505,
      "step": 21650
    },
    {
      "epoch": 1.3964862603771158,
      "grad_norm": 1.2430782318115234,
      "learning_rate": 6.0351373962288435e-05,
      "loss": 3.5403,
      "step": 21700
    },
    {
      "epoch": 1.3997039706544823,
      "grad_norm": 1.3307102918624878,
      "learning_rate": 6.002960293455178e-05,
      "loss": 3.5423,
      "step": 21750
    },
    {
      "epoch": 1.4029216809318488,
      "grad_norm": 1.392088532447815,
      "learning_rate": 5.970783190681511e-05,
      "loss": 3.5668,
      "step": 21800
    },
    {
      "epoch": 1.4061393912092155,
      "grad_norm": 1.270573377609253,
      "learning_rate": 5.9386060879078454e-05,
      "loss": 3.5733,
      "step": 21850
    },
    {
      "epoch": 1.4093571014865822,
      "grad_norm": 1.3267242908477783,
      "learning_rate": 5.906428985134179e-05,
      "loss": 3.5378,
      "step": 21900
    },
    {
      "epoch": 1.4125748117639487,
      "grad_norm": 1.277879238128662,
      "learning_rate": 5.874251882360513e-05,
      "loss": 3.557,
      "step": 21950
    },
    {
      "epoch": 1.4157925220413154,
      "grad_norm": 1.3418408632278442,
      "learning_rate": 5.842074779586846e-05,
      "loss": 3.5637,
      "step": 22000
    },
    {
      "epoch": 1.419010232318682,
      "grad_norm": 1.3161855936050415,
      "learning_rate": 5.8098976768131796e-05,
      "loss": 3.5564,
      "step": 22050
    },
    {
      "epoch": 1.4222279425960487,
      "grad_norm": 1.2217267751693726,
      "learning_rate": 5.777720574039514e-05,
      "loss": 3.5686,
      "step": 22100
    },
    {
      "epoch": 1.4254456528734152,
      "grad_norm": 1.2629040479660034,
      "learning_rate": 5.7455434712658473e-05,
      "loss": 3.588,
      "step": 22150
    },
    {
      "epoch": 1.428663363150782,
      "grad_norm": 1.3220819234848022,
      "learning_rate": 5.713366368492181e-05,
      "loss": 3.5494,
      "step": 22200
    },
    {
      "epoch": 1.4318810734281486,
      "grad_norm": 1.3728837966918945,
      "learning_rate": 5.681189265718515e-05,
      "loss": 3.5713,
      "step": 22250
    },
    {
      "epoch": 1.4350987837055151,
      "grad_norm": 1.286388874053955,
      "learning_rate": 5.6490121629448486e-05,
      "loss": 3.567,
      "step": 22300
    },
    {
      "epoch": 1.4383164939828819,
      "grad_norm": 1.286555528640747,
      "learning_rate": 5.616835060171183e-05,
      "loss": 3.5824,
      "step": 22350
    },
    {
      "epoch": 1.4415342042602484,
      "grad_norm": 1.3228219747543335,
      "learning_rate": 5.5846579573975164e-05,
      "loss": 3.5806,
      "step": 22400
    },
    {
      "epoch": 1.444751914537615,
      "grad_norm": 1.2576545476913452,
      "learning_rate": 5.5524808546238506e-05,
      "loss": 3.55,
      "step": 22450
    },
    {
      "epoch": 1.4479696248149816,
      "grad_norm": 1.2351436614990234,
      "learning_rate": 5.5203037518501835e-05,
      "loss": 3.5524,
      "step": 22500
    },
    {
      "epoch": 1.4511873350923483,
      "grad_norm": 1.247772216796875,
      "learning_rate": 5.488126649076517e-05,
      "loss": 3.5931,
      "step": 22550
    },
    {
      "epoch": 1.454405045369715,
      "grad_norm": 1.2258278131484985,
      "learning_rate": 5.455949546302851e-05,
      "loss": 3.5614,
      "step": 22600
    },
    {
      "epoch": 1.4576227556470815,
      "grad_norm": 1.3227707147598267,
      "learning_rate": 5.423772443529185e-05,
      "loss": 3.5554,
      "step": 22650
    },
    {
      "epoch": 1.460840465924448,
      "grad_norm": 1.281236171722412,
      "learning_rate": 5.391595340755519e-05,
      "loss": 3.5348,
      "step": 22700
    },
    {
      "epoch": 1.4640581762018148,
      "grad_norm": 1.3787366151809692,
      "learning_rate": 5.3594182379818525e-05,
      "loss": 3.548,
      "step": 22750
    },
    {
      "epoch": 1.4672758864791815,
      "grad_norm": 1.331135630607605,
      "learning_rate": 5.327241135208186e-05,
      "loss": 3.5875,
      "step": 22800
    },
    {
      "epoch": 1.470493596756548,
      "grad_norm": 1.2703289985656738,
      "learning_rate": 5.29506403243452e-05,
      "loss": 3.558,
      "step": 22850
    },
    {
      "epoch": 1.4737113070339147,
      "grad_norm": 1.2831870317459106,
      "learning_rate": 5.262886929660854e-05,
      "loss": 3.5767,
      "step": 22900
    },
    {
      "epoch": 1.4769290173112812,
      "grad_norm": 1.410266399383545,
      "learning_rate": 5.2307098268871867e-05,
      "loss": 3.5491,
      "step": 22950
    },
    {
      "epoch": 1.480146727588648,
      "grad_norm": 1.31901216506958,
      "learning_rate": 5.198532724113521e-05,
      "loss": 3.5792,
      "step": 23000
    },
    {
      "epoch": 1.4833644378660145,
      "grad_norm": 1.2159701585769653,
      "learning_rate": 5.1663556213398544e-05,
      "loss": 3.5738,
      "step": 23050
    },
    {
      "epoch": 1.4865821481433812,
      "grad_norm": 1.2899971008300781,
      "learning_rate": 5.1341785185661886e-05,
      "loss": 3.5664,
      "step": 23100
    },
    {
      "epoch": 1.489799858420748,
      "grad_norm": 1.2672464847564697,
      "learning_rate": 5.102001415792522e-05,
      "loss": 3.5557,
      "step": 23150
    },
    {
      "epoch": 1.4930175686981144,
      "grad_norm": 1.274307131767273,
      "learning_rate": 5.0698243130188564e-05,
      "loss": 3.5682,
      "step": 23200
    },
    {
      "epoch": 1.496235278975481,
      "grad_norm": 1.302283763885498,
      "learning_rate": 5.03764721024519e-05,
      "loss": 3.5447,
      "step": 23250
    },
    {
      "epoch": 1.4994529892528476,
      "grad_norm": 1.3259388208389282,
      "learning_rate": 5.005470107471524e-05,
      "loss": 3.546,
      "step": 23300
    },
    {
      "epoch": 1.5026706995302144,
      "grad_norm": 1.2835019826889038,
      "learning_rate": 4.973293004697857e-05,
      "loss": 3.5567,
      "step": 23350
    },
    {
      "epoch": 1.5058884098075809,
      "grad_norm": 1.3070900440216064,
      "learning_rate": 4.941115901924191e-05,
      "loss": 3.5497,
      "step": 23400
    },
    {
      "epoch": 1.5091061200849476,
      "grad_norm": 1.266768217086792,
      "learning_rate": 4.908938799150525e-05,
      "loss": 3.5512,
      "step": 23450
    },
    {
      "epoch": 1.5123238303623143,
      "grad_norm": 1.2726787328720093,
      "learning_rate": 4.876761696376859e-05,
      "loss": 3.5402,
      "step": 23500
    },
    {
      "epoch": 1.5155415406396808,
      "grad_norm": 1.3295668363571167,
      "learning_rate": 4.844584593603192e-05,
      "loss": 3.5406,
      "step": 23550
    },
    {
      "epoch": 1.5187592509170473,
      "grad_norm": 1.3746063709259033,
      "learning_rate": 4.812407490829526e-05,
      "loss": 3.5365,
      "step": 23600
    },
    {
      "epoch": 1.521976961194414,
      "grad_norm": 1.33588707447052,
      "learning_rate": 4.7802303880558596e-05,
      "loss": 3.54,
      "step": 23650
    },
    {
      "epoch": 1.5251946714717808,
      "grad_norm": 1.2937724590301514,
      "learning_rate": 4.748053285282194e-05,
      "loss": 3.5657,
      "step": 23700
    },
    {
      "epoch": 1.5284123817491473,
      "grad_norm": 1.2987608909606934,
      "learning_rate": 4.715876182508527e-05,
      "loss": 3.5608,
      "step": 23750
    },
    {
      "epoch": 1.5316300920265138,
      "grad_norm": 1.3526161909103394,
      "learning_rate": 4.683699079734861e-05,
      "loss": 3.5503,
      "step": 23800
    },
    {
      "epoch": 1.5348478023038805,
      "grad_norm": 1.2992479801177979,
      "learning_rate": 4.6515219769611944e-05,
      "loss": 3.5674,
      "step": 23850
    },
    {
      "epoch": 1.5380655125812472,
      "grad_norm": 1.3462443351745605,
      "learning_rate": 4.6193448741875286e-05,
      "loss": 3.5589,
      "step": 23900
    },
    {
      "epoch": 1.5412832228586137,
      "grad_norm": 1.3108569383621216,
      "learning_rate": 4.587167771413862e-05,
      "loss": 3.5578,
      "step": 23950
    },
    {
      "epoch": 1.5445009331359805,
      "grad_norm": 1.3113068342208862,
      "learning_rate": 4.5549906686401964e-05,
      "loss": 3.5607,
      "step": 24000
    },
    {
      "epoch": 1.5477186434133472,
      "grad_norm": 1.2990683317184448,
      "learning_rate": 4.522813565866529e-05,
      "loss": 3.5538,
      "step": 24050
    },
    {
      "epoch": 1.5509363536907137,
      "grad_norm": 1.2906781435012817,
      "learning_rate": 4.4906364630928634e-05,
      "loss": 3.5199,
      "step": 24100
    },
    {
      "epoch": 1.5541540639680802,
      "grad_norm": 1.207482933998108,
      "learning_rate": 4.458459360319197e-05,
      "loss": 3.5062,
      "step": 24150
    },
    {
      "epoch": 1.557371774245447,
      "grad_norm": 1.4329605102539062,
      "learning_rate": 4.426282257545531e-05,
      "loss": 3.5521,
      "step": 24200
    },
    {
      "epoch": 1.5605894845228137,
      "grad_norm": 1.2494091987609863,
      "learning_rate": 4.394105154771865e-05,
      "loss": 3.538,
      "step": 24250
    },
    {
      "epoch": 1.5638071948001802,
      "grad_norm": 1.3294849395751953,
      "learning_rate": 4.361928051998198e-05,
      "loss": 3.5265,
      "step": 24300
    },
    {
      "epoch": 1.5670249050775467,
      "grad_norm": 1.2724316120147705,
      "learning_rate": 4.329750949224532e-05,
      "loss": 3.5471,
      "step": 24350
    },
    {
      "epoch": 1.5702426153549136,
      "grad_norm": 1.3044368028640747,
      "learning_rate": 4.297573846450866e-05,
      "loss": 3.5572,
      "step": 24400
    },
    {
      "epoch": 1.57346032563228,
      "grad_norm": 1.2921497821807861,
      "learning_rate": 4.2653967436771995e-05,
      "loss": 3.5535,
      "step": 24450
    },
    {
      "epoch": 1.5766780359096466,
      "grad_norm": 1.2632689476013184,
      "learning_rate": 4.233219640903534e-05,
      "loss": 3.5582,
      "step": 24500
    },
    {
      "epoch": 1.5798957461870133,
      "grad_norm": 1.2083771228790283,
      "learning_rate": 4.2010425381298666e-05,
      "loss": 3.5629,
      "step": 24550
    },
    {
      "epoch": 1.58311345646438,
      "grad_norm": 1.3137965202331543,
      "learning_rate": 4.168865435356201e-05,
      "loss": 3.5467,
      "step": 24600
    },
    {
      "epoch": 1.5863311667417466,
      "grad_norm": 1.2185159921646118,
      "learning_rate": 4.1366883325825344e-05,
      "loss": 3.5436,
      "step": 24650
    },
    {
      "epoch": 1.589548877019113,
      "grad_norm": 1.3205684423446655,
      "learning_rate": 4.1045112298088686e-05,
      "loss": 3.5584,
      "step": 24700
    },
    {
      "epoch": 1.5927665872964798,
      "grad_norm": 1.278504490852356,
      "learning_rate": 4.0723341270352014e-05,
      "loss": 3.5449,
      "step": 24750
    },
    {
      "epoch": 1.5959842975738465,
      "grad_norm": 1.2913204431533813,
      "learning_rate": 4.0401570242615357e-05,
      "loss": 3.5601,
      "step": 24800
    },
    {
      "epoch": 1.599202007851213,
      "grad_norm": 1.3165305852890015,
      "learning_rate": 4.007979921487869e-05,
      "loss": 3.5214,
      "step": 24850
    },
    {
      "epoch": 1.6024197181285798,
      "grad_norm": 1.3022760152816772,
      "learning_rate": 3.9758028187142034e-05,
      "loss": 3.5303,
      "step": 24900
    },
    {
      "epoch": 1.6056374284059465,
      "grad_norm": 1.4196101427078247,
      "learning_rate": 3.943625715940537e-05,
      "loss": 3.5638,
      "step": 24950
    },
    {
      "epoch": 1.608855138683313,
      "grad_norm": 1.2809020280838013,
      "learning_rate": 3.9114486131668705e-05,
      "loss": 3.5486,
      "step": 25000
    },
    {
      "epoch": 1.608855138683313,
      "eval_loss": 3.6570425033569336,
      "eval_runtime": 263.6125,
      "eval_samples_per_second": 237.212,
      "eval_steps_per_second": 14.829,
      "step": 25000
    },
    {
      "epoch": 1.6120728489606795,
      "grad_norm": 1.3047252893447876,
      "learning_rate": 3.879271510393204e-05,
      "loss": 3.5622,
      "step": 25050
    },
    {
      "epoch": 1.6152905592380462,
      "grad_norm": 1.367287516593933,
      "learning_rate": 3.847094407619538e-05,
      "loss": 3.5302,
      "step": 25100
    },
    {
      "epoch": 1.618508269515413,
      "grad_norm": 1.2290983200073242,
      "learning_rate": 3.814917304845872e-05,
      "loss": 3.5544,
      "step": 25150
    },
    {
      "epoch": 1.6217259797927794,
      "grad_norm": 1.2848049402236938,
      "learning_rate": 3.782740202072206e-05,
      "loss": 3.5507,
      "step": 25200
    },
    {
      "epoch": 1.624943690070146,
      "grad_norm": 1.2419370412826538,
      "learning_rate": 3.750563099298539e-05,
      "loss": 3.5552,
      "step": 25250
    },
    {
      "epoch": 1.6281614003475127,
      "grad_norm": 1.2188129425048828,
      "learning_rate": 3.718385996524873e-05,
      "loss": 3.5155,
      "step": 25300
    },
    {
      "epoch": 1.6313791106248794,
      "grad_norm": 1.2972184419631958,
      "learning_rate": 3.6862088937512066e-05,
      "loss": 3.556,
      "step": 25350
    },
    {
      "epoch": 1.634596820902246,
      "grad_norm": 1.2858712673187256,
      "learning_rate": 3.654031790977541e-05,
      "loss": 3.533,
      "step": 25400
    },
    {
      "epoch": 1.6378145311796126,
      "grad_norm": 1.3090189695358276,
      "learning_rate": 3.6218546882038743e-05,
      "loss": 3.5299,
      "step": 25450
    },
    {
      "epoch": 1.6410322414569793,
      "grad_norm": 1.3037724494934082,
      "learning_rate": 3.589677585430208e-05,
      "loss": 3.5814,
      "step": 25500
    },
    {
      "epoch": 1.6442499517343458,
      "grad_norm": 1.2827953100204468,
      "learning_rate": 3.5575004826565414e-05,
      "loss": 3.5589,
      "step": 25550
    },
    {
      "epoch": 1.6474676620117124,
      "grad_norm": 1.2648186683654785,
      "learning_rate": 3.5253233798828756e-05,
      "loss": 3.59,
      "step": 25600
    },
    {
      "epoch": 1.650685372289079,
      "grad_norm": 1.2501070499420166,
      "learning_rate": 3.493146277109209e-05,
      "loss": 3.5544,
      "step": 25650
    },
    {
      "epoch": 1.6539030825664458,
      "grad_norm": 1.2651269435882568,
      "learning_rate": 3.4609691743355434e-05,
      "loss": 3.5315,
      "step": 25700
    },
    {
      "epoch": 1.6571207928438123,
      "grad_norm": 1.2369126081466675,
      "learning_rate": 3.428792071561876e-05,
      "loss": 3.565,
      "step": 25750
    },
    {
      "epoch": 1.6603385031211788,
      "grad_norm": 1.2972533702850342,
      "learning_rate": 3.3966149687882105e-05,
      "loss": 3.5512,
      "step": 25800
    },
    {
      "epoch": 1.6635562133985458,
      "grad_norm": 1.2019857168197632,
      "learning_rate": 3.364437866014544e-05,
      "loss": 3.5415,
      "step": 25850
    },
    {
      "epoch": 1.6667739236759123,
      "grad_norm": 1.2883903980255127,
      "learning_rate": 3.332260763240878e-05,
      "loss": 3.5485,
      "step": 25900
    },
    {
      "epoch": 1.6699916339532788,
      "grad_norm": 1.2838815450668335,
      "learning_rate": 3.300083660467212e-05,
      "loss": 3.5278,
      "step": 25950
    },
    {
      "epoch": 1.6732093442306455,
      "grad_norm": 1.3640496730804443,
      "learning_rate": 3.267906557693545e-05,
      "loss": 3.5635,
      "step": 26000
    },
    {
      "epoch": 1.6764270545080122,
      "grad_norm": 1.3315682411193848,
      "learning_rate": 3.235729454919879e-05,
      "loss": 3.5449,
      "step": 26050
    },
    {
      "epoch": 1.6796447647853787,
      "grad_norm": 1.2842811346054077,
      "learning_rate": 3.203552352146213e-05,
      "loss": 3.5455,
      "step": 26100
    },
    {
      "epoch": 1.6828624750627452,
      "grad_norm": 1.3724309206008911,
      "learning_rate": 3.1713752493725466e-05,
      "loss": 3.5367,
      "step": 26150
    },
    {
      "epoch": 1.686080185340112,
      "grad_norm": 1.355728268623352,
      "learning_rate": 3.13919814659888e-05,
      "loss": 3.5524,
      "step": 26200
    },
    {
      "epoch": 1.6892978956174787,
      "grad_norm": 1.2778244018554688,
      "learning_rate": 3.1070210438252137e-05,
      "loss": 3.5465,
      "step": 26250
    },
    {
      "epoch": 1.6925156058948452,
      "grad_norm": 1.3491971492767334,
      "learning_rate": 3.074843941051548e-05,
      "loss": 3.5577,
      "step": 26300
    },
    {
      "epoch": 1.695733316172212,
      "grad_norm": 1.249587893486023,
      "learning_rate": 3.0426668382778817e-05,
      "loss": 3.5514,
      "step": 26350
    },
    {
      "epoch": 1.6989510264495786,
      "grad_norm": 1.220038890838623,
      "learning_rate": 3.0104897355042156e-05,
      "loss": 3.5382,
      "step": 26400
    },
    {
      "epoch": 1.7021687367269451,
      "grad_norm": 1.1951185464859009,
      "learning_rate": 2.9783126327305488e-05,
      "loss": 3.5525,
      "step": 26450
    },
    {
      "epoch": 1.7053864470043116,
      "grad_norm": 1.2447224855422974,
      "learning_rate": 2.9461355299568827e-05,
      "loss": 3.4941,
      "step": 26500
    },
    {
      "epoch": 1.7086041572816784,
      "grad_norm": 1.272858738899231,
      "learning_rate": 2.9139584271832166e-05,
      "loss": 3.51,
      "step": 26550
    },
    {
      "epoch": 1.711821867559045,
      "grad_norm": 1.2863917350769043,
      "learning_rate": 2.8817813244095504e-05,
      "loss": 3.5416,
      "step": 26600
    },
    {
      "epoch": 1.7150395778364116,
      "grad_norm": 1.4259495735168457,
      "learning_rate": 2.8496042216358843e-05,
      "loss": 3.5512,
      "step": 26650
    },
    {
      "epoch": 1.718257288113778,
      "grad_norm": 1.2406082153320312,
      "learning_rate": 2.8174271188622175e-05,
      "loss": 3.5396,
      "step": 26700
    },
    {
      "epoch": 1.7214749983911448,
      "grad_norm": 1.2525285482406616,
      "learning_rate": 2.7852500160885514e-05,
      "loss": 3.568,
      "step": 26750
    },
    {
      "epoch": 1.7246927086685115,
      "grad_norm": 1.260101318359375,
      "learning_rate": 2.7530729133148853e-05,
      "loss": 3.5253,
      "step": 26800
    },
    {
      "epoch": 1.727910418945878,
      "grad_norm": 1.246853232383728,
      "learning_rate": 2.720895810541219e-05,
      "loss": 3.5583,
      "step": 26850
    },
    {
      "epoch": 1.7311281292232448,
      "grad_norm": 1.2904361486434937,
      "learning_rate": 2.688718707767553e-05,
      "loss": 3.5277,
      "step": 26900
    },
    {
      "epoch": 1.7343458395006115,
      "grad_norm": 1.3091943264007568,
      "learning_rate": 2.6565416049938862e-05,
      "loss": 3.5511,
      "step": 26950
    },
    {
      "epoch": 1.737563549777978,
      "grad_norm": 1.2663532495498657,
      "learning_rate": 2.62436450222022e-05,
      "loss": 3.5379,
      "step": 27000
    },
    {
      "epoch": 1.7407812600553445,
      "grad_norm": 1.211729645729065,
      "learning_rate": 2.592187399446554e-05,
      "loss": 3.552,
      "step": 27050
    },
    {
      "epoch": 1.7439989703327112,
      "grad_norm": 1.287627935409546,
      "learning_rate": 2.560010296672888e-05,
      "loss": 3.5467,
      "step": 27100
    },
    {
      "epoch": 1.747216680610078,
      "grad_norm": 1.2840991020202637,
      "learning_rate": 2.5278331938992217e-05,
      "loss": 3.5432,
      "step": 27150
    },
    {
      "epoch": 1.7504343908874445,
      "grad_norm": 1.2757116556167603,
      "learning_rate": 2.4956560911255553e-05,
      "loss": 3.563,
      "step": 27200
    },
    {
      "epoch": 1.753652101164811,
      "grad_norm": 1.2718431949615479,
      "learning_rate": 2.4634789883518888e-05,
      "loss": 3.5545,
      "step": 27250
    },
    {
      "epoch": 1.756869811442178,
      "grad_norm": 1.2515286207199097,
      "learning_rate": 2.4313018855782227e-05,
      "loss": 3.5115,
      "step": 27300
    },
    {
      "epoch": 1.7600875217195444,
      "grad_norm": 1.3773773908615112,
      "learning_rate": 2.3991247828045562e-05,
      "loss": 3.5432,
      "step": 27350
    },
    {
      "epoch": 1.763305231996911,
      "grad_norm": 1.2870670557022095,
      "learning_rate": 2.36694768003089e-05,
      "loss": 3.5241,
      "step": 27400
    },
    {
      "epoch": 1.7665229422742776,
      "grad_norm": 1.2350050210952759,
      "learning_rate": 2.334770577257224e-05,
      "loss": 3.5125,
      "step": 27450
    },
    {
      "epoch": 1.7697406525516444,
      "grad_norm": 1.2884199619293213,
      "learning_rate": 2.3025934744835575e-05,
      "loss": 3.5323,
      "step": 27500
    },
    {
      "epoch": 1.7729583628290109,
      "grad_norm": 1.261075496673584,
      "learning_rate": 2.2704163717098914e-05,
      "loss": 3.5397,
      "step": 27550
    },
    {
      "epoch": 1.7761760731063774,
      "grad_norm": 1.3090267181396484,
      "learning_rate": 2.238239268936225e-05,
      "loss": 3.5382,
      "step": 27600
    },
    {
      "epoch": 1.779393783383744,
      "grad_norm": 1.2327871322631836,
      "learning_rate": 2.2060621661625588e-05,
      "loss": 3.551,
      "step": 27650
    },
    {
      "epoch": 1.7826114936611108,
      "grad_norm": 1.2681137323379517,
      "learning_rate": 2.1738850633888927e-05,
      "loss": 3.5331,
      "step": 27700
    },
    {
      "epoch": 1.7858292039384773,
      "grad_norm": 1.3589391708374023,
      "learning_rate": 2.1417079606152262e-05,
      "loss": 3.5271,
      "step": 27750
    },
    {
      "epoch": 1.789046914215844,
      "grad_norm": 1.2553538084030151,
      "learning_rate": 2.10953085784156e-05,
      "loss": 3.5417,
      "step": 27800
    },
    {
      "epoch": 1.7922646244932108,
      "grad_norm": 1.2997630834579468,
      "learning_rate": 2.0773537550678936e-05,
      "loss": 3.5712,
      "step": 27850
    },
    {
      "epoch": 1.7954823347705773,
      "grad_norm": 1.3229711055755615,
      "learning_rate": 2.0451766522942275e-05,
      "loss": 3.5276,
      "step": 27900
    },
    {
      "epoch": 1.7987000450479438,
      "grad_norm": 1.2619260549545288,
      "learning_rate": 2.012999549520561e-05,
      "loss": 3.5555,
      "step": 27950
    },
    {
      "epoch": 1.8019177553253105,
      "grad_norm": 1.2315177917480469,
      "learning_rate": 1.980822446746895e-05,
      "loss": 3.5158,
      "step": 28000
    },
    {
      "epoch": 1.8051354656026772,
      "grad_norm": 1.2504409551620483,
      "learning_rate": 1.9486453439732288e-05,
      "loss": 3.5236,
      "step": 28050
    },
    {
      "epoch": 1.8083531758800437,
      "grad_norm": 1.2533422708511353,
      "learning_rate": 1.9164682411995623e-05,
      "loss": 3.5578,
      "step": 28100
    },
    {
      "epoch": 1.8115708861574102,
      "grad_norm": 1.3075077533721924,
      "learning_rate": 1.8842911384258962e-05,
      "loss": 3.5277,
      "step": 28150
    },
    {
      "epoch": 1.814788596434777,
      "grad_norm": 1.2095754146575928,
      "learning_rate": 1.8521140356522297e-05,
      "loss": 3.5396,
      "step": 28200
    },
    {
      "epoch": 1.8180063067121437,
      "grad_norm": 1.2893251180648804,
      "learning_rate": 1.8199369328785636e-05,
      "loss": 3.5438,
      "step": 28250
    },
    {
      "epoch": 1.8212240169895102,
      "grad_norm": 1.2990188598632812,
      "learning_rate": 1.7877598301048975e-05,
      "loss": 3.5473,
      "step": 28300
    },
    {
      "epoch": 1.824441727266877,
      "grad_norm": 1.3312139511108398,
      "learning_rate": 1.755582727331231e-05,
      "loss": 3.5287,
      "step": 28350
    },
    {
      "epoch": 1.8276594375442436,
      "grad_norm": 1.2772151231765747,
      "learning_rate": 1.723405624557565e-05,
      "loss": 3.5646,
      "step": 28400
    },
    {
      "epoch": 1.8308771478216102,
      "grad_norm": 1.2128275632858276,
      "learning_rate": 1.6912285217838984e-05,
      "loss": 3.5425,
      "step": 28450
    },
    {
      "epoch": 1.8340948580989767,
      "grad_norm": 1.304142713546753,
      "learning_rate": 1.6590514190102323e-05,
      "loss": 3.5377,
      "step": 28500
    },
    {
      "epoch": 1.8373125683763434,
      "grad_norm": 1.330354928970337,
      "learning_rate": 1.6268743162365662e-05,
      "loss": 3.5136,
      "step": 28550
    },
    {
      "epoch": 1.84053027865371,
      "grad_norm": 1.2991867065429688,
      "learning_rate": 1.5946972134628997e-05,
      "loss": 3.5398,
      "step": 28600
    },
    {
      "epoch": 1.8437479889310766,
      "grad_norm": 1.2988240718841553,
      "learning_rate": 1.5625201106892336e-05,
      "loss": 3.5441,
      "step": 28650
    },
    {
      "epoch": 1.8469656992084431,
      "grad_norm": 1.3023701906204224,
      "learning_rate": 1.530343007915567e-05,
      "loss": 3.5348,
      "step": 28700
    },
    {
      "epoch": 1.85018340948581,
      "grad_norm": 1.1993770599365234,
      "learning_rate": 1.4981659051419012e-05,
      "loss": 3.5523,
      "step": 28750
    },
    {
      "epoch": 1.8534011197631766,
      "grad_norm": 1.293747067451477,
      "learning_rate": 1.4659888023682347e-05,
      "loss": 3.537,
      "step": 28800
    },
    {
      "epoch": 1.856618830040543,
      "grad_norm": 1.2770278453826904,
      "learning_rate": 1.4338116995945686e-05,
      "loss": 3.5281,
      "step": 28850
    },
    {
      "epoch": 1.8598365403179098,
      "grad_norm": 1.3115310668945312,
      "learning_rate": 1.4016345968209025e-05,
      "loss": 3.544,
      "step": 28900
    },
    {
      "epoch": 1.8630542505952765,
      "grad_norm": 1.2634989023208618,
      "learning_rate": 1.369457494047236e-05,
      "loss": 3.5321,
      "step": 28950
    },
    {
      "epoch": 1.866271960872643,
      "grad_norm": 1.3136500120162964,
      "learning_rate": 1.3372803912735699e-05,
      "loss": 3.5419,
      "step": 29000
    },
    {
      "epoch": 1.8694896711500095,
      "grad_norm": 1.200295329093933,
      "learning_rate": 1.3051032884999034e-05,
      "loss": 3.5256,
      "step": 29050
    },
    {
      "epoch": 1.8727073814273762,
      "grad_norm": 1.2246503829956055,
      "learning_rate": 1.2729261857262373e-05,
      "loss": 3.5141,
      "step": 29100
    },
    {
      "epoch": 1.875925091704743,
      "grad_norm": 1.2869391441345215,
      "learning_rate": 1.240749082952571e-05,
      "loss": 3.5357,
      "step": 29150
    },
    {
      "epoch": 1.8791428019821095,
      "grad_norm": 1.251574158668518,
      "learning_rate": 1.2085719801789047e-05,
      "loss": 3.5218,
      "step": 29200
    },
    {
      "epoch": 1.8823605122594762,
      "grad_norm": 1.292765498161316,
      "learning_rate": 1.1763948774052384e-05,
      "loss": 3.5416,
      "step": 29250
    },
    {
      "epoch": 1.885578222536843,
      "grad_norm": 1.3382939100265503,
      "learning_rate": 1.1442177746315723e-05,
      "loss": 3.5408,
      "step": 29300
    },
    {
      "epoch": 1.8887959328142094,
      "grad_norm": 1.2381967306137085,
      "learning_rate": 1.112040671857906e-05,
      "loss": 3.5222,
      "step": 29350
    },
    {
      "epoch": 1.892013643091576,
      "grad_norm": 1.2764540910720825,
      "learning_rate": 1.0798635690842397e-05,
      "loss": 3.5276,
      "step": 29400
    },
    {
      "epoch": 1.8952313533689427,
      "grad_norm": 1.267702341079712,
      "learning_rate": 1.0476864663105734e-05,
      "loss": 3.489,
      "step": 29450
    },
    {
      "epoch": 1.8984490636463094,
      "grad_norm": 1.2875975370407104,
      "learning_rate": 1.0155093635369071e-05,
      "loss": 3.517,
      "step": 29500
    },
    {
      "epoch": 1.901666773923676,
      "grad_norm": 1.285875916481018,
      "learning_rate": 9.833322607632408e-06,
      "loss": 3.5562,
      "step": 29550
    },
    {
      "epoch": 1.9048844842010424,
      "grad_norm": 1.278387427330017,
      "learning_rate": 9.511551579895747e-06,
      "loss": 3.5298,
      "step": 29600
    },
    {
      "epoch": 1.9081021944784091,
      "grad_norm": 1.2457938194274902,
      "learning_rate": 9.189780552159084e-06,
      "loss": 3.5243,
      "step": 29650
    },
    {
      "epoch": 1.9113199047557758,
      "grad_norm": 1.274623155593872,
      "learning_rate": 8.868009524422421e-06,
      "loss": 3.5195,
      "step": 29700
    },
    {
      "epoch": 1.9145376150331423,
      "grad_norm": 1.2131880521774292,
      "learning_rate": 8.546238496685758e-06,
      "loss": 3.5183,
      "step": 29750
    },
    {
      "epoch": 1.917755325310509,
      "grad_norm": 1.2955363988876343,
      "learning_rate": 8.224467468949095e-06,
      "loss": 3.5264,
      "step": 29800
    },
    {
      "epoch": 1.9209730355878758,
      "grad_norm": 1.264331340789795,
      "learning_rate": 7.902696441212432e-06,
      "loss": 3.5429,
      "step": 29850
    },
    {
      "epoch": 1.9241907458652423,
      "grad_norm": 1.1989474296569824,
      "learning_rate": 7.580925413475771e-06,
      "loss": 3.5083,
      "step": 29900
    },
    {
      "epoch": 1.9274084561426088,
      "grad_norm": 1.2672398090362549,
      "learning_rate": 7.259154385739109e-06,
      "loss": 3.5366,
      "step": 29950
    },
    {
      "epoch": 1.9306261664199755,
      "grad_norm": 1.2762906551361084,
      "learning_rate": 6.937383358002446e-06,
      "loss": 3.5239,
      "step": 30000
    },
    {
      "epoch": 1.9306261664199755,
      "eval_loss": 3.638216972351074,
      "eval_runtime": 263.5495,
      "eval_samples_per_second": 237.268,
      "eval_steps_per_second": 14.832,
      "step": 30000
    }
  ],
  "logging_steps": 50,
  "max_steps": 31078,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 5000,
  "total_flos": 1.6282778259632947e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
