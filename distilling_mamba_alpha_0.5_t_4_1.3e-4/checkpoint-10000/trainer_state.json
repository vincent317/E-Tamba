{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8044405116241654,
  "eval_steps": 20000,
  "global_step": 10000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004022202558120827,
      "grad_norm": 3.3023500442504883,
      "learning_rate": 0.0001294771136674443,
      "loss": 8.1025,
      "step": 50
    },
    {
      "epoch": 0.008044405116241654,
      "grad_norm": 2.637528419494629,
      "learning_rate": 0.00012895422733488857,
      "loss": 4.2325,
      "step": 100
    },
    {
      "epoch": 0.01206660767436248,
      "grad_norm": 2.252962589263916,
      "learning_rate": 0.00012843134100233287,
      "loss": 3.6198,
      "step": 150
    },
    {
      "epoch": 0.01608881023248331,
      "grad_norm": 2.0023462772369385,
      "learning_rate": 0.00012790845466977715,
      "loss": 3.3855,
      "step": 200
    },
    {
      "epoch": 0.020111012790604135,
      "grad_norm": 2.2038733959198,
      "learning_rate": 0.00012738556833722146,
      "loss": 3.2235,
      "step": 250
    },
    {
      "epoch": 0.02413321534872496,
      "grad_norm": 1.7735941410064697,
      "learning_rate": 0.00012686268200466576,
      "loss": 3.0905,
      "step": 300
    },
    {
      "epoch": 0.028155417906845788,
      "grad_norm": 4.013556480407715,
      "learning_rate": 0.00012633979567211004,
      "loss": 2.9811,
      "step": 350
    },
    {
      "epoch": 0.03217762046496662,
      "grad_norm": 2.569406747817993,
      "learning_rate": 0.00012581690933955432,
      "loss": 2.9015,
      "step": 400
    },
    {
      "epoch": 0.03619982302308744,
      "grad_norm": 1.4092061519622803,
      "learning_rate": 0.00012529402300699862,
      "loss": 2.8108,
      "step": 450
    },
    {
      "epoch": 0.04022202558120827,
      "grad_norm": 1.5542864799499512,
      "learning_rate": 0.00012477113667444292,
      "loss": 2.7582,
      "step": 500
    },
    {
      "epoch": 0.04424422813932909,
      "grad_norm": 7.7945990562438965,
      "learning_rate": 0.0001242482503418872,
      "loss": 2.706,
      "step": 550
    },
    {
      "epoch": 0.04826643069744992,
      "grad_norm": 1.5563784837722778,
      "learning_rate": 0.0001237253640093315,
      "loss": 2.6582,
      "step": 600
    },
    {
      "epoch": 0.05228863325557075,
      "grad_norm": 1.4099286794662476,
      "learning_rate": 0.00012320247767677578,
      "loss": 2.6366,
      "step": 650
    },
    {
      "epoch": 0.056310835813691576,
      "grad_norm": 1.572760820388794,
      "learning_rate": 0.0001226795913442201,
      "loss": 2.598,
      "step": 700
    },
    {
      "epoch": 0.060333038371812406,
      "grad_norm": 1.4766918420791626,
      "learning_rate": 0.0001221567050116644,
      "loss": 2.5755,
      "step": 750
    },
    {
      "epoch": 0.06435524092993324,
      "grad_norm": 1.5935109853744507,
      "learning_rate": 0.00012163381867910867,
      "loss": 2.5567,
      "step": 800
    },
    {
      "epoch": 0.06837744348805405,
      "grad_norm": 1.6903096437454224,
      "learning_rate": 0.00012111093234655296,
      "loss": 2.5166,
      "step": 850
    },
    {
      "epoch": 0.07239964604617488,
      "grad_norm": 1.3658119440078735,
      "learning_rate": 0.00012058804601399725,
      "loss": 2.49,
      "step": 900
    },
    {
      "epoch": 0.07642184860429571,
      "grad_norm": 1.4478133916854858,
      "learning_rate": 0.00012006515968144155,
      "loss": 2.4729,
      "step": 950
    },
    {
      "epoch": 0.08044405116241654,
      "grad_norm": 1.5631364583969116,
      "learning_rate": 0.00011954227334888585,
      "loss": 2.4699,
      "step": 1000
    },
    {
      "epoch": 0.08446625372053737,
      "grad_norm": 1.1974740028381348,
      "learning_rate": 0.00011901938701633012,
      "loss": 2.4598,
      "step": 1050
    },
    {
      "epoch": 0.08848845627865819,
      "grad_norm": 1.304477334022522,
      "learning_rate": 0.00011849650068377443,
      "loss": 2.4434,
      "step": 1100
    },
    {
      "epoch": 0.09251065883677902,
      "grad_norm": 1.277292013168335,
      "learning_rate": 0.00011797361435121872,
      "loss": 2.4296,
      "step": 1150
    },
    {
      "epoch": 0.09653286139489985,
      "grad_norm": 1.1822142601013184,
      "learning_rate": 0.00011745072801866301,
      "loss": 2.4295,
      "step": 1200
    },
    {
      "epoch": 0.10055506395302068,
      "grad_norm": 1.1566431522369385,
      "learning_rate": 0.0001169278416861073,
      "loss": 2.4088,
      "step": 1250
    },
    {
      "epoch": 0.1045772665111415,
      "grad_norm": 1.3570786714553833,
      "learning_rate": 0.00011640495535355159,
      "loss": 2.3962,
      "step": 1300
    },
    {
      "epoch": 0.10859946906926232,
      "grad_norm": 1.1071287393569946,
      "learning_rate": 0.00011588206902099588,
      "loss": 2.3942,
      "step": 1350
    },
    {
      "epoch": 0.11262167162738315,
      "grad_norm": 1.3656704425811768,
      "learning_rate": 0.00011535918268844019,
      "loss": 2.3847,
      "step": 1400
    },
    {
      "epoch": 0.11664387418550398,
      "grad_norm": 1.1248879432678223,
      "learning_rate": 0.00011483629635588446,
      "loss": 2.3644,
      "step": 1450
    },
    {
      "epoch": 0.12066607674362481,
      "grad_norm": 1.203421950340271,
      "learning_rate": 0.00011431341002332877,
      "loss": 2.3501,
      "step": 1500
    },
    {
      "epoch": 0.12468827930174564,
      "grad_norm": 1.1559522151947021,
      "learning_rate": 0.00011379052369077306,
      "loss": 2.3559,
      "step": 1550
    },
    {
      "epoch": 0.12871048185986647,
      "grad_norm": 1.3298025131225586,
      "learning_rate": 0.00011326763735821735,
      "loss": 2.3505,
      "step": 1600
    },
    {
      "epoch": 0.1327326844179873,
      "grad_norm": 1.0712151527404785,
      "learning_rate": 0.00011274475102566165,
      "loss": 2.3411,
      "step": 1650
    },
    {
      "epoch": 0.1367548869761081,
      "grad_norm": 1.1762127876281738,
      "learning_rate": 0.00011222186469310593,
      "loss": 2.3274,
      "step": 1700
    },
    {
      "epoch": 0.14077708953422893,
      "grad_norm": 1.1289801597595215,
      "learning_rate": 0.00011169897836055022,
      "loss": 2.3215,
      "step": 1750
    },
    {
      "epoch": 0.14479929209234976,
      "grad_norm": 1.1998076438903809,
      "learning_rate": 0.00011117609202799453,
      "loss": 2.327,
      "step": 1800
    },
    {
      "epoch": 0.1488214946504706,
      "grad_norm": 1.1116183996200562,
      "learning_rate": 0.00011065320569543882,
      "loss": 2.3307,
      "step": 1850
    },
    {
      "epoch": 0.15284369720859142,
      "grad_norm": 1.2091175317764282,
      "learning_rate": 0.0001101303193628831,
      "loss": 2.31,
      "step": 1900
    },
    {
      "epoch": 0.15686589976671225,
      "grad_norm": 1.10642409324646,
      "learning_rate": 0.0001096074330303274,
      "loss": 2.3189,
      "step": 1950
    },
    {
      "epoch": 0.16088810232483308,
      "grad_norm": 1.0921801328659058,
      "learning_rate": 0.00010908454669777169,
      "loss": 2.2959,
      "step": 2000
    },
    {
      "epoch": 0.1649103048829539,
      "grad_norm": 1.0263155698776245,
      "learning_rate": 0.00010856166036521598,
      "loss": 2.2949,
      "step": 2050
    },
    {
      "epoch": 0.16893250744107474,
      "grad_norm": 1.0524482727050781,
      "learning_rate": 0.00010803877403266028,
      "loss": 2.2946,
      "step": 2100
    },
    {
      "epoch": 0.17295470999919557,
      "grad_norm": 1.0455615520477295,
      "learning_rate": 0.00010751588770010456,
      "loss": 2.2884,
      "step": 2150
    },
    {
      "epoch": 0.17697691255731637,
      "grad_norm": 1.0112613439559937,
      "learning_rate": 0.00010699300136754885,
      "loss": 2.2781,
      "step": 2200
    },
    {
      "epoch": 0.1809991151154372,
      "grad_norm": 1.1429574489593506,
      "learning_rate": 0.00010647011503499316,
      "loss": 2.2966,
      "step": 2250
    },
    {
      "epoch": 0.18502131767355803,
      "grad_norm": 1.0632085800170898,
      "learning_rate": 0.00010594722870243745,
      "loss": 2.2778,
      "step": 2300
    },
    {
      "epoch": 0.18904352023167886,
      "grad_norm": 2.326152801513672,
      "learning_rate": 0.00010542434236988173,
      "loss": 2.2899,
      "step": 2350
    },
    {
      "epoch": 0.1930657227897997,
      "grad_norm": 1.0596238374710083,
      "learning_rate": 0.00010490145603732603,
      "loss": 2.2732,
      "step": 2400
    },
    {
      "epoch": 0.19708792534792052,
      "grad_norm": 1.1641870737075806,
      "learning_rate": 0.00010437856970477032,
      "loss": 2.2736,
      "step": 2450
    },
    {
      "epoch": 0.20111012790604135,
      "grad_norm": 1.0899361371994019,
      "learning_rate": 0.00010385568337221462,
      "loss": 2.2651,
      "step": 2500
    },
    {
      "epoch": 0.20513233046416218,
      "grad_norm": 1.228621482849121,
      "learning_rate": 0.00010333279703965892,
      "loss": 2.2644,
      "step": 2550
    },
    {
      "epoch": 0.209154533022283,
      "grad_norm": 1.1953600645065308,
      "learning_rate": 0.00010280991070710319,
      "loss": 2.2517,
      "step": 2600
    },
    {
      "epoch": 0.21317673558040384,
      "grad_norm": 1.0391244888305664,
      "learning_rate": 0.0001022870243745475,
      "loss": 2.2621,
      "step": 2650
    },
    {
      "epoch": 0.21719893813852464,
      "grad_norm": 1.0364060401916504,
      "learning_rate": 0.00010176413804199179,
      "loss": 2.2542,
      "step": 2700
    },
    {
      "epoch": 0.22122114069664547,
      "grad_norm": 1.1058675050735474,
      "learning_rate": 0.00010124125170943608,
      "loss": 2.2531,
      "step": 2750
    },
    {
      "epoch": 0.2252433432547663,
      "grad_norm": 1.0456746816635132,
      "learning_rate": 0.00010071836537688037,
      "loss": 2.2468,
      "step": 2800
    },
    {
      "epoch": 0.22926554581288713,
      "grad_norm": 1.1862246990203857,
      "learning_rate": 0.00010019547904432466,
      "loss": 2.2503,
      "step": 2850
    },
    {
      "epoch": 0.23328774837100796,
      "grad_norm": 1.012621521949768,
      "learning_rate": 9.967259271176895e-05,
      "loss": 2.2532,
      "step": 2900
    },
    {
      "epoch": 0.2373099509291288,
      "grad_norm": 1.1268526315689087,
      "learning_rate": 9.914970637921326e-05,
      "loss": 2.2448,
      "step": 2950
    },
    {
      "epoch": 0.24133215348724962,
      "grad_norm": 1.1375298500061035,
      "learning_rate": 9.862682004665755e-05,
      "loss": 2.2415,
      "step": 3000
    },
    {
      "epoch": 0.24535435604537045,
      "grad_norm": 0.9819005131721497,
      "learning_rate": 9.810393371410182e-05,
      "loss": 2.2188,
      "step": 3050
    },
    {
      "epoch": 0.24937655860349128,
      "grad_norm": 0.9961263537406921,
      "learning_rate": 9.758104738154613e-05,
      "loss": 2.2296,
      "step": 3100
    },
    {
      "epoch": 0.2533987611616121,
      "grad_norm": 1.0252803564071655,
      "learning_rate": 9.705816104899042e-05,
      "loss": 2.231,
      "step": 3150
    },
    {
      "epoch": 0.25742096371973294,
      "grad_norm": 0.987392246723175,
      "learning_rate": 9.653527471643471e-05,
      "loss": 2.2345,
      "step": 3200
    },
    {
      "epoch": 0.26144316627785374,
      "grad_norm": 1.0581119060516357,
      "learning_rate": 9.6012388383879e-05,
      "loss": 2.23,
      "step": 3250
    },
    {
      "epoch": 0.2654653688359746,
      "grad_norm": 0.9907195568084717,
      "learning_rate": 9.548950205132329e-05,
      "loss": 2.2344,
      "step": 3300
    },
    {
      "epoch": 0.2694875713940954,
      "grad_norm": 1.001019835472107,
      "learning_rate": 9.49666157187676e-05,
      "loss": 2.2152,
      "step": 3350
    },
    {
      "epoch": 0.2735097739522162,
      "grad_norm": 0.9666115045547485,
      "learning_rate": 9.444372938621189e-05,
      "loss": 2.2206,
      "step": 3400
    },
    {
      "epoch": 0.27753197651033706,
      "grad_norm": 0.9297523498535156,
      "learning_rate": 9.392084305365618e-05,
      "loss": 2.2149,
      "step": 3450
    },
    {
      "epoch": 0.28155417906845787,
      "grad_norm": 1.0965352058410645,
      "learning_rate": 9.339795672110047e-05,
      "loss": 2.208,
      "step": 3500
    },
    {
      "epoch": 0.2855763816265787,
      "grad_norm": 1.0600225925445557,
      "learning_rate": 9.287507038854476e-05,
      "loss": 2.2106,
      "step": 3550
    },
    {
      "epoch": 0.2895985841846995,
      "grad_norm": 1.0122445821762085,
      "learning_rate": 9.235218405598905e-05,
      "loss": 2.2224,
      "step": 3600
    },
    {
      "epoch": 0.2936207867428204,
      "grad_norm": 0.928297758102417,
      "learning_rate": 9.182929772343335e-05,
      "loss": 2.1972,
      "step": 3650
    },
    {
      "epoch": 0.2976429893009412,
      "grad_norm": 1.0775258541107178,
      "learning_rate": 9.130641139087763e-05,
      "loss": 2.1987,
      "step": 3700
    },
    {
      "epoch": 0.30166519185906204,
      "grad_norm": 1.0209004878997803,
      "learning_rate": 9.078352505832192e-05,
      "loss": 2.2077,
      "step": 3750
    },
    {
      "epoch": 0.30568739441718284,
      "grad_norm": 1.0448936223983765,
      "learning_rate": 9.026063872576623e-05,
      "loss": 2.1938,
      "step": 3800
    },
    {
      "epoch": 0.3097095969753037,
      "grad_norm": 1.0175206661224365,
      "learning_rate": 8.973775239321052e-05,
      "loss": 2.2009,
      "step": 3850
    },
    {
      "epoch": 0.3137317995334245,
      "grad_norm": 1.0027189254760742,
      "learning_rate": 8.921486606065481e-05,
      "loss": 2.2024,
      "step": 3900
    },
    {
      "epoch": 0.3177540020915453,
      "grad_norm": 0.996286928653717,
      "learning_rate": 8.86919797280991e-05,
      "loss": 2.2004,
      "step": 3950
    },
    {
      "epoch": 0.32177620464966616,
      "grad_norm": 0.9086203575134277,
      "learning_rate": 8.816909339554339e-05,
      "loss": 2.1881,
      "step": 4000
    },
    {
      "epoch": 0.32579840720778697,
      "grad_norm": 0.9190940260887146,
      "learning_rate": 8.764620706298768e-05,
      "loss": 2.1847,
      "step": 4050
    },
    {
      "epoch": 0.3298206097659078,
      "grad_norm": 0.9179061055183411,
      "learning_rate": 8.712332073043199e-05,
      "loss": 2.1919,
      "step": 4100
    },
    {
      "epoch": 0.3338428123240286,
      "grad_norm": 0.9599146842956543,
      "learning_rate": 8.660043439787626e-05,
      "loss": 2.1877,
      "step": 4150
    },
    {
      "epoch": 0.3378650148821495,
      "grad_norm": 1.1294949054718018,
      "learning_rate": 8.607754806532055e-05,
      "loss": 2.1822,
      "step": 4200
    },
    {
      "epoch": 0.3418872174402703,
      "grad_norm": 0.9144675135612488,
      "learning_rate": 8.555466173276486e-05,
      "loss": 2.1775,
      "step": 4250
    },
    {
      "epoch": 0.34590941999839114,
      "grad_norm": 0.9624238610267639,
      "learning_rate": 8.503177540020915e-05,
      "loss": 2.1698,
      "step": 4300
    },
    {
      "epoch": 0.34993162255651195,
      "grad_norm": 1.0250585079193115,
      "learning_rate": 8.450888906765343e-05,
      "loss": 2.1796,
      "step": 4350
    },
    {
      "epoch": 0.35395382511463275,
      "grad_norm": 0.9244903922080994,
      "learning_rate": 8.398600273509773e-05,
      "loss": 2.19,
      "step": 4400
    },
    {
      "epoch": 0.3579760276727536,
      "grad_norm": 1.1166059970855713,
      "learning_rate": 8.346311640254202e-05,
      "loss": 2.176,
      "step": 4450
    },
    {
      "epoch": 0.3619982302308744,
      "grad_norm": 0.9269395470619202,
      "learning_rate": 8.294023006998633e-05,
      "loss": 2.1816,
      "step": 4500
    },
    {
      "epoch": 0.36602043278899526,
      "grad_norm": 0.8989355564117432,
      "learning_rate": 8.241734373743062e-05,
      "loss": 2.1849,
      "step": 4550
    },
    {
      "epoch": 0.37004263534711607,
      "grad_norm": 0.8544034361839294,
      "learning_rate": 8.18944574048749e-05,
      "loss": 2.1755,
      "step": 4600
    },
    {
      "epoch": 0.3740648379052369,
      "grad_norm": 0.9847265481948853,
      "learning_rate": 8.13715710723192e-05,
      "loss": 2.1722,
      "step": 4650
    },
    {
      "epoch": 0.3780870404633577,
      "grad_norm": 0.9149718284606934,
      "learning_rate": 8.084868473976349e-05,
      "loss": 2.175,
      "step": 4700
    },
    {
      "epoch": 0.3821092430214786,
      "grad_norm": 0.9103447794914246,
      "learning_rate": 8.032579840720778e-05,
      "loss": 2.1786,
      "step": 4750
    },
    {
      "epoch": 0.3861314455795994,
      "grad_norm": 0.9728833436965942,
      "learning_rate": 7.980291207465207e-05,
      "loss": 2.1754,
      "step": 4800
    },
    {
      "epoch": 0.3901536481377202,
      "grad_norm": 0.8829015493392944,
      "learning_rate": 7.928002574209636e-05,
      "loss": 2.169,
      "step": 4850
    },
    {
      "epoch": 0.39417585069584105,
      "grad_norm": 0.9063399434089661,
      "learning_rate": 7.875713940954065e-05,
      "loss": 2.17,
      "step": 4900
    },
    {
      "epoch": 0.39819805325396185,
      "grad_norm": 1.0901614427566528,
      "learning_rate": 7.823425307698496e-05,
      "loss": 2.1707,
      "step": 4950
    },
    {
      "epoch": 0.4022202558120827,
      "grad_norm": 0.9562727212905884,
      "learning_rate": 7.771136674442925e-05,
      "loss": 2.1587,
      "step": 5000
    },
    {
      "epoch": 0.4062424583702035,
      "grad_norm": 0.8401046395301819,
      "learning_rate": 7.718848041187352e-05,
      "loss": 2.1618,
      "step": 5050
    },
    {
      "epoch": 0.41026466092832437,
      "grad_norm": 0.8482632040977478,
      "learning_rate": 7.666559407931783e-05,
      "loss": 2.1724,
      "step": 5100
    },
    {
      "epoch": 0.41428686348644517,
      "grad_norm": 0.8782591223716736,
      "learning_rate": 7.614270774676212e-05,
      "loss": 2.1715,
      "step": 5150
    },
    {
      "epoch": 0.418309066044566,
      "grad_norm": 0.97053462266922,
      "learning_rate": 7.561982141420641e-05,
      "loss": 2.1718,
      "step": 5200
    },
    {
      "epoch": 0.4223312686026868,
      "grad_norm": 0.8564451336860657,
      "learning_rate": 7.50969350816507e-05,
      "loss": 2.1564,
      "step": 5250
    },
    {
      "epoch": 0.4263534711608077,
      "grad_norm": 0.8381989598274231,
      "learning_rate": 7.457404874909499e-05,
      "loss": 2.1575,
      "step": 5300
    },
    {
      "epoch": 0.4303756737189285,
      "grad_norm": 0.9161102771759033,
      "learning_rate": 7.40511624165393e-05,
      "loss": 2.1608,
      "step": 5350
    },
    {
      "epoch": 0.4343978762770493,
      "grad_norm": 0.8915149569511414,
      "learning_rate": 7.352827608398359e-05,
      "loss": 2.1529,
      "step": 5400
    },
    {
      "epoch": 0.43842007883517015,
      "grad_norm": 0.834687352180481,
      "learning_rate": 7.300538975142788e-05,
      "loss": 2.1613,
      "step": 5450
    },
    {
      "epoch": 0.44244228139329095,
      "grad_norm": 0.8853316903114319,
      "learning_rate": 7.248250341887217e-05,
      "loss": 2.1663,
      "step": 5500
    },
    {
      "epoch": 0.4464644839514118,
      "grad_norm": 0.8279590010643005,
      "learning_rate": 7.195961708631646e-05,
      "loss": 2.1513,
      "step": 5550
    },
    {
      "epoch": 0.4504866865095326,
      "grad_norm": 0.9302346706390381,
      "learning_rate": 7.143673075376075e-05,
      "loss": 2.1506,
      "step": 5600
    },
    {
      "epoch": 0.45450888906765347,
      "grad_norm": 0.8954822421073914,
      "learning_rate": 7.091384442120506e-05,
      "loss": 2.1509,
      "step": 5650
    },
    {
      "epoch": 0.45853109162577427,
      "grad_norm": 0.8942301869392395,
      "learning_rate": 7.039095808864933e-05,
      "loss": 2.1493,
      "step": 5700
    },
    {
      "epoch": 0.4625532941838951,
      "grad_norm": 0.9270651340484619,
      "learning_rate": 6.986807175609362e-05,
      "loss": 2.1457,
      "step": 5750
    },
    {
      "epoch": 0.4665754967420159,
      "grad_norm": 1.0195597410202026,
      "learning_rate": 6.934518542353793e-05,
      "loss": 2.1545,
      "step": 5800
    },
    {
      "epoch": 0.47059769930013673,
      "grad_norm": 0.8875053524971008,
      "learning_rate": 6.882229909098222e-05,
      "loss": 2.1441,
      "step": 5850
    },
    {
      "epoch": 0.4746199018582576,
      "grad_norm": 0.8682165145874023,
      "learning_rate": 6.829941275842651e-05,
      "loss": 2.1596,
      "step": 5900
    },
    {
      "epoch": 0.4786421044163784,
      "grad_norm": 0.8761241436004639,
      "learning_rate": 6.77765264258708e-05,
      "loss": 2.1408,
      "step": 5950
    },
    {
      "epoch": 0.48266430697449925,
      "grad_norm": 0.8784369230270386,
      "learning_rate": 6.725364009331509e-05,
      "loss": 2.1479,
      "step": 6000
    },
    {
      "epoch": 0.48668650953262005,
      "grad_norm": 0.8487966656684875,
      "learning_rate": 6.673075376075938e-05,
      "loss": 2.1469,
      "step": 6050
    },
    {
      "epoch": 0.4907087120907409,
      "grad_norm": 0.8022636771202087,
      "learning_rate": 6.620786742820369e-05,
      "loss": 2.1465,
      "step": 6100
    },
    {
      "epoch": 0.4947309146488617,
      "grad_norm": 0.823695719242096,
      "learning_rate": 6.568498109564796e-05,
      "loss": 2.149,
      "step": 6150
    },
    {
      "epoch": 0.49875311720698257,
      "grad_norm": 0.8356454372406006,
      "learning_rate": 6.516209476309225e-05,
      "loss": 2.1412,
      "step": 6200
    },
    {
      "epoch": 0.5027753197651034,
      "grad_norm": 0.8241539597511292,
      "learning_rate": 6.463920843053656e-05,
      "loss": 2.1417,
      "step": 6250
    },
    {
      "epoch": 0.5067975223232242,
      "grad_norm": 1.0024274587631226,
      "learning_rate": 6.411632209798085e-05,
      "loss": 2.1463,
      "step": 6300
    },
    {
      "epoch": 0.510819724881345,
      "grad_norm": 0.8271703124046326,
      "learning_rate": 6.359343576542514e-05,
      "loss": 2.1406,
      "step": 6350
    },
    {
      "epoch": 0.5148419274394659,
      "grad_norm": 0.9107121229171753,
      "learning_rate": 6.307054943286943e-05,
      "loss": 2.1452,
      "step": 6400
    },
    {
      "epoch": 0.5188641299975867,
      "grad_norm": 0.9012697339057922,
      "learning_rate": 6.254766310031372e-05,
      "loss": 2.1394,
      "step": 6450
    },
    {
      "epoch": 0.5228863325557075,
      "grad_norm": 0.8101403117179871,
      "learning_rate": 6.202477676775803e-05,
      "loss": 2.1398,
      "step": 6500
    },
    {
      "epoch": 0.5269085351138283,
      "grad_norm": 0.8299278020858765,
      "learning_rate": 6.15018904352023e-05,
      "loss": 2.1471,
      "step": 6550
    },
    {
      "epoch": 0.5309307376719492,
      "grad_norm": 0.8058769702911377,
      "learning_rate": 6.097900410264661e-05,
      "loss": 2.132,
      "step": 6600
    },
    {
      "epoch": 0.53495294023007,
      "grad_norm": 0.8880935907363892,
      "learning_rate": 6.045611777009089e-05,
      "loss": 2.1359,
      "step": 6650
    },
    {
      "epoch": 0.5389751427881908,
      "grad_norm": 0.891340434551239,
      "learning_rate": 5.993323143753519e-05,
      "loss": 2.1308,
      "step": 6700
    },
    {
      "epoch": 0.5429973453463116,
      "grad_norm": 0.8562505841255188,
      "learning_rate": 5.941034510497948e-05,
      "loss": 2.1354,
      "step": 6750
    },
    {
      "epoch": 0.5470195479044324,
      "grad_norm": 0.8208202719688416,
      "learning_rate": 5.888745877242377e-05,
      "loss": 2.1288,
      "step": 6800
    },
    {
      "epoch": 0.5510417504625533,
      "grad_norm": 0.9225845336914062,
      "learning_rate": 5.836457243986807e-05,
      "loss": 2.1294,
      "step": 6850
    },
    {
      "epoch": 0.5550639530206741,
      "grad_norm": 0.8093341588973999,
      "learning_rate": 5.784168610731236e-05,
      "loss": 2.1404,
      "step": 6900
    },
    {
      "epoch": 0.5590861555787949,
      "grad_norm": 0.8880066275596619,
      "learning_rate": 5.731879977475666e-05,
      "loss": 2.1303,
      "step": 6950
    },
    {
      "epoch": 0.5631083581369157,
      "grad_norm": 0.8782626986503601,
      "learning_rate": 5.679591344220094e-05,
      "loss": 2.1285,
      "step": 7000
    },
    {
      "epoch": 0.5671305606950366,
      "grad_norm": 0.8613048791885376,
      "learning_rate": 5.627302710964524e-05,
      "loss": 2.1271,
      "step": 7050
    },
    {
      "epoch": 0.5711527632531574,
      "grad_norm": 0.870970606803894,
      "learning_rate": 5.575014077708953e-05,
      "loss": 2.1319,
      "step": 7100
    },
    {
      "epoch": 0.5751749658112782,
      "grad_norm": 0.8520379662513733,
      "learning_rate": 5.522725444453382e-05,
      "loss": 2.1285,
      "step": 7150
    },
    {
      "epoch": 0.579197168369399,
      "grad_norm": 0.867788553237915,
      "learning_rate": 5.470436811197811e-05,
      "loss": 2.1304,
      "step": 7200
    },
    {
      "epoch": 0.58321937092752,
      "grad_norm": 0.8711346387863159,
      "learning_rate": 5.418148177942241e-05,
      "loss": 2.1289,
      "step": 7250
    },
    {
      "epoch": 0.5872415734856408,
      "grad_norm": 0.841195821762085,
      "learning_rate": 5.36585954468667e-05,
      "loss": 2.1163,
      "step": 7300
    },
    {
      "epoch": 0.5912637760437616,
      "grad_norm": 0.8163903951644897,
      "learning_rate": 5.313570911431099e-05,
      "loss": 2.1223,
      "step": 7350
    },
    {
      "epoch": 0.5952859786018824,
      "grad_norm": 0.8984071612358093,
      "learning_rate": 5.261282278175529e-05,
      "loss": 2.1299,
      "step": 7400
    },
    {
      "epoch": 0.5993081811600032,
      "grad_norm": 0.8565675020217896,
      "learning_rate": 5.208993644919958e-05,
      "loss": 2.13,
      "step": 7450
    },
    {
      "epoch": 0.6033303837181241,
      "grad_norm": 0.8320642709732056,
      "learning_rate": 5.156705011664387e-05,
      "loss": 2.1189,
      "step": 7500
    },
    {
      "epoch": 0.6073525862762449,
      "grad_norm": 0.9265781044960022,
      "learning_rate": 5.104416378408816e-05,
      "loss": 2.1192,
      "step": 7550
    },
    {
      "epoch": 0.6113747888343657,
      "grad_norm": 0.9069074988365173,
      "learning_rate": 5.052127745153246e-05,
      "loss": 2.1251,
      "step": 7600
    },
    {
      "epoch": 0.6153969913924865,
      "grad_norm": 0.8391423225402832,
      "learning_rate": 4.999839111897674e-05,
      "loss": 2.1204,
      "step": 7650
    },
    {
      "epoch": 0.6194191939506074,
      "grad_norm": 0.8553786277770996,
      "learning_rate": 4.947550478642104e-05,
      "loss": 2.1276,
      "step": 7700
    },
    {
      "epoch": 0.6234413965087282,
      "grad_norm": 0.8420078754425049,
      "learning_rate": 4.895261845386533e-05,
      "loss": 2.1316,
      "step": 7750
    },
    {
      "epoch": 0.627463599066849,
      "grad_norm": 0.7850531339645386,
      "learning_rate": 4.842973212130962e-05,
      "loss": 2.1034,
      "step": 7800
    },
    {
      "epoch": 0.6314858016249698,
      "grad_norm": 0.8962454795837402,
      "learning_rate": 4.790684578875392e-05,
      "loss": 2.1163,
      "step": 7850
    },
    {
      "epoch": 0.6355080041830906,
      "grad_norm": 0.8375434875488281,
      "learning_rate": 4.738395945619821e-05,
      "loss": 2.1171,
      "step": 7900
    },
    {
      "epoch": 0.6395302067412115,
      "grad_norm": 0.9064497947692871,
      "learning_rate": 4.686107312364251e-05,
      "loss": 2.1162,
      "step": 7950
    },
    {
      "epoch": 0.6435524092993323,
      "grad_norm": 0.911891758441925,
      "learning_rate": 4.633818679108679e-05,
      "loss": 2.1209,
      "step": 8000
    },
    {
      "epoch": 0.6475746118574531,
      "grad_norm": 0.8545840382575989,
      "learning_rate": 4.581530045853109e-05,
      "loss": 2.1259,
      "step": 8050
    },
    {
      "epoch": 0.6515968144155739,
      "grad_norm": 0.853543221950531,
      "learning_rate": 4.529241412597538e-05,
      "loss": 2.118,
      "step": 8100
    },
    {
      "epoch": 0.6556190169736948,
      "grad_norm": 0.8432933688163757,
      "learning_rate": 4.476952779341967e-05,
      "loss": 2.1111,
      "step": 8150
    },
    {
      "epoch": 0.6596412195318156,
      "grad_norm": 0.7955495119094849,
      "learning_rate": 4.424664146086396e-05,
      "loss": 2.1247,
      "step": 8200
    },
    {
      "epoch": 0.6636634220899365,
      "grad_norm": 0.8685803413391113,
      "learning_rate": 4.372375512830826e-05,
      "loss": 2.1107,
      "step": 8250
    },
    {
      "epoch": 0.6676856246480573,
      "grad_norm": 0.8647788166999817,
      "learning_rate": 4.320086879575255e-05,
      "loss": 2.117,
      "step": 8300
    },
    {
      "epoch": 0.671707827206178,
      "grad_norm": 0.8243076801300049,
      "learning_rate": 4.267798246319684e-05,
      "loss": 2.1211,
      "step": 8350
    },
    {
      "epoch": 0.675730029764299,
      "grad_norm": 0.8076698184013367,
      "learning_rate": 4.215509613064114e-05,
      "loss": 2.1188,
      "step": 8400
    },
    {
      "epoch": 0.6797522323224198,
      "grad_norm": 0.7406877875328064,
      "learning_rate": 4.163220979808543e-05,
      "loss": 2.1171,
      "step": 8450
    },
    {
      "epoch": 0.6837744348805406,
      "grad_norm": 0.8020715117454529,
      "learning_rate": 4.110932346552972e-05,
      "loss": 2.11,
      "step": 8500
    },
    {
      "epoch": 0.6877966374386614,
      "grad_norm": 0.8089524507522583,
      "learning_rate": 4.058643713297401e-05,
      "loss": 2.1155,
      "step": 8550
    },
    {
      "epoch": 0.6918188399967823,
      "grad_norm": 0.8233861327171326,
      "learning_rate": 4.006355080041831e-05,
      "loss": 2.0984,
      "step": 8600
    },
    {
      "epoch": 0.6958410425549031,
      "grad_norm": 0.8152355551719666,
      "learning_rate": 3.954066446786259e-05,
      "loss": 2.0999,
      "step": 8650
    },
    {
      "epoch": 0.6998632451130239,
      "grad_norm": 0.8338763117790222,
      "learning_rate": 3.901777813530689e-05,
      "loss": 2.1112,
      "step": 8700
    },
    {
      "epoch": 0.7038854476711447,
      "grad_norm": 0.8358407616615295,
      "learning_rate": 3.849489180275119e-05,
      "loss": 2.1012,
      "step": 8750
    },
    {
      "epoch": 0.7079076502292655,
      "grad_norm": 0.7586352229118347,
      "learning_rate": 3.797200547019547e-05,
      "loss": 2.1087,
      "step": 8800
    },
    {
      "epoch": 0.7119298527873864,
      "grad_norm": 0.818315863609314,
      "learning_rate": 3.744911913763977e-05,
      "loss": 2.1045,
      "step": 8850
    },
    {
      "epoch": 0.7159520553455072,
      "grad_norm": 0.8242735862731934,
      "learning_rate": 3.692623280508406e-05,
      "loss": 2.1053,
      "step": 8900
    },
    {
      "epoch": 0.719974257903628,
      "grad_norm": 0.7599959373474121,
      "learning_rate": 3.640334647252836e-05,
      "loss": 2.1077,
      "step": 8950
    },
    {
      "epoch": 0.7239964604617488,
      "grad_norm": 0.8952964544296265,
      "learning_rate": 3.588046013997264e-05,
      "loss": 2.1017,
      "step": 9000
    },
    {
      "epoch": 0.7280186630198697,
      "grad_norm": 0.7834117412567139,
      "learning_rate": 3.535757380741694e-05,
      "loss": 2.1027,
      "step": 9050
    },
    {
      "epoch": 0.7320408655779905,
      "grad_norm": 0.8121761083602905,
      "learning_rate": 3.483468747486123e-05,
      "loss": 2.1086,
      "step": 9100
    },
    {
      "epoch": 0.7360630681361113,
      "grad_norm": 0.772212028503418,
      "learning_rate": 3.431180114230552e-05,
      "loss": 2.1043,
      "step": 9150
    },
    {
      "epoch": 0.7400852706942321,
      "grad_norm": 0.8858734965324402,
      "learning_rate": 3.378891480974981e-05,
      "loss": 2.1043,
      "step": 9200
    },
    {
      "epoch": 0.7441074732523529,
      "grad_norm": 0.8062294721603394,
      "learning_rate": 3.326602847719411e-05,
      "loss": 2.1059,
      "step": 9250
    },
    {
      "epoch": 0.7481296758104738,
      "grad_norm": 0.8722843527793884,
      "learning_rate": 3.27431421446384e-05,
      "loss": 2.1135,
      "step": 9300
    },
    {
      "epoch": 0.7521518783685947,
      "grad_norm": 0.8117589950561523,
      "learning_rate": 3.222025581208269e-05,
      "loss": 2.1052,
      "step": 9350
    },
    {
      "epoch": 0.7561740809267155,
      "grad_norm": 0.853124737739563,
      "learning_rate": 3.169736947952698e-05,
      "loss": 2.0973,
      "step": 9400
    },
    {
      "epoch": 0.7601962834848363,
      "grad_norm": 0.830467164516449,
      "learning_rate": 3.117448314697128e-05,
      "loss": 2.0969,
      "step": 9450
    },
    {
      "epoch": 0.7642184860429572,
      "grad_norm": 0.7784795761108398,
      "learning_rate": 3.065159681441557e-05,
      "loss": 2.1118,
      "step": 9500
    },
    {
      "epoch": 0.768240688601078,
      "grad_norm": 0.8183643221855164,
      "learning_rate": 3.0128710481859865e-05,
      "loss": 2.1099,
      "step": 9550
    },
    {
      "epoch": 0.7722628911591988,
      "grad_norm": 0.7725605368614197,
      "learning_rate": 2.9605824149304156e-05,
      "loss": 2.0909,
      "step": 9600
    },
    {
      "epoch": 0.7762850937173196,
      "grad_norm": 0.8230785727500916,
      "learning_rate": 2.908293781674845e-05,
      "loss": 2.1092,
      "step": 9650
    },
    {
      "epoch": 0.7803072962754404,
      "grad_norm": 0.8227550983428955,
      "learning_rate": 2.856005148419274e-05,
      "loss": 2.091,
      "step": 9700
    },
    {
      "epoch": 0.7843294988335613,
      "grad_norm": 0.7650012969970703,
      "learning_rate": 2.8037165151637035e-05,
      "loss": 2.0977,
      "step": 9750
    },
    {
      "epoch": 0.7883517013916821,
      "grad_norm": 0.8824275732040405,
      "learning_rate": 2.7514278819081326e-05,
      "loss": 2.1132,
      "step": 9800
    },
    {
      "epoch": 0.7923739039498029,
      "grad_norm": 0.8139621019363403,
      "learning_rate": 2.6991392486525617e-05,
      "loss": 2.1009,
      "step": 9850
    },
    {
      "epoch": 0.7963961065079237,
      "grad_norm": 0.8344975113868713,
      "learning_rate": 2.646850615396991e-05,
      "loss": 2.1007,
      "step": 9900
    },
    {
      "epoch": 0.8004183090660446,
      "grad_norm": 0.770078182220459,
      "learning_rate": 2.5945619821414205e-05,
      "loss": 2.0939,
      "step": 9950
    },
    {
      "epoch": 0.8044405116241654,
      "grad_norm": 0.8030127286911011,
      "learning_rate": 2.54227334888585e-05,
      "loss": 2.1091,
      "step": 10000
    }
  ],
  "logging_steps": 50,
  "max_steps": 12431,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 5000,
  "total_flos": 1.43841558528e+17,
  "train_batch_size": 20,
  "trial_name": null,
  "trial_params": null
}
