{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "name_phone_pairs = []\n",
    "with open('./phonebook.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line[-1] == ',':\n",
    "            line = line[:-1]\n",
    "        pair = ast.literal_eval(line)\n",
    "        name_phone_pairs.append((pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "device = 'cuda'\n",
    "softmax = nn.Softmax(dim=2)\n",
    "\n",
    "def phone_book_task(batch_size=64, batches=10, book_size=20, model=None, tokenizer=None):\n",
    "    book = ''\n",
    "    success_lookups = 0\n",
    "    for i in range(book_size):\n",
    "        name = name_phone_pairs[i][0]\n",
    "        phone = name_phone_pairs[i][1]\n",
    "        book = book + name + ': ' + phone + '.\\n'\n",
    "    book += 'Liam: 436-725-2906\\nOlivia: 192-311-5790\\n\\n'\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(batches)):\n",
    "            cur_batch = []\n",
    "            gt_numbers = []\n",
    "            max_num_tokens = 30\n",
    "            for _ in range(batch_size):\n",
    "                query_pair_idx = random.randint(2, book_size)\n",
    "                query = book + name_phone_pairs[query_pair_idx][0] + ':'\n",
    "                \n",
    "                gt_numbers.append(name_phone_pairs[query_pair_idx][1])\n",
    "                cur_batch.append(query)\n",
    "                \n",
    "            input_ids = tokenizer(cur_batch, return_tensors=\"pt\", padding=True).to(device)[\"input_ids\"]\n",
    "            for i in range(max_num_tokens-1):\n",
    "                bs, seq_len = input_ids.size()\n",
    "                mask = torch.ones(bs, seq_len).to('cuda')\n",
    "                logits = model(input_ids=input_ids, attention_mask=mask, labels=None)['logits'] # bs, seq_len, vocab_size\n",
    "                next_token = torch.unsqueeze(torch.argmax(softmax(logits), dim=-1)[:, -1], 1)\n",
    "                input_ids = torch.cat((input_ids, next_token), dim=-1) # bs, seq_len, 1\n",
    "            for count in range(batch_size):\n",
    "                true_number = gt_numbers[count]\n",
    "                output_answer = tokenizer.decode(input_ids[count])\n",
    "                if output_answer.count(true_number) > 1:\n",
    "                    success_lookups += 1\n",
    "    return success_lookups / (batch_size * batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mamba_out/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ec2-user/anaconda3/envs/mamba_out/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.528125\n"
     ]
    }
   ],
   "source": [
    "from modeling_mamba_transformer import MambaTransformerForLM, MambaTransformerConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint_point_path = 'sft_1_epoch_100_length_2000_samples/checkpoint-540/model.safetensors'\n",
    "hybrid_model = MambaTransformerForLM(MambaTransformerConfig(), checkpoint_point_path).to(device)\n",
    "hybrid_model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-160m', padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "phone_book_task(batch_size=32, model=hybrid_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:04<00:00, 30.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MambaForCausalLM, AutoTokenizer\n",
    "\n",
    "mamba_130m_tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\", padding_side='left')\n",
    "mamba_130m_tokenizer.pad_token_id = mamba_130m_tokenizer.eos_token_id\n",
    "mamba_130m_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\").to(device)\n",
    "mamba_130m_model.eval()\n",
    "phone_book_task(batch_size=32, model=mamba_130m_model, tokenizer=mamba_130m_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 10/10 [00:57<00:00,  5.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.571875"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPTNeoXForCausalLM\n",
    "pythia_160m_model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\").to(device)\n",
    "pythia_160m_model.eval()\n",
    "pythia_160m_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\", padding_side='left')\n",
    "pythia_160m_tokenizer.pad_token_id = pythia_160m_tokenizer.eos_token_id\n",
    "phone_book_task(batch_size=32, model=pythia_160m_model, tokenizer=pythia_160m_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_out",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
