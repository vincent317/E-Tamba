{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.684563758389262,
  "eval_steps": 2000,
  "global_step": 10000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013422818791946308,
      "grad_norm": 4.331658363342285,
      "learning_rate": 0.00019910514541387027,
      "loss": 5.7401,
      "step": 50
    },
    {
      "epoch": 0.026845637583892617,
      "grad_norm": 5.529572486877441,
      "learning_rate": 0.00019821029082774049,
      "loss": 4.9748,
      "step": 100
    },
    {
      "epoch": 0.040268456375838924,
      "grad_norm": 4.421049118041992,
      "learning_rate": 0.00019731543624161075,
      "loss": 4.7893,
      "step": 150
    },
    {
      "epoch": 0.053691275167785234,
      "grad_norm": 3.809321165084839,
      "learning_rate": 0.000196420581655481,
      "loss": 4.5541,
      "step": 200
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 2.468681573867798,
      "learning_rate": 0.00019552572706935123,
      "loss": 4.3838,
      "step": 250
    },
    {
      "epoch": 0.08053691275167785,
      "grad_norm": 3.105877637863159,
      "learning_rate": 0.0001946308724832215,
      "loss": 4.2705,
      "step": 300
    },
    {
      "epoch": 0.09395973154362416,
      "grad_norm": 2.9405276775360107,
      "learning_rate": 0.00019373601789709173,
      "loss": 4.2315,
      "step": 350
    },
    {
      "epoch": 0.10738255033557047,
      "grad_norm": 2.0100722312927246,
      "learning_rate": 0.00019284116331096197,
      "loss": 4.1612,
      "step": 400
    },
    {
      "epoch": 0.12080536912751678,
      "grad_norm": 2.285937786102295,
      "learning_rate": 0.0001919463087248322,
      "loss": 4.1272,
      "step": 450
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 2.3108229637145996,
      "learning_rate": 0.00019105145413870247,
      "loss": 4.0924,
      "step": 500
    },
    {
      "epoch": 0.1476510067114094,
      "grad_norm": 2.4311270713806152,
      "learning_rate": 0.0001901565995525727,
      "loss": 4.0502,
      "step": 550
    },
    {
      "epoch": 0.1610738255033557,
      "grad_norm": 2.1033289432525635,
      "learning_rate": 0.00018926174496644295,
      "loss": 4.0403,
      "step": 600
    },
    {
      "epoch": 0.174496644295302,
      "grad_norm": 1.9160010814666748,
      "learning_rate": 0.0001883668903803132,
      "loss": 4.021,
      "step": 650
    },
    {
      "epoch": 0.18791946308724833,
      "grad_norm": 1.8887832164764404,
      "learning_rate": 0.00018747203579418348,
      "loss": 4.0071,
      "step": 700
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 1.7523225545883179,
      "learning_rate": 0.0001865771812080537,
      "loss": 3.9751,
      "step": 750
    },
    {
      "epoch": 0.21476510067114093,
      "grad_norm": 1.597248911857605,
      "learning_rate": 0.00018568232662192395,
      "loss": 3.9719,
      "step": 800
    },
    {
      "epoch": 0.22818791946308725,
      "grad_norm": 1.6256470680236816,
      "learning_rate": 0.0001847874720357942,
      "loss": 3.9917,
      "step": 850
    },
    {
      "epoch": 0.24161073825503357,
      "grad_norm": 1.976833462715149,
      "learning_rate": 0.00018389261744966443,
      "loss": 3.945,
      "step": 900
    },
    {
      "epoch": 0.2550335570469799,
      "grad_norm": 2.1878585815429688,
      "learning_rate": 0.0001829977628635347,
      "loss": 3.9299,
      "step": 950
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 1.9553755521774292,
      "learning_rate": 0.00018210290827740493,
      "loss": 3.9328,
      "step": 1000
    },
    {
      "epoch": 0.28187919463087246,
      "grad_norm": 2.401923894882202,
      "learning_rate": 0.00018120805369127517,
      "loss": 3.8987,
      "step": 1050
    },
    {
      "epoch": 0.2953020134228188,
      "grad_norm": 1.8957901000976562,
      "learning_rate": 0.0001803131991051454,
      "loss": 3.8752,
      "step": 1100
    },
    {
      "epoch": 0.3087248322147651,
      "grad_norm": 1.7688950300216675,
      "learning_rate": 0.00017941834451901567,
      "loss": 3.9168,
      "step": 1150
    },
    {
      "epoch": 0.3221476510067114,
      "grad_norm": 1.5705783367156982,
      "learning_rate": 0.0001785234899328859,
      "loss": 3.9,
      "step": 1200
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 1.8071385622024536,
      "learning_rate": 0.00017762863534675615,
      "loss": 3.9012,
      "step": 1250
    },
    {
      "epoch": 0.348993288590604,
      "grad_norm": 1.9806935787200928,
      "learning_rate": 0.00017673378076062642,
      "loss": 3.8591,
      "step": 1300
    },
    {
      "epoch": 0.3624161073825503,
      "grad_norm": 1.8220000267028809,
      "learning_rate": 0.00017583892617449665,
      "loss": 3.8704,
      "step": 1350
    },
    {
      "epoch": 0.37583892617449666,
      "grad_norm": 1.83719003200531,
      "learning_rate": 0.0001749440715883669,
      "loss": 3.8919,
      "step": 1400
    },
    {
      "epoch": 0.38926174496644295,
      "grad_norm": 1.5578128099441528,
      "learning_rate": 0.00017404921700223716,
      "loss": 3.8428,
      "step": 1450
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 1.7673043012619019,
      "learning_rate": 0.0001731543624161074,
      "loss": 3.8087,
      "step": 1500
    },
    {
      "epoch": 0.4161073825503356,
      "grad_norm": 1.5053743124008179,
      "learning_rate": 0.00017225950782997763,
      "loss": 3.8434,
      "step": 1550
    },
    {
      "epoch": 0.42953020134228187,
      "grad_norm": 1.482499599456787,
      "learning_rate": 0.00017136465324384787,
      "loss": 3.8415,
      "step": 1600
    },
    {
      "epoch": 0.4429530201342282,
      "grad_norm": 1.7264599800109863,
      "learning_rate": 0.00017046979865771814,
      "loss": 3.8455,
      "step": 1650
    },
    {
      "epoch": 0.4563758389261745,
      "grad_norm": 1.4367274045944214,
      "learning_rate": 0.00016957494407158837,
      "loss": 3.8435,
      "step": 1700
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 1.617556095123291,
      "learning_rate": 0.0001686800894854586,
      "loss": 3.8151,
      "step": 1750
    },
    {
      "epoch": 0.48322147651006714,
      "grad_norm": 1.5971447229385376,
      "learning_rate": 0.00016778523489932888,
      "loss": 3.8339,
      "step": 1800
    },
    {
      "epoch": 0.4966442953020134,
      "grad_norm": 1.62541663646698,
      "learning_rate": 0.00016689038031319912,
      "loss": 3.7842,
      "step": 1850
    },
    {
      "epoch": 0.5100671140939598,
      "grad_norm": 1.6772133111953735,
      "learning_rate": 0.00016599552572706935,
      "loss": 3.792,
      "step": 1900
    },
    {
      "epoch": 0.5234899328859061,
      "grad_norm": 1.4955146312713623,
      "learning_rate": 0.00016510067114093962,
      "loss": 3.7581,
      "step": 1950
    },
    {
      "epoch": 0.5369127516778524,
      "grad_norm": 2.0554795265197754,
      "learning_rate": 0.00016420581655480986,
      "loss": 3.7769,
      "step": 2000
    },
    {
      "epoch": 0.5369127516778524,
      "eval_loss": 3.814523696899414,
      "eval_runtime": 160.6365,
      "eval_samples_per_second": 46.029,
      "eval_steps_per_second": 5.758,
      "step": 2000
    },
    {
      "epoch": 0.5503355704697986,
      "grad_norm": 1.6254785060882568,
      "learning_rate": 0.0001633109619686801,
      "loss": 3.8161,
      "step": 2050
    },
    {
      "epoch": 0.5637583892617449,
      "grad_norm": 1.4754825830459595,
      "learning_rate": 0.00016241610738255036,
      "loss": 3.813,
      "step": 2100
    },
    {
      "epoch": 0.5771812080536913,
      "grad_norm": 1.7137662172317505,
      "learning_rate": 0.0001615212527964206,
      "loss": 3.788,
      "step": 2150
    },
    {
      "epoch": 0.5906040268456376,
      "grad_norm": 1.7225347757339478,
      "learning_rate": 0.00016062639821029084,
      "loss": 3.7603,
      "step": 2200
    },
    {
      "epoch": 0.6040268456375839,
      "grad_norm": 1.4335722923278809,
      "learning_rate": 0.00015973154362416107,
      "loss": 3.7619,
      "step": 2250
    },
    {
      "epoch": 0.6174496644295302,
      "grad_norm": 1.5777276754379272,
      "learning_rate": 0.00015883668903803134,
      "loss": 3.7593,
      "step": 2300
    },
    {
      "epoch": 0.6308724832214765,
      "grad_norm": 1.6351587772369385,
      "learning_rate": 0.00015794183445190158,
      "loss": 3.8014,
      "step": 2350
    },
    {
      "epoch": 0.6442953020134228,
      "grad_norm": 1.5782785415649414,
      "learning_rate": 0.00015704697986577181,
      "loss": 3.7886,
      "step": 2400
    },
    {
      "epoch": 0.6577181208053692,
      "grad_norm": 1.5427836179733276,
      "learning_rate": 0.00015615212527964208,
      "loss": 3.7722,
      "step": 2450
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 1.6062443256378174,
      "learning_rate": 0.0001552572706935123,
      "loss": 3.7505,
      "step": 2500
    },
    {
      "epoch": 0.6845637583892618,
      "grad_norm": 1.4861245155334473,
      "learning_rate": 0.00015436241610738256,
      "loss": 3.791,
      "step": 2550
    },
    {
      "epoch": 0.697986577181208,
      "grad_norm": 1.5946927070617676,
      "learning_rate": 0.00015346756152125282,
      "loss": 3.7766,
      "step": 2600
    },
    {
      "epoch": 0.7114093959731543,
      "grad_norm": 1.5163426399230957,
      "learning_rate": 0.00015257270693512303,
      "loss": 3.7878,
      "step": 2650
    },
    {
      "epoch": 0.7248322147651006,
      "grad_norm": 1.570044994354248,
      "learning_rate": 0.0001516778523489933,
      "loss": 3.7614,
      "step": 2700
    },
    {
      "epoch": 0.738255033557047,
      "grad_norm": 1.4106194972991943,
      "learning_rate": 0.00015078299776286354,
      "loss": 3.7596,
      "step": 2750
    },
    {
      "epoch": 0.7516778523489933,
      "grad_norm": 1.3291428089141846,
      "learning_rate": 0.00014988814317673377,
      "loss": 3.7554,
      "step": 2800
    },
    {
      "epoch": 0.7651006711409396,
      "grad_norm": 1.7325454950332642,
      "learning_rate": 0.00014899328859060404,
      "loss": 3.7413,
      "step": 2850
    },
    {
      "epoch": 0.7785234899328859,
      "grad_norm": 1.6765992641448975,
      "learning_rate": 0.00014809843400447428,
      "loss": 3.7413,
      "step": 2900
    },
    {
      "epoch": 0.7919463087248322,
      "grad_norm": 1.5999215841293335,
      "learning_rate": 0.00014720357941834454,
      "loss": 3.732,
      "step": 2950
    },
    {
      "epoch": 0.8053691275167785,
      "grad_norm": 1.6257884502410889,
      "learning_rate": 0.00014630872483221478,
      "loss": 3.7624,
      "step": 3000
    },
    {
      "epoch": 0.8187919463087249,
      "grad_norm": 1.3995184898376465,
      "learning_rate": 0.00014541387024608502,
      "loss": 3.7193,
      "step": 3050
    },
    {
      "epoch": 0.8322147651006712,
      "grad_norm": 1.44975745677948,
      "learning_rate": 0.00014451901565995528,
      "loss": 3.6979,
      "step": 3100
    },
    {
      "epoch": 0.8456375838926175,
      "grad_norm": 1.605334997177124,
      "learning_rate": 0.0001436241610738255,
      "loss": 3.7191,
      "step": 3150
    },
    {
      "epoch": 0.8590604026845637,
      "grad_norm": 1.434482455253601,
      "learning_rate": 0.00014272930648769576,
      "loss": 3.7364,
      "step": 3200
    },
    {
      "epoch": 0.87248322147651,
      "grad_norm": 1.3426491022109985,
      "learning_rate": 0.00014183445190156602,
      "loss": 3.7004,
      "step": 3250
    },
    {
      "epoch": 0.8859060402684564,
      "grad_norm": 1.445858120918274,
      "learning_rate": 0.00014093959731543624,
      "loss": 3.7062,
      "step": 3300
    },
    {
      "epoch": 0.8993288590604027,
      "grad_norm": 1.4559910297393799,
      "learning_rate": 0.0001400447427293065,
      "loss": 3.6956,
      "step": 3350
    },
    {
      "epoch": 0.912751677852349,
      "grad_norm": 1.4707359075546265,
      "learning_rate": 0.00013914988814317674,
      "loss": 3.7059,
      "step": 3400
    },
    {
      "epoch": 0.9261744966442953,
      "grad_norm": 1.2071975469589233,
      "learning_rate": 0.00013825503355704698,
      "loss": 3.7136,
      "step": 3450
    },
    {
      "epoch": 0.9395973154362416,
      "grad_norm": 1.3247848749160767,
      "learning_rate": 0.00013736017897091724,
      "loss": 3.7247,
      "step": 3500
    },
    {
      "epoch": 0.9530201342281879,
      "grad_norm": 1.370126485824585,
      "learning_rate": 0.00013646532438478748,
      "loss": 3.6763,
      "step": 3550
    },
    {
      "epoch": 0.9664429530201343,
      "grad_norm": 1.255983591079712,
      "learning_rate": 0.00013557046979865772,
      "loss": 3.6884,
      "step": 3600
    },
    {
      "epoch": 0.9798657718120806,
      "grad_norm": 1.4038152694702148,
      "learning_rate": 0.00013467561521252796,
      "loss": 3.6874,
      "step": 3650
    },
    {
      "epoch": 0.9932885906040269,
      "grad_norm": 1.6416434049606323,
      "learning_rate": 0.00013378076062639822,
      "loss": 3.6898,
      "step": 3700
    },
    {
      "epoch": 1.0067114093959733,
      "grad_norm": 1.456504225730896,
      "learning_rate": 0.00013288590604026846,
      "loss": 3.6268,
      "step": 3750
    },
    {
      "epoch": 1.0201342281879195,
      "grad_norm": 1.4252519607543945,
      "learning_rate": 0.0001319910514541387,
      "loss": 3.5779,
      "step": 3800
    },
    {
      "epoch": 1.0335570469798658,
      "grad_norm": 1.4981025457382202,
      "learning_rate": 0.00013109619686800896,
      "loss": 3.5725,
      "step": 3850
    },
    {
      "epoch": 1.0469798657718121,
      "grad_norm": 1.318354606628418,
      "learning_rate": 0.0001302013422818792,
      "loss": 3.5718,
      "step": 3900
    },
    {
      "epoch": 1.0604026845637584,
      "grad_norm": 1.4555351734161377,
      "learning_rate": 0.00012930648769574944,
      "loss": 3.6081,
      "step": 3950
    },
    {
      "epoch": 1.0738255033557047,
      "grad_norm": 1.331344723701477,
      "learning_rate": 0.0001284116331096197,
      "loss": 3.5943,
      "step": 4000
    },
    {
      "epoch": 1.0738255033557047,
      "eval_loss": 3.7147610187530518,
      "eval_runtime": 160.7366,
      "eval_samples_per_second": 46.001,
      "eval_steps_per_second": 5.755,
      "step": 4000
    },
    {
      "epoch": 1.087248322147651,
      "grad_norm": 1.9148635864257812,
      "learning_rate": 0.00012751677852348994,
      "loss": 3.5697,
      "step": 4050
    },
    {
      "epoch": 1.1006711409395973,
      "grad_norm": 1.330578088760376,
      "learning_rate": 0.00012662192393736018,
      "loss": 3.5591,
      "step": 4100
    },
    {
      "epoch": 1.1140939597315436,
      "grad_norm": 1.4913350343704224,
      "learning_rate": 0.00012572706935123044,
      "loss": 3.6032,
      "step": 4150
    },
    {
      "epoch": 1.1275167785234899,
      "grad_norm": 1.2783904075622559,
      "learning_rate": 0.00012483221476510068,
      "loss": 3.5811,
      "step": 4200
    },
    {
      "epoch": 1.1409395973154361,
      "grad_norm": 1.4084818363189697,
      "learning_rate": 0.00012393736017897092,
      "loss": 3.6046,
      "step": 4250
    },
    {
      "epoch": 1.1543624161073827,
      "grad_norm": 1.428909182548523,
      "learning_rate": 0.00012304250559284116,
      "loss": 3.5742,
      "step": 4300
    },
    {
      "epoch": 1.167785234899329,
      "grad_norm": 1.3215819597244263,
      "learning_rate": 0.00012214765100671142,
      "loss": 3.559,
      "step": 4350
    },
    {
      "epoch": 1.1812080536912752,
      "grad_norm": 1.3037523031234741,
      "learning_rate": 0.00012125279642058168,
      "loss": 3.5477,
      "step": 4400
    },
    {
      "epoch": 1.1946308724832215,
      "grad_norm": 1.2591322660446167,
      "learning_rate": 0.0001203579418344519,
      "loss": 3.5375,
      "step": 4450
    },
    {
      "epoch": 1.2080536912751678,
      "grad_norm": 1.409932255744934,
      "learning_rate": 0.00011946308724832216,
      "loss": 3.5834,
      "step": 4500
    },
    {
      "epoch": 1.221476510067114,
      "grad_norm": 1.2722625732421875,
      "learning_rate": 0.00011856823266219239,
      "loss": 3.5525,
      "step": 4550
    },
    {
      "epoch": 1.2348993288590604,
      "grad_norm": 1.4338796138763428,
      "learning_rate": 0.00011767337807606264,
      "loss": 3.5562,
      "step": 4600
    },
    {
      "epoch": 1.2483221476510067,
      "grad_norm": 1.3932002782821655,
      "learning_rate": 0.0001167785234899329,
      "loss": 3.563,
      "step": 4650
    },
    {
      "epoch": 1.261744966442953,
      "grad_norm": 1.2881182432174683,
      "learning_rate": 0.00011588366890380313,
      "loss": 3.5859,
      "step": 4700
    },
    {
      "epoch": 1.2751677852348993,
      "grad_norm": 1.2991312742233276,
      "learning_rate": 0.00011498881431767338,
      "loss": 3.5687,
      "step": 4750
    },
    {
      "epoch": 1.2885906040268456,
      "grad_norm": 1.3873487710952759,
      "learning_rate": 0.00011409395973154362,
      "loss": 3.553,
      "step": 4800
    },
    {
      "epoch": 1.302013422818792,
      "grad_norm": 1.4115965366363525,
      "learning_rate": 0.00011319910514541387,
      "loss": 3.5399,
      "step": 4850
    },
    {
      "epoch": 1.3154362416107381,
      "grad_norm": 1.388763427734375,
      "learning_rate": 0.00011230425055928412,
      "loss": 3.5511,
      "step": 4900
    },
    {
      "epoch": 1.3288590604026846,
      "grad_norm": 1.2778903245925903,
      "learning_rate": 0.00011140939597315436,
      "loss": 3.5658,
      "step": 4950
    },
    {
      "epoch": 1.342281879194631,
      "grad_norm": 1.298576831817627,
      "learning_rate": 0.00011051454138702461,
      "loss": 3.5346,
      "step": 5000
    },
    {
      "epoch": 1.3557046979865772,
      "grad_norm": 1.3854693174362183,
      "learning_rate": 0.00010961968680089485,
      "loss": 3.5486,
      "step": 5050
    },
    {
      "epoch": 1.3691275167785235,
      "grad_norm": 1.3056483268737793,
      "learning_rate": 0.0001087248322147651,
      "loss": 3.5427,
      "step": 5100
    },
    {
      "epoch": 1.3825503355704698,
      "grad_norm": 1.3943376541137695,
      "learning_rate": 0.00010782997762863535,
      "loss": 3.5865,
      "step": 5150
    },
    {
      "epoch": 1.395973154362416,
      "grad_norm": 1.3089570999145508,
      "learning_rate": 0.00010693512304250559,
      "loss": 3.5681,
      "step": 5200
    },
    {
      "epoch": 1.4093959731543624,
      "grad_norm": 1.3537273406982422,
      "learning_rate": 0.00010604026845637584,
      "loss": 3.5464,
      "step": 5250
    },
    {
      "epoch": 1.4228187919463087,
      "grad_norm": 1.4109883308410645,
      "learning_rate": 0.0001051454138702461,
      "loss": 3.539,
      "step": 5300
    },
    {
      "epoch": 1.436241610738255,
      "grad_norm": 1.4202760457992554,
      "learning_rate": 0.00010425055928411633,
      "loss": 3.5269,
      "step": 5350
    },
    {
      "epoch": 1.4496644295302015,
      "grad_norm": 1.4676991701126099,
      "learning_rate": 0.00010335570469798659,
      "loss": 3.4961,
      "step": 5400
    },
    {
      "epoch": 1.4630872483221475,
      "grad_norm": 1.3759191036224365,
      "learning_rate": 0.00010246085011185682,
      "loss": 3.5996,
      "step": 5450
    },
    {
      "epoch": 1.476510067114094,
      "grad_norm": 1.4241914749145508,
      "learning_rate": 0.00010156599552572707,
      "loss": 3.5525,
      "step": 5500
    },
    {
      "epoch": 1.4899328859060403,
      "grad_norm": 1.3427865505218506,
      "learning_rate": 0.00010067114093959733,
      "loss": 3.5786,
      "step": 5550
    },
    {
      "epoch": 1.5033557046979866,
      "grad_norm": 1.4299763441085815,
      "learning_rate": 9.977628635346756e-05,
      "loss": 3.5787,
      "step": 5600
    },
    {
      "epoch": 1.516778523489933,
      "grad_norm": 1.4245294332504272,
      "learning_rate": 9.888143176733782e-05,
      "loss": 3.5333,
      "step": 5650
    },
    {
      "epoch": 1.5302013422818792,
      "grad_norm": 1.2835743427276611,
      "learning_rate": 9.798657718120807e-05,
      "loss": 3.5438,
      "step": 5700
    },
    {
      "epoch": 1.5436241610738255,
      "grad_norm": 1.3301087617874146,
      "learning_rate": 9.70917225950783e-05,
      "loss": 3.5875,
      "step": 5750
    },
    {
      "epoch": 1.5570469798657718,
      "grad_norm": 1.312548041343689,
      "learning_rate": 9.619686800894854e-05,
      "loss": 3.5494,
      "step": 5800
    },
    {
      "epoch": 1.570469798657718,
      "grad_norm": 1.4865622520446777,
      "learning_rate": 9.53020134228188e-05,
      "loss": 3.5419,
      "step": 5850
    },
    {
      "epoch": 1.5838926174496644,
      "grad_norm": 1.2228237390518188,
      "learning_rate": 9.440715883668905e-05,
      "loss": 3.5737,
      "step": 5900
    },
    {
      "epoch": 1.5973154362416109,
      "grad_norm": 1.2016816139221191,
      "learning_rate": 9.351230425055928e-05,
      "loss": 3.5526,
      "step": 5950
    },
    {
      "epoch": 1.610738255033557,
      "grad_norm": 1.430254578590393,
      "learning_rate": 9.261744966442954e-05,
      "loss": 3.492,
      "step": 6000
    },
    {
      "epoch": 1.610738255033557,
      "eval_loss": 3.662513017654419,
      "eval_runtime": 160.5066,
      "eval_samples_per_second": 46.067,
      "eval_steps_per_second": 5.763,
      "step": 6000
    },
    {
      "epoch": 1.6241610738255035,
      "grad_norm": 1.2108131647109985,
      "learning_rate": 9.172259507829977e-05,
      "loss": 3.5471,
      "step": 6050
    },
    {
      "epoch": 1.6375838926174495,
      "grad_norm": 1.2500758171081543,
      "learning_rate": 9.082774049217003e-05,
      "loss": 3.5532,
      "step": 6100
    },
    {
      "epoch": 1.651006711409396,
      "grad_norm": 1.306998610496521,
      "learning_rate": 8.993288590604028e-05,
      "loss": 3.5259,
      "step": 6150
    },
    {
      "epoch": 1.6644295302013423,
      "grad_norm": 1.2956898212432861,
      "learning_rate": 8.903803131991052e-05,
      "loss": 3.5772,
      "step": 6200
    },
    {
      "epoch": 1.6778523489932886,
      "grad_norm": 1.308685064315796,
      "learning_rate": 8.814317673378077e-05,
      "loss": 3.587,
      "step": 6250
    },
    {
      "epoch": 1.691275167785235,
      "grad_norm": 1.3199269771575928,
      "learning_rate": 8.7248322147651e-05,
      "loss": 3.5231,
      "step": 6300
    },
    {
      "epoch": 1.7046979865771812,
      "grad_norm": 1.2023247480392456,
      "learning_rate": 8.635346756152126e-05,
      "loss": 3.5436,
      "step": 6350
    },
    {
      "epoch": 1.7181208053691275,
      "grad_norm": 1.2556174993515015,
      "learning_rate": 8.545861297539151e-05,
      "loss": 3.5267,
      "step": 6400
    },
    {
      "epoch": 1.7315436241610738,
      "grad_norm": 1.4117474555969238,
      "learning_rate": 8.456375838926175e-05,
      "loss": 3.5498,
      "step": 6450
    },
    {
      "epoch": 1.7449664429530203,
      "grad_norm": 1.2745451927185059,
      "learning_rate": 8.3668903803132e-05,
      "loss": 3.554,
      "step": 6500
    },
    {
      "epoch": 1.7583892617449663,
      "grad_norm": 1.3570306301116943,
      "learning_rate": 8.277404921700224e-05,
      "loss": 3.5176,
      "step": 6550
    },
    {
      "epoch": 1.7718120805369129,
      "grad_norm": 1.3094173669815063,
      "learning_rate": 8.187919463087249e-05,
      "loss": 3.5044,
      "step": 6600
    },
    {
      "epoch": 1.785234899328859,
      "grad_norm": 1.5406798124313354,
      "learning_rate": 8.098434004474274e-05,
      "loss": 3.5513,
      "step": 6650
    },
    {
      "epoch": 1.7986577181208054,
      "grad_norm": 1.2493666410446167,
      "learning_rate": 8.008948545861298e-05,
      "loss": 3.5671,
      "step": 6700
    },
    {
      "epoch": 1.8120805369127517,
      "grad_norm": 1.1460031270980835,
      "learning_rate": 7.919463087248322e-05,
      "loss": 3.5278,
      "step": 6750
    },
    {
      "epoch": 1.825503355704698,
      "grad_norm": 1.4283984899520874,
      "learning_rate": 7.829977628635348e-05,
      "loss": 3.5292,
      "step": 6800
    },
    {
      "epoch": 1.8389261744966443,
      "grad_norm": 1.304113745689392,
      "learning_rate": 7.740492170022372e-05,
      "loss": 3.5408,
      "step": 6850
    },
    {
      "epoch": 1.8523489932885906,
      "grad_norm": 1.302276849746704,
      "learning_rate": 7.651006711409397e-05,
      "loss": 3.5478,
      "step": 6900
    },
    {
      "epoch": 1.8657718120805369,
      "grad_norm": 1.2865560054779053,
      "learning_rate": 7.561521252796421e-05,
      "loss": 3.5086,
      "step": 6950
    },
    {
      "epoch": 1.8791946308724832,
      "grad_norm": 1.36785888671875,
      "learning_rate": 7.472035794183445e-05,
      "loss": 3.5276,
      "step": 7000
    },
    {
      "epoch": 1.8926174496644297,
      "grad_norm": 1.2727367877960205,
      "learning_rate": 7.382550335570471e-05,
      "loss": 3.5111,
      "step": 7050
    },
    {
      "epoch": 1.9060402684563758,
      "grad_norm": 1.2593352794647217,
      "learning_rate": 7.293064876957495e-05,
      "loss": 3.5191,
      "step": 7100
    },
    {
      "epoch": 1.9194630872483223,
      "grad_norm": 1.1890029907226562,
      "learning_rate": 7.203579418344519e-05,
      "loss": 3.5193,
      "step": 7150
    },
    {
      "epoch": 1.9328859060402683,
      "grad_norm": 1.2617650032043457,
      "learning_rate": 7.114093959731544e-05,
      "loss": 3.5323,
      "step": 7200
    },
    {
      "epoch": 1.9463087248322148,
      "grad_norm": 1.2428998947143555,
      "learning_rate": 7.024608501118568e-05,
      "loss": 3.5004,
      "step": 7250
    },
    {
      "epoch": 1.959731543624161,
      "grad_norm": 1.2964656352996826,
      "learning_rate": 6.935123042505593e-05,
      "loss": 3.4929,
      "step": 7300
    },
    {
      "epoch": 1.9731543624161074,
      "grad_norm": 1.2201952934265137,
      "learning_rate": 6.845637583892618e-05,
      "loss": 3.5214,
      "step": 7350
    },
    {
      "epoch": 1.9865771812080537,
      "grad_norm": 1.2699192762374878,
      "learning_rate": 6.756152125279642e-05,
      "loss": 3.5507,
      "step": 7400
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.5790187120437622,
      "learning_rate": 6.666666666666667e-05,
      "loss": 3.534,
      "step": 7450
    },
    {
      "epoch": 2.0134228187919465,
      "grad_norm": 1.2694017887115479,
      "learning_rate": 6.577181208053692e-05,
      "loss": 3.3821,
      "step": 7500
    },
    {
      "epoch": 2.0268456375838926,
      "grad_norm": 1.5267505645751953,
      "learning_rate": 6.487695749440716e-05,
      "loss": 3.3592,
      "step": 7550
    },
    {
      "epoch": 2.040268456375839,
      "grad_norm": 1.7461830377578735,
      "learning_rate": 6.398210290827741e-05,
      "loss": 3.4016,
      "step": 7600
    },
    {
      "epoch": 2.053691275167785,
      "grad_norm": 1.1864370107650757,
      "learning_rate": 6.308724832214765e-05,
      "loss": 3.3629,
      "step": 7650
    },
    {
      "epoch": 2.0671140939597317,
      "grad_norm": 1.2269188165664673,
      "learning_rate": 6.21923937360179e-05,
      "loss": 3.3766,
      "step": 7700
    },
    {
      "epoch": 2.0805369127516777,
      "grad_norm": 1.2432667016983032,
      "learning_rate": 6.129753914988815e-05,
      "loss": 3.4069,
      "step": 7750
    },
    {
      "epoch": 2.0939597315436242,
      "grad_norm": 1.230528473854065,
      "learning_rate": 6.04026845637584e-05,
      "loss": 3.404,
      "step": 7800
    },
    {
      "epoch": 2.1073825503355703,
      "grad_norm": 1.271167516708374,
      "learning_rate": 5.9507829977628635e-05,
      "loss": 3.3707,
      "step": 7850
    },
    {
      "epoch": 2.120805369127517,
      "grad_norm": 1.2680243253707886,
      "learning_rate": 5.861297539149888e-05,
      "loss": 3.4175,
      "step": 7900
    },
    {
      "epoch": 2.134228187919463,
      "grad_norm": 1.385233759880066,
      "learning_rate": 5.771812080536914e-05,
      "loss": 3.3962,
      "step": 7950
    },
    {
      "epoch": 2.1476510067114094,
      "grad_norm": 1.2558672428131104,
      "learning_rate": 5.6823266219239376e-05,
      "loss": 3.4042,
      "step": 8000
    },
    {
      "epoch": 2.1476510067114094,
      "eval_loss": 3.647078514099121,
      "eval_runtime": 160.7081,
      "eval_samples_per_second": 46.009,
      "eval_steps_per_second": 5.756,
      "step": 8000
    },
    {
      "epoch": 2.1610738255033555,
      "grad_norm": 1.4059550762176514,
      "learning_rate": 5.592841163310962e-05,
      "loss": 3.3946,
      "step": 8050
    },
    {
      "epoch": 2.174496644295302,
      "grad_norm": 1.2538508176803589,
      "learning_rate": 5.5033557046979866e-05,
      "loss": 3.4081,
      "step": 8100
    },
    {
      "epoch": 2.1879194630872485,
      "grad_norm": 1.2596479654312134,
      "learning_rate": 5.413870246085011e-05,
      "loss": 3.403,
      "step": 8150
    },
    {
      "epoch": 2.2013422818791946,
      "grad_norm": 1.1909927129745483,
      "learning_rate": 5.324384787472036e-05,
      "loss": 3.3776,
      "step": 8200
    },
    {
      "epoch": 2.214765100671141,
      "grad_norm": 1.3816574811935425,
      "learning_rate": 5.234899328859061e-05,
      "loss": 3.3744,
      "step": 8250
    },
    {
      "epoch": 2.228187919463087,
      "grad_norm": 1.2549500465393066,
      "learning_rate": 5.145413870246085e-05,
      "loss": 3.4104,
      "step": 8300
    },
    {
      "epoch": 2.2416107382550337,
      "grad_norm": 1.2799832820892334,
      "learning_rate": 5.05592841163311e-05,
      "loss": 3.3983,
      "step": 8350
    },
    {
      "epoch": 2.2550335570469797,
      "grad_norm": 1.1772255897521973,
      "learning_rate": 4.966442953020135e-05,
      "loss": 3.44,
      "step": 8400
    },
    {
      "epoch": 2.2684563758389262,
      "grad_norm": 1.2702186107635498,
      "learning_rate": 4.8769574944071586e-05,
      "loss": 3.3801,
      "step": 8450
    },
    {
      "epoch": 2.2818791946308723,
      "grad_norm": 1.1908215284347534,
      "learning_rate": 4.787472035794184e-05,
      "loss": 3.3788,
      "step": 8500
    },
    {
      "epoch": 2.295302013422819,
      "grad_norm": 1.1937949657440186,
      "learning_rate": 4.697986577181208e-05,
      "loss": 3.3756,
      "step": 8550
    },
    {
      "epoch": 2.3087248322147653,
      "grad_norm": 1.1631317138671875,
      "learning_rate": 4.608501118568233e-05,
      "loss": 3.4102,
      "step": 8600
    },
    {
      "epoch": 2.3221476510067114,
      "grad_norm": 1.2737177610397339,
      "learning_rate": 4.519015659955257e-05,
      "loss": 3.3921,
      "step": 8650
    },
    {
      "epoch": 2.335570469798658,
      "grad_norm": 1.2309436798095703,
      "learning_rate": 4.4295302013422824e-05,
      "loss": 3.4268,
      "step": 8700
    },
    {
      "epoch": 2.348993288590604,
      "grad_norm": 1.2984459400177002,
      "learning_rate": 4.340044742729307e-05,
      "loss": 3.3621,
      "step": 8750
    },
    {
      "epoch": 2.3624161073825505,
      "grad_norm": 1.216471552848816,
      "learning_rate": 4.2505592841163314e-05,
      "loss": 3.432,
      "step": 8800
    },
    {
      "epoch": 2.3758389261744965,
      "grad_norm": 1.251264214515686,
      "learning_rate": 4.161073825503356e-05,
      "loss": 3.3858,
      "step": 8850
    },
    {
      "epoch": 2.389261744966443,
      "grad_norm": 1.3080073595046997,
      "learning_rate": 4.07158836689038e-05,
      "loss": 3.3756,
      "step": 8900
    },
    {
      "epoch": 2.402684563758389,
      "grad_norm": 1.3074533939361572,
      "learning_rate": 3.9821029082774055e-05,
      "loss": 3.3847,
      "step": 8950
    },
    {
      "epoch": 2.4161073825503356,
      "grad_norm": 1.2133820056915283,
      "learning_rate": 3.89261744966443e-05,
      "loss": 3.37,
      "step": 9000
    },
    {
      "epoch": 2.4295302013422817,
      "grad_norm": 1.1900385618209839,
      "learning_rate": 3.8031319910514545e-05,
      "loss": 3.3748,
      "step": 9050
    },
    {
      "epoch": 2.442953020134228,
      "grad_norm": 1.207905888557434,
      "learning_rate": 3.713646532438479e-05,
      "loss": 3.3696,
      "step": 9100
    },
    {
      "epoch": 2.4563758389261743,
      "grad_norm": 1.1775819063186646,
      "learning_rate": 3.6241610738255034e-05,
      "loss": 3.3983,
      "step": 9150
    },
    {
      "epoch": 2.469798657718121,
      "grad_norm": 1.1880874633789062,
      "learning_rate": 3.534675615212528e-05,
      "loss": 3.3916,
      "step": 9200
    },
    {
      "epoch": 2.4832214765100673,
      "grad_norm": 1.230336308479309,
      "learning_rate": 3.4451901565995524e-05,
      "loss": 3.4009,
      "step": 9250
    },
    {
      "epoch": 2.4966442953020134,
      "grad_norm": 1.1863869428634644,
      "learning_rate": 3.3557046979865775e-05,
      "loss": 3.4005,
      "step": 9300
    },
    {
      "epoch": 2.51006711409396,
      "grad_norm": 1.1461865901947021,
      "learning_rate": 3.266219239373602e-05,
      "loss": 3.4016,
      "step": 9350
    },
    {
      "epoch": 2.523489932885906,
      "grad_norm": 1.285322666168213,
      "learning_rate": 3.1767337807606265e-05,
      "loss": 3.4013,
      "step": 9400
    },
    {
      "epoch": 2.5369127516778525,
      "grad_norm": 1.2835720777511597,
      "learning_rate": 3.087248322147651e-05,
      "loss": 3.3815,
      "step": 9450
    },
    {
      "epoch": 2.5503355704697985,
      "grad_norm": 1.2103140354156494,
      "learning_rate": 2.9977628635346755e-05,
      "loss": 3.3881,
      "step": 9500
    },
    {
      "epoch": 2.563758389261745,
      "grad_norm": 1.2864307165145874,
      "learning_rate": 2.9082774049217003e-05,
      "loss": 3.3399,
      "step": 9550
    },
    {
      "epoch": 2.577181208053691,
      "grad_norm": 1.2455202341079712,
      "learning_rate": 2.8187919463087248e-05,
      "loss": 3.409,
      "step": 9600
    },
    {
      "epoch": 2.5906040268456376,
      "grad_norm": 1.203572154045105,
      "learning_rate": 2.7293064876957496e-05,
      "loss": 3.4031,
      "step": 9650
    },
    {
      "epoch": 2.604026845637584,
      "grad_norm": 1.1698640584945679,
      "learning_rate": 2.639821029082774e-05,
      "loss": 3.3969,
      "step": 9700
    },
    {
      "epoch": 2.61744966442953,
      "grad_norm": 1.1377533674240112,
      "learning_rate": 2.550335570469799e-05,
      "loss": 3.3811,
      "step": 9750
    },
    {
      "epoch": 2.6308724832214763,
      "grad_norm": 1.1829670667648315,
      "learning_rate": 2.4608501118568234e-05,
      "loss": 3.3612,
      "step": 9800
    },
    {
      "epoch": 2.6442953020134228,
      "grad_norm": 1.1789509057998657,
      "learning_rate": 2.371364653243848e-05,
      "loss": 3.4053,
      "step": 9850
    },
    {
      "epoch": 2.6577181208053693,
      "grad_norm": 1.1494375467300415,
      "learning_rate": 2.2818791946308727e-05,
      "loss": 3.3969,
      "step": 9900
    },
    {
      "epoch": 2.6711409395973154,
      "grad_norm": 1.281331181526184,
      "learning_rate": 2.192393736017897e-05,
      "loss": 3.4149,
      "step": 9950
    },
    {
      "epoch": 2.684563758389262,
      "grad_norm": 1.203518271446228,
      "learning_rate": 2.102908277404922e-05,
      "loss": 3.4003,
      "step": 10000
    },
    {
      "epoch": 2.684563758389262,
      "eval_loss": 3.6236562728881836,
      "eval_runtime": 160.7695,
      "eval_samples_per_second": 45.991,
      "eval_steps_per_second": 5.754,
      "step": 10000
    }
  ],
  "logging_steps": 50,
  "max_steps": 11175,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 2000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.090012799481938e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
