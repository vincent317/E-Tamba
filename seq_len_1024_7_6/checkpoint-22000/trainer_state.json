{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.953020134228188,
  "eval_steps": 2000,
  "global_step": 22000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006711409395973154,
      "grad_norm": 3.5613391399383545,
      "learning_rate": 0.00019955257270693513,
      "loss": 7.5011,
      "step": 50
    },
    {
      "epoch": 0.013422818791946308,
      "grad_norm": 4.314239025115967,
      "learning_rate": 0.00019910514541387027,
      "loss": 5.198,
      "step": 100
    },
    {
      "epoch": 0.020134228187919462,
      "grad_norm": 3.6825859546661377,
      "learning_rate": 0.0001986577181208054,
      "loss": 4.9452,
      "step": 150
    },
    {
      "epoch": 0.026845637583892617,
      "grad_norm": 3.1003365516662598,
      "learning_rate": 0.00019821029082774049,
      "loss": 4.7544,
      "step": 200
    },
    {
      "epoch": 0.03355704697986577,
      "grad_norm": 3.049410104751587,
      "learning_rate": 0.00019776286353467563,
      "loss": 4.667,
      "step": 250
    },
    {
      "epoch": 0.040268456375838924,
      "grad_norm": 3.000237226486206,
      "learning_rate": 0.00019731543624161075,
      "loss": 4.5755,
      "step": 300
    },
    {
      "epoch": 0.04697986577181208,
      "grad_norm": 2.8228561878204346,
      "learning_rate": 0.00019686800894854587,
      "loss": 4.5442,
      "step": 350
    },
    {
      "epoch": 0.053691275167785234,
      "grad_norm": 2.996987819671631,
      "learning_rate": 0.000196420581655481,
      "loss": 4.4991,
      "step": 400
    },
    {
      "epoch": 0.06040268456375839,
      "grad_norm": 2.4870355129241943,
      "learning_rate": 0.00019597315436241613,
      "loss": 4.4209,
      "step": 450
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 2.594226360321045,
      "learning_rate": 0.00019552572706935123,
      "loss": 4.4325,
      "step": 500
    },
    {
      "epoch": 0.0738255033557047,
      "grad_norm": 2.963494300842285,
      "learning_rate": 0.00019507829977628635,
      "loss": 4.3586,
      "step": 550
    },
    {
      "epoch": 0.08053691275167785,
      "grad_norm": 2.869959592819214,
      "learning_rate": 0.0001946308724832215,
      "loss": 4.3441,
      "step": 600
    },
    {
      "epoch": 0.087248322147651,
      "grad_norm": 2.2811975479125977,
      "learning_rate": 0.0001941834451901566,
      "loss": 4.3599,
      "step": 650
    },
    {
      "epoch": 0.09395973154362416,
      "grad_norm": 2.593201160430908,
      "learning_rate": 0.00019373601789709173,
      "loss": 4.3219,
      "step": 700
    },
    {
      "epoch": 0.10067114093959731,
      "grad_norm": 2.533369779586792,
      "learning_rate": 0.00019328859060402688,
      "loss": 4.2875,
      "step": 750
    },
    {
      "epoch": 0.10738255033557047,
      "grad_norm": 2.4208686351776123,
      "learning_rate": 0.00019284116331096197,
      "loss": 4.274,
      "step": 800
    },
    {
      "epoch": 0.11409395973154363,
      "grad_norm": 2.306711196899414,
      "learning_rate": 0.0001923937360178971,
      "loss": 4.2708,
      "step": 850
    },
    {
      "epoch": 0.12080536912751678,
      "grad_norm": 2.304151773452759,
      "learning_rate": 0.0001919463087248322,
      "loss": 4.2481,
      "step": 900
    },
    {
      "epoch": 0.12751677852348994,
      "grad_norm": 2.2853920459747314,
      "learning_rate": 0.00019149888143176735,
      "loss": 4.2266,
      "step": 950
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 2.6689445972442627,
      "learning_rate": 0.00019105145413870247,
      "loss": 4.2135,
      "step": 1000
    },
    {
      "epoch": 0.14093959731543623,
      "grad_norm": 2.913907051086426,
      "learning_rate": 0.0001906040268456376,
      "loss": 4.1781,
      "step": 1050
    },
    {
      "epoch": 0.1476510067114094,
      "grad_norm": 2.201493978500366,
      "learning_rate": 0.0001901565995525727,
      "loss": 4.1812,
      "step": 1100
    },
    {
      "epoch": 0.15436241610738255,
      "grad_norm": 2.4570066928863525,
      "learning_rate": 0.00018970917225950783,
      "loss": 4.176,
      "step": 1150
    },
    {
      "epoch": 0.1610738255033557,
      "grad_norm": 2.3497414588928223,
      "learning_rate": 0.00018926174496644295,
      "loss": 4.1636,
      "step": 1200
    },
    {
      "epoch": 0.16778523489932887,
      "grad_norm": 2.362673282623291,
      "learning_rate": 0.0001888143176733781,
      "loss": 4.1643,
      "step": 1250
    },
    {
      "epoch": 0.174496644295302,
      "grad_norm": 2.1276164054870605,
      "learning_rate": 0.0001883668903803132,
      "loss": 4.1419,
      "step": 1300
    },
    {
      "epoch": 0.18120805369127516,
      "grad_norm": 2.086036443710327,
      "learning_rate": 0.00018791946308724833,
      "loss": 4.1669,
      "step": 1350
    },
    {
      "epoch": 0.18791946308724833,
      "grad_norm": 2.1535191535949707,
      "learning_rate": 0.00018747203579418348,
      "loss": 4.1033,
      "step": 1400
    },
    {
      "epoch": 0.19463087248322147,
      "grad_norm": 2.069810152053833,
      "learning_rate": 0.00018702460850111857,
      "loss": 4.1454,
      "step": 1450
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 2.0469672679901123,
      "learning_rate": 0.0001865771812080537,
      "loss": 4.0602,
      "step": 1500
    },
    {
      "epoch": 0.2080536912751678,
      "grad_norm": 2.6542458534240723,
      "learning_rate": 0.0001861297539149888,
      "loss": 4.0969,
      "step": 1550
    },
    {
      "epoch": 0.21476510067114093,
      "grad_norm": 2.0114519596099854,
      "learning_rate": 0.00018568232662192395,
      "loss": 4.1041,
      "step": 1600
    },
    {
      "epoch": 0.2214765100671141,
      "grad_norm": 2.3044593334198,
      "learning_rate": 0.00018523489932885907,
      "loss": 4.1425,
      "step": 1650
    },
    {
      "epoch": 0.22818791946308725,
      "grad_norm": 2.355194568634033,
      "learning_rate": 0.0001847874720357942,
      "loss": 4.0911,
      "step": 1700
    },
    {
      "epoch": 0.2348993288590604,
      "grad_norm": 2.111178398132324,
      "learning_rate": 0.0001843400447427293,
      "loss": 4.0636,
      "step": 1750
    },
    {
      "epoch": 0.24161073825503357,
      "grad_norm": 1.8307017087936401,
      "learning_rate": 0.00018389261744966443,
      "loss": 4.0889,
      "step": 1800
    },
    {
      "epoch": 0.2483221476510067,
      "grad_norm": 1.9296836853027344,
      "learning_rate": 0.00018344519015659955,
      "loss": 4.0851,
      "step": 1850
    },
    {
      "epoch": 0.2550335570469799,
      "grad_norm": 1.8685574531555176,
      "learning_rate": 0.0001829977628635347,
      "loss": 4.0327,
      "step": 1900
    },
    {
      "epoch": 0.26174496644295303,
      "grad_norm": 2.1938345432281494,
      "learning_rate": 0.00018255033557046981,
      "loss": 4.0947,
      "step": 1950
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 1.9324837923049927,
      "learning_rate": 0.00018210290827740493,
      "loss": 4.0281,
      "step": 2000
    },
    {
      "epoch": 0.2684563758389262,
      "eval_loss": 4.062779426574707,
      "eval_runtime": 144.5696,
      "eval_samples_per_second": 51.145,
      "eval_steps_per_second": 6.398,
      "step": 2000
    },
    {
      "epoch": 0.2751677852348993,
      "grad_norm": 2.124959707260132,
      "learning_rate": 0.00018165548098434005,
      "loss": 4.0646,
      "step": 2050
    },
    {
      "epoch": 0.28187919463087246,
      "grad_norm": 1.829638123512268,
      "learning_rate": 0.00018120805369127517,
      "loss": 3.99,
      "step": 2100
    },
    {
      "epoch": 0.28859060402684567,
      "grad_norm": 1.9526196718215942,
      "learning_rate": 0.0001807606263982103,
      "loss": 4.0349,
      "step": 2150
    },
    {
      "epoch": 0.2953020134228188,
      "grad_norm": 1.8562641143798828,
      "learning_rate": 0.0001803131991051454,
      "loss": 3.9617,
      "step": 2200
    },
    {
      "epoch": 0.30201342281879195,
      "grad_norm": 1.852710485458374,
      "learning_rate": 0.00017986577181208056,
      "loss": 4.0078,
      "step": 2250
    },
    {
      "epoch": 0.3087248322147651,
      "grad_norm": 1.625553011894226,
      "learning_rate": 0.00017941834451901567,
      "loss": 4.0734,
      "step": 2300
    },
    {
      "epoch": 0.31543624161073824,
      "grad_norm": 1.9639756679534912,
      "learning_rate": 0.0001789709172259508,
      "loss": 4.0138,
      "step": 2350
    },
    {
      "epoch": 0.3221476510067114,
      "grad_norm": 1.7103734016418457,
      "learning_rate": 0.0001785234899328859,
      "loss": 4.0266,
      "step": 2400
    },
    {
      "epoch": 0.3288590604026846,
      "grad_norm": 2.072753667831421,
      "learning_rate": 0.00017807606263982103,
      "loss": 4.0673,
      "step": 2450
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 2.096376895904541,
      "learning_rate": 0.00017762863534675615,
      "loss": 3.9802,
      "step": 2500
    },
    {
      "epoch": 0.3422818791946309,
      "grad_norm": 2.1199116706848145,
      "learning_rate": 0.0001771812080536913,
      "loss": 3.9923,
      "step": 2550
    },
    {
      "epoch": 0.348993288590604,
      "grad_norm": 2.1061394214630127,
      "learning_rate": 0.00017673378076062642,
      "loss": 3.9633,
      "step": 2600
    },
    {
      "epoch": 0.35570469798657717,
      "grad_norm": 1.7723675966262817,
      "learning_rate": 0.00017628635346756153,
      "loss": 4.0041,
      "step": 2650
    },
    {
      "epoch": 0.3624161073825503,
      "grad_norm": 2.0422401428222656,
      "learning_rate": 0.00017583892617449665,
      "loss": 3.9788,
      "step": 2700
    },
    {
      "epoch": 0.3691275167785235,
      "grad_norm": 1.7768224477767944,
      "learning_rate": 0.00017539149888143177,
      "loss": 4.0272,
      "step": 2750
    },
    {
      "epoch": 0.37583892617449666,
      "grad_norm": 1.9336994886398315,
      "learning_rate": 0.0001749440715883669,
      "loss": 3.9945,
      "step": 2800
    },
    {
      "epoch": 0.3825503355704698,
      "grad_norm": 1.9530959129333496,
      "learning_rate": 0.000174496644295302,
      "loss": 3.913,
      "step": 2850
    },
    {
      "epoch": 0.38926174496644295,
      "grad_norm": 1.71542227268219,
      "learning_rate": 0.00017404921700223716,
      "loss": 4.0081,
      "step": 2900
    },
    {
      "epoch": 0.3959731543624161,
      "grad_norm": 1.6979695558547974,
      "learning_rate": 0.00017360178970917228,
      "loss": 3.9507,
      "step": 2950
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 1.8487330675125122,
      "learning_rate": 0.0001731543624161074,
      "loss": 3.899,
      "step": 3000
    },
    {
      "epoch": 0.40939597315436244,
      "grad_norm": 1.6493421792984009,
      "learning_rate": 0.00017270693512304251,
      "loss": 3.9459,
      "step": 3050
    },
    {
      "epoch": 0.4161073825503356,
      "grad_norm": 1.6863625049591064,
      "learning_rate": 0.00017225950782997763,
      "loss": 3.9734,
      "step": 3100
    },
    {
      "epoch": 0.4228187919463087,
      "grad_norm": 1.8592959642410278,
      "learning_rate": 0.00017181208053691275,
      "loss": 3.9735,
      "step": 3150
    },
    {
      "epoch": 0.42953020134228187,
      "grad_norm": 1.691666603088379,
      "learning_rate": 0.00017136465324384787,
      "loss": 3.9412,
      "step": 3200
    },
    {
      "epoch": 0.436241610738255,
      "grad_norm": 1.9038920402526855,
      "learning_rate": 0.00017091722595078302,
      "loss": 3.9775,
      "step": 3250
    },
    {
      "epoch": 0.4429530201342282,
      "grad_norm": 1.8707391023635864,
      "learning_rate": 0.00017046979865771814,
      "loss": 3.9428,
      "step": 3300
    },
    {
      "epoch": 0.44966442953020136,
      "grad_norm": 1.7542327642440796,
      "learning_rate": 0.00017002237136465325,
      "loss": 3.9639,
      "step": 3350
    },
    {
      "epoch": 0.4563758389261745,
      "grad_norm": 1.7346277236938477,
      "learning_rate": 0.00016957494407158837,
      "loss": 3.9548,
      "step": 3400
    },
    {
      "epoch": 0.46308724832214765,
      "grad_norm": 1.730469822883606,
      "learning_rate": 0.0001691275167785235,
      "loss": 3.948,
      "step": 3450
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 2.557299852371216,
      "learning_rate": 0.0001686800894854586,
      "loss": 3.9066,
      "step": 3500
    },
    {
      "epoch": 0.47651006711409394,
      "grad_norm": 1.8236955404281616,
      "learning_rate": 0.00016823266219239376,
      "loss": 3.9636,
      "step": 3550
    },
    {
      "epoch": 0.48322147651006714,
      "grad_norm": 1.7076221704483032,
      "learning_rate": 0.00016778523489932888,
      "loss": 3.9323,
      "step": 3600
    },
    {
      "epoch": 0.4899328859060403,
      "grad_norm": 1.769276738166809,
      "learning_rate": 0.000167337807606264,
      "loss": 3.8816,
      "step": 3650
    },
    {
      "epoch": 0.4966442953020134,
      "grad_norm": 1.797559142112732,
      "learning_rate": 0.00016689038031319912,
      "loss": 3.9174,
      "step": 3700
    },
    {
      "epoch": 0.5033557046979866,
      "grad_norm": 2.2756094932556152,
      "learning_rate": 0.00016644295302013423,
      "loss": 3.9222,
      "step": 3750
    },
    {
      "epoch": 0.5100671140939598,
      "grad_norm": 3.795208692550659,
      "learning_rate": 0.00016599552572706935,
      "loss": 3.8879,
      "step": 3800
    },
    {
      "epoch": 0.5167785234899329,
      "grad_norm": 1.691339135169983,
      "learning_rate": 0.00016554809843400447,
      "loss": 3.8344,
      "step": 3850
    },
    {
      "epoch": 0.5234899328859061,
      "grad_norm": 1.9614479541778564,
      "learning_rate": 0.00016510067114093962,
      "loss": 3.898,
      "step": 3900
    },
    {
      "epoch": 0.5302013422818792,
      "grad_norm": 1.8787270784378052,
      "learning_rate": 0.00016465324384787474,
      "loss": 3.8707,
      "step": 3950
    },
    {
      "epoch": 0.5369127516778524,
      "grad_norm": 1.6300791501998901,
      "learning_rate": 0.00016420581655480986,
      "loss": 3.8964,
      "step": 4000
    },
    {
      "epoch": 0.5369127516778524,
      "eval_loss": 3.923323154449463,
      "eval_runtime": 144.3735,
      "eval_samples_per_second": 51.214,
      "eval_steps_per_second": 6.407,
      "step": 4000
    },
    {
      "epoch": 0.5436241610738255,
      "grad_norm": 1.805530309677124,
      "learning_rate": 0.00016375838926174498,
      "loss": 3.9514,
      "step": 4050
    },
    {
      "epoch": 0.5503355704697986,
      "grad_norm": 1.9180899858474731,
      "learning_rate": 0.0001633109619686801,
      "loss": 3.8984,
      "step": 4100
    },
    {
      "epoch": 0.5570469798657718,
      "grad_norm": 1.6027545928955078,
      "learning_rate": 0.0001628635346756152,
      "loss": 3.8616,
      "step": 4150
    },
    {
      "epoch": 0.5637583892617449,
      "grad_norm": 1.7655643224716187,
      "learning_rate": 0.00016241610738255036,
      "loss": 3.9804,
      "step": 4200
    },
    {
      "epoch": 0.5704697986577181,
      "grad_norm": 1.775492548942566,
      "learning_rate": 0.00016196868008948548,
      "loss": 3.8789,
      "step": 4250
    },
    {
      "epoch": 0.5771812080536913,
      "grad_norm": 2.2725613117218018,
      "learning_rate": 0.0001615212527964206,
      "loss": 3.9102,
      "step": 4300
    },
    {
      "epoch": 0.5838926174496645,
      "grad_norm": 1.588985800743103,
      "learning_rate": 0.0001610738255033557,
      "loss": 3.8433,
      "step": 4350
    },
    {
      "epoch": 0.5906040268456376,
      "grad_norm": 1.6709990501403809,
      "learning_rate": 0.00016062639821029084,
      "loss": 3.892,
      "step": 4400
    },
    {
      "epoch": 0.5973154362416108,
      "grad_norm": 1.8671759366989136,
      "learning_rate": 0.00016017897091722595,
      "loss": 3.8811,
      "step": 4450
    },
    {
      "epoch": 0.6040268456375839,
      "grad_norm": 1.8151754140853882,
      "learning_rate": 0.00015973154362416107,
      "loss": 3.8503,
      "step": 4500
    },
    {
      "epoch": 0.610738255033557,
      "grad_norm": 1.5007747411727905,
      "learning_rate": 0.00015928411633109622,
      "loss": 3.8529,
      "step": 4550
    },
    {
      "epoch": 0.6174496644295302,
      "grad_norm": 1.8328663110733032,
      "learning_rate": 0.00015883668903803134,
      "loss": 3.8747,
      "step": 4600
    },
    {
      "epoch": 0.6241610738255033,
      "grad_norm": 1.7683871984481812,
      "learning_rate": 0.00015838926174496643,
      "loss": 3.9225,
      "step": 4650
    },
    {
      "epoch": 0.6308724832214765,
      "grad_norm": 1.796919822692871,
      "learning_rate": 0.00015794183445190158,
      "loss": 3.8938,
      "step": 4700
    },
    {
      "epoch": 0.6375838926174496,
      "grad_norm": 1.5620266199111938,
      "learning_rate": 0.0001574944071588367,
      "loss": 3.8773,
      "step": 4750
    },
    {
      "epoch": 0.6442953020134228,
      "grad_norm": 1.649091362953186,
      "learning_rate": 0.00015704697986577181,
      "loss": 3.9097,
      "step": 4800
    },
    {
      "epoch": 0.6510067114093959,
      "grad_norm": 2.0243630409240723,
      "learning_rate": 0.00015659955257270696,
      "loss": 3.8661,
      "step": 4850
    },
    {
      "epoch": 0.6577181208053692,
      "grad_norm": 1.8112616539001465,
      "learning_rate": 0.00015615212527964208,
      "loss": 3.8853,
      "step": 4900
    },
    {
      "epoch": 0.6644295302013423,
      "grad_norm": 1.713965654373169,
      "learning_rate": 0.0001557046979865772,
      "loss": 3.8447,
      "step": 4950
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 1.5461416244506836,
      "learning_rate": 0.0001552572706935123,
      "loss": 3.8562,
      "step": 5000
    },
    {
      "epoch": 0.6778523489932886,
      "grad_norm": 1.7593475580215454,
      "learning_rate": 0.00015480984340044744,
      "loss": 3.8949,
      "step": 5050
    },
    {
      "epoch": 0.6845637583892618,
      "grad_norm": 1.7529305219650269,
      "learning_rate": 0.00015436241610738256,
      "loss": 3.8958,
      "step": 5100
    },
    {
      "epoch": 0.6912751677852349,
      "grad_norm": 1.8191717863082886,
      "learning_rate": 0.00015391498881431768,
      "loss": 3.8832,
      "step": 5150
    },
    {
      "epoch": 0.697986577181208,
      "grad_norm": 1.7058318853378296,
      "learning_rate": 0.00015346756152125282,
      "loss": 3.8644,
      "step": 5200
    },
    {
      "epoch": 0.7046979865771812,
      "grad_norm": 1.817438006401062,
      "learning_rate": 0.00015302013422818794,
      "loss": 3.8532,
      "step": 5250
    },
    {
      "epoch": 0.7114093959731543,
      "grad_norm": 1.5524605512619019,
      "learning_rate": 0.00015257270693512303,
      "loss": 3.9188,
      "step": 5300
    },
    {
      "epoch": 0.7181208053691275,
      "grad_norm": 1.6457562446594238,
      "learning_rate": 0.00015212527964205818,
      "loss": 3.8153,
      "step": 5350
    },
    {
      "epoch": 0.7248322147651006,
      "grad_norm": 1.6103270053863525,
      "learning_rate": 0.0001516778523489933,
      "loss": 3.9125,
      "step": 5400
    },
    {
      "epoch": 0.7315436241610739,
      "grad_norm": 1.6322933435440063,
      "learning_rate": 0.00015123042505592842,
      "loss": 3.8204,
      "step": 5450
    },
    {
      "epoch": 0.738255033557047,
      "grad_norm": 1.6034770011901855,
      "learning_rate": 0.00015078299776286354,
      "loss": 3.903,
      "step": 5500
    },
    {
      "epoch": 0.7449664429530202,
      "grad_norm": 1.679211974143982,
      "learning_rate": 0.00015033557046979868,
      "loss": 3.8821,
      "step": 5550
    },
    {
      "epoch": 0.7516778523489933,
      "grad_norm": 1.6670112609863281,
      "learning_rate": 0.00014988814317673377,
      "loss": 3.8368,
      "step": 5600
    },
    {
      "epoch": 0.7583892617449665,
      "grad_norm": 1.6835535764694214,
      "learning_rate": 0.0001494407158836689,
      "loss": 3.8472,
      "step": 5650
    },
    {
      "epoch": 0.7651006711409396,
      "grad_norm": 1.5463663339614868,
      "learning_rate": 0.00014899328859060404,
      "loss": 3.8358,
      "step": 5700
    },
    {
      "epoch": 0.7718120805369127,
      "grad_norm": 1.6267484426498413,
      "learning_rate": 0.00014854586129753916,
      "loss": 3.8396,
      "step": 5750
    },
    {
      "epoch": 0.7785234899328859,
      "grad_norm": 1.7533957958221436,
      "learning_rate": 0.00014809843400447428,
      "loss": 3.8364,
      "step": 5800
    },
    {
      "epoch": 0.785234899328859,
      "grad_norm": 1.4724597930908203,
      "learning_rate": 0.00014765100671140942,
      "loss": 3.8067,
      "step": 5850
    },
    {
      "epoch": 0.7919463087248322,
      "grad_norm": 1.6552202701568604,
      "learning_rate": 0.00014720357941834454,
      "loss": 3.8602,
      "step": 5900
    },
    {
      "epoch": 0.7986577181208053,
      "grad_norm": 1.4966628551483154,
      "learning_rate": 0.00014675615212527963,
      "loss": 3.8552,
      "step": 5950
    },
    {
      "epoch": 0.8053691275167785,
      "grad_norm": 1.6071648597717285,
      "learning_rate": 0.00014630872483221478,
      "loss": 3.8576,
      "step": 6000
    },
    {
      "epoch": 0.8053691275167785,
      "eval_loss": 3.848992109298706,
      "eval_runtime": 145.0033,
      "eval_samples_per_second": 50.992,
      "eval_steps_per_second": 6.379,
      "step": 6000
    },
    {
      "epoch": 0.8120805369127517,
      "grad_norm": 1.640501618385315,
      "learning_rate": 0.0001458612975391499,
      "loss": 3.7876,
      "step": 6050
    },
    {
      "epoch": 0.8187919463087249,
      "grad_norm": 1.6720528602600098,
      "learning_rate": 0.00014541387024608502,
      "loss": 3.849,
      "step": 6100
    },
    {
      "epoch": 0.825503355704698,
      "grad_norm": 1.6370937824249268,
      "learning_rate": 0.00014496644295302014,
      "loss": 3.8105,
      "step": 6150
    },
    {
      "epoch": 0.8322147651006712,
      "grad_norm": 1.5350916385650635,
      "learning_rate": 0.00014451901565995528,
      "loss": 3.7829,
      "step": 6200
    },
    {
      "epoch": 0.8389261744966443,
      "grad_norm": 1.5561962127685547,
      "learning_rate": 0.00014407158836689037,
      "loss": 3.8165,
      "step": 6250
    },
    {
      "epoch": 0.8456375838926175,
      "grad_norm": 1.595008373260498,
      "learning_rate": 0.0001436241610738255,
      "loss": 3.8157,
      "step": 6300
    },
    {
      "epoch": 0.8523489932885906,
      "grad_norm": 1.658492088317871,
      "learning_rate": 0.00014317673378076064,
      "loss": 3.852,
      "step": 6350
    },
    {
      "epoch": 0.8590604026845637,
      "grad_norm": 1.5933942794799805,
      "learning_rate": 0.00014272930648769576,
      "loss": 3.8132,
      "step": 6400
    },
    {
      "epoch": 0.8657718120805369,
      "grad_norm": 1.6453660726547241,
      "learning_rate": 0.00014228187919463088,
      "loss": 3.762,
      "step": 6450
    },
    {
      "epoch": 0.87248322147651,
      "grad_norm": 1.5500538349151611,
      "learning_rate": 0.00014183445190156602,
      "loss": 3.8312,
      "step": 6500
    },
    {
      "epoch": 0.8791946308724832,
      "grad_norm": 1.481394648551941,
      "learning_rate": 0.00014138702460850112,
      "loss": 3.8528,
      "step": 6550
    },
    {
      "epoch": 0.8859060402684564,
      "grad_norm": 1.7049065828323364,
      "learning_rate": 0.00014093959731543624,
      "loss": 3.7553,
      "step": 6600
    },
    {
      "epoch": 0.8926174496644296,
      "grad_norm": 1.4906160831451416,
      "learning_rate": 0.00014049217002237135,
      "loss": 3.7775,
      "step": 6650
    },
    {
      "epoch": 0.8993288590604027,
      "grad_norm": 1.5617246627807617,
      "learning_rate": 0.0001400447427293065,
      "loss": 3.7975,
      "step": 6700
    },
    {
      "epoch": 0.9060402684563759,
      "grad_norm": 1.8210984468460083,
      "learning_rate": 0.00013959731543624162,
      "loss": 3.7817,
      "step": 6750
    },
    {
      "epoch": 0.912751677852349,
      "grad_norm": 1.5936098098754883,
      "learning_rate": 0.00013914988814317674,
      "loss": 3.8216,
      "step": 6800
    },
    {
      "epoch": 0.9194630872483222,
      "grad_norm": 1.7696548700332642,
      "learning_rate": 0.00013870246085011186,
      "loss": 3.8182,
      "step": 6850
    },
    {
      "epoch": 0.9261744966442953,
      "grad_norm": 1.5218265056610107,
      "learning_rate": 0.00013825503355704698,
      "loss": 3.7978,
      "step": 6900
    },
    {
      "epoch": 0.9328859060402684,
      "grad_norm": 1.5328521728515625,
      "learning_rate": 0.0001378076062639821,
      "loss": 3.7952,
      "step": 6950
    },
    {
      "epoch": 0.9395973154362416,
      "grad_norm": 1.59311842918396,
      "learning_rate": 0.00013736017897091724,
      "loss": 3.8379,
      "step": 7000
    },
    {
      "epoch": 0.9463087248322147,
      "grad_norm": 1.6311696767807007,
      "learning_rate": 0.00013691275167785236,
      "loss": 3.7489,
      "step": 7050
    },
    {
      "epoch": 0.9530201342281879,
      "grad_norm": 1.4563713073730469,
      "learning_rate": 0.00013646532438478748,
      "loss": 3.7883,
      "step": 7100
    },
    {
      "epoch": 0.959731543624161,
      "grad_norm": 1.671294093132019,
      "learning_rate": 0.00013601789709172263,
      "loss": 3.7595,
      "step": 7150
    },
    {
      "epoch": 0.9664429530201343,
      "grad_norm": 1.4520143270492554,
      "learning_rate": 0.00013557046979865772,
      "loss": 3.802,
      "step": 7200
    },
    {
      "epoch": 0.9731543624161074,
      "grad_norm": 2.0470027923583984,
      "learning_rate": 0.00013512304250559284,
      "loss": 3.8192,
      "step": 7250
    },
    {
      "epoch": 0.9798657718120806,
      "grad_norm": 1.5055596828460693,
      "learning_rate": 0.00013467561521252796,
      "loss": 3.7391,
      "step": 7300
    },
    {
      "epoch": 0.9865771812080537,
      "grad_norm": 1.661137580871582,
      "learning_rate": 0.0001342281879194631,
      "loss": 3.7739,
      "step": 7350
    },
    {
      "epoch": 0.9932885906040269,
      "grad_norm": 1.6579140424728394,
      "learning_rate": 0.00013378076062639822,
      "loss": 3.7875,
      "step": 7400
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.6233348846435547,
      "learning_rate": 0.00013333333333333334,
      "loss": 3.7898,
      "step": 7450
    },
    {
      "epoch": 1.0067114093959733,
      "grad_norm": 1.623328685760498,
      "learning_rate": 0.00013288590604026846,
      "loss": 3.6392,
      "step": 7500
    },
    {
      "epoch": 1.0134228187919463,
      "grad_norm": 1.401444435119629,
      "learning_rate": 0.00013243847874720358,
      "loss": 3.6635,
      "step": 7550
    },
    {
      "epoch": 1.0201342281879195,
      "grad_norm": 1.4189460277557373,
      "learning_rate": 0.0001319910514541387,
      "loss": 3.6601,
      "step": 7600
    },
    {
      "epoch": 1.0268456375838926,
      "grad_norm": 1.8638505935668945,
      "learning_rate": 0.00013154362416107384,
      "loss": 3.6703,
      "step": 7650
    },
    {
      "epoch": 1.0335570469798658,
      "grad_norm": 1.5044100284576416,
      "learning_rate": 0.00013109619686800896,
      "loss": 3.6368,
      "step": 7700
    },
    {
      "epoch": 1.0402684563758389,
      "grad_norm": 1.670531988143921,
      "learning_rate": 0.00013064876957494408,
      "loss": 3.6256,
      "step": 7750
    },
    {
      "epoch": 1.0469798657718121,
      "grad_norm": 1.4509773254394531,
      "learning_rate": 0.0001302013422818792,
      "loss": 3.6859,
      "step": 7800
    },
    {
      "epoch": 1.0536912751677852,
      "grad_norm": 1.4360761642456055,
      "learning_rate": 0.00012975391498881432,
      "loss": 3.6936,
      "step": 7850
    },
    {
      "epoch": 1.0604026845637584,
      "grad_norm": 1.3896279335021973,
      "learning_rate": 0.00012930648769574944,
      "loss": 3.687,
      "step": 7900
    },
    {
      "epoch": 1.0671140939597314,
      "grad_norm": 1.4790128469467163,
      "learning_rate": 0.00012885906040268456,
      "loss": 3.6717,
      "step": 7950
    },
    {
      "epoch": 1.0738255033557047,
      "grad_norm": 1.4724504947662354,
      "learning_rate": 0.0001284116331096197,
      "loss": 3.6777,
      "step": 8000
    },
    {
      "epoch": 1.0738255033557047,
      "eval_loss": 3.811058282852173,
      "eval_runtime": 144.247,
      "eval_samples_per_second": 51.259,
      "eval_steps_per_second": 6.413,
      "step": 8000
    },
    {
      "epoch": 1.0805369127516777,
      "grad_norm": 1.3998316526412964,
      "learning_rate": 0.00012796420581655482,
      "loss": 3.6684,
      "step": 8050
    },
    {
      "epoch": 1.087248322147651,
      "grad_norm": 1.5306284427642822,
      "learning_rate": 0.00012751677852348994,
      "loss": 3.6402,
      "step": 8100
    },
    {
      "epoch": 1.0939597315436242,
      "grad_norm": 1.4835550785064697,
      "learning_rate": 0.00012706935123042506,
      "loss": 3.651,
      "step": 8150
    },
    {
      "epoch": 1.1006711409395973,
      "grad_norm": 1.4030218124389648,
      "learning_rate": 0.00012662192393736018,
      "loss": 3.6319,
      "step": 8200
    },
    {
      "epoch": 1.1073825503355705,
      "grad_norm": 1.4021626710891724,
      "learning_rate": 0.0001261744966442953,
      "loss": 3.6528,
      "step": 8250
    },
    {
      "epoch": 1.1140939597315436,
      "grad_norm": 1.7656571865081787,
      "learning_rate": 0.00012572706935123044,
      "loss": 3.7219,
      "step": 8300
    },
    {
      "epoch": 1.1208053691275168,
      "grad_norm": 1.4483096599578857,
      "learning_rate": 0.00012527964205816556,
      "loss": 3.6844,
      "step": 8350
    },
    {
      "epoch": 1.1275167785234899,
      "grad_norm": 1.7020807266235352,
      "learning_rate": 0.00012483221476510068,
      "loss": 3.6399,
      "step": 8400
    },
    {
      "epoch": 1.1342281879194631,
      "grad_norm": 1.2887364625930786,
      "learning_rate": 0.0001243847874720358,
      "loss": 3.6498,
      "step": 8450
    },
    {
      "epoch": 1.1409395973154361,
      "grad_norm": 1.4476324319839478,
      "learning_rate": 0.00012393736017897092,
      "loss": 3.7218,
      "step": 8500
    },
    {
      "epoch": 1.1476510067114094,
      "grad_norm": 1.4191803932189941,
      "learning_rate": 0.00012348993288590604,
      "loss": 3.6588,
      "step": 8550
    },
    {
      "epoch": 1.1543624161073827,
      "grad_norm": 1.4744926691055298,
      "learning_rate": 0.00012304250559284116,
      "loss": 3.6549,
      "step": 8600
    },
    {
      "epoch": 1.1610738255033557,
      "grad_norm": 1.4448449611663818,
      "learning_rate": 0.0001225950782997763,
      "loss": 3.6548,
      "step": 8650
    },
    {
      "epoch": 1.167785234899329,
      "grad_norm": 1.5347216129302979,
      "learning_rate": 0.00012214765100671142,
      "loss": 3.6292,
      "step": 8700
    },
    {
      "epoch": 1.174496644295302,
      "grad_norm": 1.4625962972640991,
      "learning_rate": 0.00012170022371364653,
      "loss": 3.623,
      "step": 8750
    },
    {
      "epoch": 1.1812080536912752,
      "grad_norm": 1.6087092161178589,
      "learning_rate": 0.00012125279642058168,
      "loss": 3.6366,
      "step": 8800
    },
    {
      "epoch": 1.1879194630872483,
      "grad_norm": 1.5162018537521362,
      "learning_rate": 0.0001208053691275168,
      "loss": 3.6308,
      "step": 8850
    },
    {
      "epoch": 1.1946308724832215,
      "grad_norm": 1.3965378999710083,
      "learning_rate": 0.0001203579418344519,
      "loss": 3.6063,
      "step": 8900
    },
    {
      "epoch": 1.2013422818791946,
      "grad_norm": 1.543205738067627,
      "learning_rate": 0.00011991051454138702,
      "loss": 3.6714,
      "step": 8950
    },
    {
      "epoch": 1.2080536912751678,
      "grad_norm": 1.4066786766052246,
      "learning_rate": 0.00011946308724832216,
      "loss": 3.6561,
      "step": 9000
    },
    {
      "epoch": 1.2147651006711409,
      "grad_norm": 1.5562002658843994,
      "learning_rate": 0.00011901565995525727,
      "loss": 3.6246,
      "step": 9050
    },
    {
      "epoch": 1.221476510067114,
      "grad_norm": 1.6219595670700073,
      "learning_rate": 0.00011856823266219239,
      "loss": 3.6379,
      "step": 9100
    },
    {
      "epoch": 1.2281879194630871,
      "grad_norm": 1.4772199392318726,
      "learning_rate": 0.00011812080536912754,
      "loss": 3.6578,
      "step": 9150
    },
    {
      "epoch": 1.2348993288590604,
      "grad_norm": 1.5186686515808105,
      "learning_rate": 0.00011767337807606264,
      "loss": 3.6166,
      "step": 9200
    },
    {
      "epoch": 1.2416107382550337,
      "grad_norm": 1.4212759733200073,
      "learning_rate": 0.00011722595078299776,
      "loss": 3.6289,
      "step": 9250
    },
    {
      "epoch": 1.2483221476510067,
      "grad_norm": 1.4648581743240356,
      "learning_rate": 0.0001167785234899329,
      "loss": 3.6625,
      "step": 9300
    },
    {
      "epoch": 1.25503355704698,
      "grad_norm": 1.4564766883850098,
      "learning_rate": 0.00011633109619686801,
      "loss": 3.6637,
      "step": 9350
    },
    {
      "epoch": 1.261744966442953,
      "grad_norm": 1.670180082321167,
      "learning_rate": 0.00011588366890380313,
      "loss": 3.6677,
      "step": 9400
    },
    {
      "epoch": 1.2684563758389262,
      "grad_norm": 1.6582748889923096,
      "learning_rate": 0.00011543624161073828,
      "loss": 3.6407,
      "step": 9450
    },
    {
      "epoch": 1.2751677852348993,
      "grad_norm": 1.6207473278045654,
      "learning_rate": 0.00011498881431767338,
      "loss": 3.6537,
      "step": 9500
    },
    {
      "epoch": 1.2818791946308725,
      "grad_norm": 1.530391812324524,
      "learning_rate": 0.0001145413870246085,
      "loss": 3.6307,
      "step": 9550
    },
    {
      "epoch": 1.2885906040268456,
      "grad_norm": 1.336413025856018,
      "learning_rate": 0.00011409395973154362,
      "loss": 3.6314,
      "step": 9600
    },
    {
      "epoch": 1.2953020134228188,
      "grad_norm": 1.4267421960830688,
      "learning_rate": 0.00011364653243847875,
      "loss": 3.6423,
      "step": 9650
    },
    {
      "epoch": 1.302013422818792,
      "grad_norm": 1.4315217733383179,
      "learning_rate": 0.00011319910514541387,
      "loss": 3.5971,
      "step": 9700
    },
    {
      "epoch": 1.308724832214765,
      "grad_norm": 1.4830008745193481,
      "learning_rate": 0.00011275167785234899,
      "loss": 3.6798,
      "step": 9750
    },
    {
      "epoch": 1.3154362416107381,
      "grad_norm": 1.5397043228149414,
      "learning_rate": 0.00011230425055928412,
      "loss": 3.5875,
      "step": 9800
    },
    {
      "epoch": 1.3221476510067114,
      "grad_norm": 1.461154580116272,
      "learning_rate": 0.00011185682326621924,
      "loss": 3.6653,
      "step": 9850
    },
    {
      "epoch": 1.3288590604026846,
      "grad_norm": 1.5200227499008179,
      "learning_rate": 0.00011140939597315436,
      "loss": 3.6292,
      "step": 9900
    },
    {
      "epoch": 1.3355704697986577,
      "grad_norm": 1.4786139726638794,
      "learning_rate": 0.00011096196868008951,
      "loss": 3.617,
      "step": 9950
    },
    {
      "epoch": 1.342281879194631,
      "grad_norm": 1.3305097818374634,
      "learning_rate": 0.00011051454138702461,
      "loss": 3.6067,
      "step": 10000
    },
    {
      "epoch": 1.342281879194631,
      "eval_loss": 3.779414415359497,
      "eval_runtime": 144.1321,
      "eval_samples_per_second": 51.3,
      "eval_steps_per_second": 6.418,
      "step": 10000
    },
    {
      "epoch": 1.348993288590604,
      "grad_norm": 1.576263666152954,
      "learning_rate": 0.00011006711409395973,
      "loss": 3.6187,
      "step": 10050
    },
    {
      "epoch": 1.3557046979865772,
      "grad_norm": 1.4658153057098389,
      "learning_rate": 0.00010961968680089485,
      "loss": 3.6419,
      "step": 10100
    },
    {
      "epoch": 1.3624161073825503,
      "grad_norm": 1.5514627695083618,
      "learning_rate": 0.00010917225950782998,
      "loss": 3.6305,
      "step": 10150
    },
    {
      "epoch": 1.3691275167785235,
      "grad_norm": 1.4047478437423706,
      "learning_rate": 0.0001087248322147651,
      "loss": 3.6181,
      "step": 10200
    },
    {
      "epoch": 1.3758389261744965,
      "grad_norm": 1.3977216482162476,
      "learning_rate": 0.00010827740492170022,
      "loss": 3.7084,
      "step": 10250
    },
    {
      "epoch": 1.3825503355704698,
      "grad_norm": 1.6502108573913574,
      "learning_rate": 0.00010782997762863535,
      "loss": 3.6271,
      "step": 10300
    },
    {
      "epoch": 1.389261744966443,
      "grad_norm": 1.4689030647277832,
      "learning_rate": 0.00010738255033557047,
      "loss": 3.6789,
      "step": 10350
    },
    {
      "epoch": 1.395973154362416,
      "grad_norm": 1.5760418176651,
      "learning_rate": 0.00010693512304250559,
      "loss": 3.6115,
      "step": 10400
    },
    {
      "epoch": 1.4026845637583891,
      "grad_norm": 1.5083377361297607,
      "learning_rate": 0.00010648769574944072,
      "loss": 3.5954,
      "step": 10450
    },
    {
      "epoch": 1.4093959731543624,
      "grad_norm": 1.5172828435897827,
      "learning_rate": 0.00010604026845637584,
      "loss": 3.6532,
      "step": 10500
    },
    {
      "epoch": 1.4161073825503356,
      "grad_norm": 1.487184762954712,
      "learning_rate": 0.00010559284116331096,
      "loss": 3.6175,
      "step": 10550
    },
    {
      "epoch": 1.4228187919463087,
      "grad_norm": 1.635140061378479,
      "learning_rate": 0.0001051454138702461,
      "loss": 3.6137,
      "step": 10600
    },
    {
      "epoch": 1.429530201342282,
      "grad_norm": 1.3522452116012573,
      "learning_rate": 0.00010469798657718121,
      "loss": 3.6223,
      "step": 10650
    },
    {
      "epoch": 1.436241610738255,
      "grad_norm": 1.5383473634719849,
      "learning_rate": 0.00010425055928411633,
      "loss": 3.5884,
      "step": 10700
    },
    {
      "epoch": 1.4429530201342282,
      "grad_norm": 1.2671105861663818,
      "learning_rate": 0.00010380313199105145,
      "loss": 3.5525,
      "step": 10750
    },
    {
      "epoch": 1.4496644295302015,
      "grad_norm": 1.4742680788040161,
      "learning_rate": 0.00010335570469798659,
      "loss": 3.596,
      "step": 10800
    },
    {
      "epoch": 1.4563758389261745,
      "grad_norm": 1.4162925481796265,
      "learning_rate": 0.0001029082774049217,
      "loss": 3.6663,
      "step": 10850
    },
    {
      "epoch": 1.4630872483221475,
      "grad_norm": 1.4959796667099,
      "learning_rate": 0.00010246085011185682,
      "loss": 3.6854,
      "step": 10900
    },
    {
      "epoch": 1.4697986577181208,
      "grad_norm": 1.4169886112213135,
      "learning_rate": 0.00010201342281879196,
      "loss": 3.6648,
      "step": 10950
    },
    {
      "epoch": 1.476510067114094,
      "grad_norm": 1.4363176822662354,
      "learning_rate": 0.00010156599552572707,
      "loss": 3.5956,
      "step": 11000
    },
    {
      "epoch": 1.483221476510067,
      "grad_norm": 1.489763617515564,
      "learning_rate": 0.0001011185682326622,
      "loss": 3.668,
      "step": 11050
    },
    {
      "epoch": 1.4899328859060403,
      "grad_norm": 1.5238407850265503,
      "learning_rate": 0.00010067114093959733,
      "loss": 3.6398,
      "step": 11100
    },
    {
      "epoch": 1.4966442953020134,
      "grad_norm": 1.555504322052002,
      "learning_rate": 0.00010022371364653245,
      "loss": 3.6387,
      "step": 11150
    },
    {
      "epoch": 1.5033557046979866,
      "grad_norm": 1.6287062168121338,
      "learning_rate": 9.977628635346756e-05,
      "loss": 3.6777,
      "step": 11200
    },
    {
      "epoch": 1.5100671140939599,
      "grad_norm": 1.2463858127593994,
      "learning_rate": 9.93288590604027e-05,
      "loss": 3.6135,
      "step": 11250
    },
    {
      "epoch": 1.516778523489933,
      "grad_norm": 1.5596609115600586,
      "learning_rate": 9.888143176733782e-05,
      "loss": 3.6064,
      "step": 11300
    },
    {
      "epoch": 1.523489932885906,
      "grad_norm": 1.3713308572769165,
      "learning_rate": 9.843400447427293e-05,
      "loss": 3.6192,
      "step": 11350
    },
    {
      "epoch": 1.5302013422818792,
      "grad_norm": 1.524563193321228,
      "learning_rate": 9.798657718120807e-05,
      "loss": 3.621,
      "step": 11400
    },
    {
      "epoch": 1.5369127516778525,
      "grad_norm": 1.428417444229126,
      "learning_rate": 9.753914988814317e-05,
      "loss": 3.6535,
      "step": 11450
    },
    {
      "epoch": 1.5436241610738255,
      "grad_norm": 1.3587461709976196,
      "learning_rate": 9.70917225950783e-05,
      "loss": 3.6745,
      "step": 11500
    },
    {
      "epoch": 1.5503355704697985,
      "grad_norm": 1.4122793674468994,
      "learning_rate": 9.664429530201344e-05,
      "loss": 3.6681,
      "step": 11550
    },
    {
      "epoch": 1.5570469798657718,
      "grad_norm": 1.558948040008545,
      "learning_rate": 9.619686800894854e-05,
      "loss": 3.5874,
      "step": 11600
    },
    {
      "epoch": 1.563758389261745,
      "grad_norm": 1.3646557331085205,
      "learning_rate": 9.574944071588368e-05,
      "loss": 3.6029,
      "step": 11650
    },
    {
      "epoch": 1.570469798657718,
      "grad_norm": 1.3305017948150635,
      "learning_rate": 9.53020134228188e-05,
      "loss": 3.6331,
      "step": 11700
    },
    {
      "epoch": 1.5771812080536913,
      "grad_norm": 1.4727534055709839,
      "learning_rate": 9.485458612975391e-05,
      "loss": 3.6437,
      "step": 11750
    },
    {
      "epoch": 1.5838926174496644,
      "grad_norm": 1.4096434116363525,
      "learning_rate": 9.440715883668905e-05,
      "loss": 3.6564,
      "step": 11800
    },
    {
      "epoch": 1.5906040268456376,
      "grad_norm": 1.4635002613067627,
      "learning_rate": 9.395973154362417e-05,
      "loss": 3.627,
      "step": 11850
    },
    {
      "epoch": 1.5973154362416109,
      "grad_norm": 1.413581371307373,
      "learning_rate": 9.351230425055928e-05,
      "loss": 3.6351,
      "step": 11900
    },
    {
      "epoch": 1.604026845637584,
      "grad_norm": 1.6756060123443604,
      "learning_rate": 9.30648769574944e-05,
      "loss": 3.5855,
      "step": 11950
    },
    {
      "epoch": 1.610738255033557,
      "grad_norm": 1.668570637702942,
      "learning_rate": 9.261744966442954e-05,
      "loss": 3.5484,
      "step": 12000
    },
    {
      "epoch": 1.610738255033557,
      "eval_loss": 3.750319242477417,
      "eval_runtime": 145.8383,
      "eval_samples_per_second": 50.7,
      "eval_steps_per_second": 6.343,
      "step": 12000
    },
    {
      "epoch": 1.6174496644295302,
      "grad_norm": 1.3500957489013672,
      "learning_rate": 9.217002237136466e-05,
      "loss": 3.6554,
      "step": 12050
    },
    {
      "epoch": 1.6241610738255035,
      "grad_norm": 1.3274294137954712,
      "learning_rate": 9.172259507829977e-05,
      "loss": 3.5887,
      "step": 12100
    },
    {
      "epoch": 1.6308724832214765,
      "grad_norm": 1.6212280988693237,
      "learning_rate": 9.127516778523491e-05,
      "loss": 3.6043,
      "step": 12150
    },
    {
      "epoch": 1.6375838926174495,
      "grad_norm": 1.3783278465270996,
      "learning_rate": 9.082774049217003e-05,
      "loss": 3.6513,
      "step": 12200
    },
    {
      "epoch": 1.6442953020134228,
      "grad_norm": 1.4420546293258667,
      "learning_rate": 9.038031319910515e-05,
      "loss": 3.5899,
      "step": 12250
    },
    {
      "epoch": 1.651006711409396,
      "grad_norm": 1.387144684791565,
      "learning_rate": 8.993288590604028e-05,
      "loss": 3.6182,
      "step": 12300
    },
    {
      "epoch": 1.6577181208053693,
      "grad_norm": 1.3508657217025757,
      "learning_rate": 8.94854586129754e-05,
      "loss": 3.6524,
      "step": 12350
    },
    {
      "epoch": 1.6644295302013423,
      "grad_norm": 1.419236421585083,
      "learning_rate": 8.903803131991052e-05,
      "loss": 3.6466,
      "step": 12400
    },
    {
      "epoch": 1.6711409395973154,
      "grad_norm": 1.4081248044967651,
      "learning_rate": 8.859060402684565e-05,
      "loss": 3.6363,
      "step": 12450
    },
    {
      "epoch": 1.6778523489932886,
      "grad_norm": 1.5515587329864502,
      "learning_rate": 8.814317673378077e-05,
      "loss": 3.6883,
      "step": 12500
    },
    {
      "epoch": 1.6845637583892619,
      "grad_norm": 1.4033807516098022,
      "learning_rate": 8.769574944071589e-05,
      "loss": 3.601,
      "step": 12550
    },
    {
      "epoch": 1.691275167785235,
      "grad_norm": 1.428920030593872,
      "learning_rate": 8.7248322147651e-05,
      "loss": 3.601,
      "step": 12600
    },
    {
      "epoch": 1.697986577181208,
      "grad_norm": 1.5414478778839111,
      "learning_rate": 8.680089485458614e-05,
      "loss": 3.6056,
      "step": 12650
    },
    {
      "epoch": 1.7046979865771812,
      "grad_norm": 1.3829660415649414,
      "learning_rate": 8.635346756152126e-05,
      "loss": 3.6329,
      "step": 12700
    },
    {
      "epoch": 1.7114093959731544,
      "grad_norm": 1.5271310806274414,
      "learning_rate": 8.590604026845638e-05,
      "loss": 3.6287,
      "step": 12750
    },
    {
      "epoch": 1.7181208053691275,
      "grad_norm": 1.397341251373291,
      "learning_rate": 8.545861297539151e-05,
      "loss": 3.5729,
      "step": 12800
    },
    {
      "epoch": 1.7248322147651005,
      "grad_norm": 1.3723100423812866,
      "learning_rate": 8.501118568232663e-05,
      "loss": 3.6023,
      "step": 12850
    },
    {
      "epoch": 1.7315436241610738,
      "grad_norm": 1.5062816143035889,
      "learning_rate": 8.456375838926175e-05,
      "loss": 3.6501,
      "step": 12900
    },
    {
      "epoch": 1.738255033557047,
      "grad_norm": 1.6082983016967773,
      "learning_rate": 8.411633109619688e-05,
      "loss": 3.6033,
      "step": 12950
    },
    {
      "epoch": 1.7449664429530203,
      "grad_norm": 1.547529935836792,
      "learning_rate": 8.3668903803132e-05,
      "loss": 3.6523,
      "step": 13000
    },
    {
      "epoch": 1.7516778523489933,
      "grad_norm": 1.5343382358551025,
      "learning_rate": 8.322147651006712e-05,
      "loss": 3.6,
      "step": 13050
    },
    {
      "epoch": 1.7583892617449663,
      "grad_norm": 1.3752423524856567,
      "learning_rate": 8.277404921700224e-05,
      "loss": 3.5871,
      "step": 13100
    },
    {
      "epoch": 1.7651006711409396,
      "grad_norm": 1.4587936401367188,
      "learning_rate": 8.232662192393737e-05,
      "loss": 3.5971,
      "step": 13150
    },
    {
      "epoch": 1.7718120805369129,
      "grad_norm": 1.5973613262176514,
      "learning_rate": 8.187919463087249e-05,
      "loss": 3.5623,
      "step": 13200
    },
    {
      "epoch": 1.778523489932886,
      "grad_norm": 1.4659441709518433,
      "learning_rate": 8.14317673378076e-05,
      "loss": 3.6111,
      "step": 13250
    },
    {
      "epoch": 1.785234899328859,
      "grad_norm": 1.3756611347198486,
      "learning_rate": 8.098434004474274e-05,
      "loss": 3.6409,
      "step": 13300
    },
    {
      "epoch": 1.7919463087248322,
      "grad_norm": 1.3681559562683105,
      "learning_rate": 8.053691275167784e-05,
      "loss": 3.6618,
      "step": 13350
    },
    {
      "epoch": 1.7986577181208054,
      "grad_norm": 1.4028927087783813,
      "learning_rate": 8.008948545861298e-05,
      "loss": 3.6221,
      "step": 13400
    },
    {
      "epoch": 1.8053691275167785,
      "grad_norm": 1.4120440483093262,
      "learning_rate": 7.964205816554811e-05,
      "loss": 3.6034,
      "step": 13450
    },
    {
      "epoch": 1.8120805369127517,
      "grad_norm": 1.2528046369552612,
      "learning_rate": 7.919463087248322e-05,
      "loss": 3.5981,
      "step": 13500
    },
    {
      "epoch": 1.8187919463087248,
      "grad_norm": 1.4052443504333496,
      "learning_rate": 7.874720357941835e-05,
      "loss": 3.6157,
      "step": 13550
    },
    {
      "epoch": 1.825503355704698,
      "grad_norm": 1.574573278427124,
      "learning_rate": 7.829977628635348e-05,
      "loss": 3.5873,
      "step": 13600
    },
    {
      "epoch": 1.8322147651006713,
      "grad_norm": 1.3830605745315552,
      "learning_rate": 7.78523489932886e-05,
      "loss": 3.6071,
      "step": 13650
    },
    {
      "epoch": 1.8389261744966443,
      "grad_norm": 1.4531782865524292,
      "learning_rate": 7.740492170022372e-05,
      "loss": 3.6293,
      "step": 13700
    },
    {
      "epoch": 1.8456375838926173,
      "grad_norm": 1.5581395626068115,
      "learning_rate": 7.695749440715884e-05,
      "loss": 3.6596,
      "step": 13750
    },
    {
      "epoch": 1.8523489932885906,
      "grad_norm": 1.4376715421676636,
      "learning_rate": 7.651006711409397e-05,
      "loss": 3.5871,
      "step": 13800
    },
    {
      "epoch": 1.8590604026845639,
      "grad_norm": 1.4196974039077759,
      "learning_rate": 7.606263982102909e-05,
      "loss": 3.594,
      "step": 13850
    },
    {
      "epoch": 1.8657718120805369,
      "grad_norm": 1.350027084350586,
      "learning_rate": 7.561521252796421e-05,
      "loss": 3.5724,
      "step": 13900
    },
    {
      "epoch": 1.87248322147651,
      "grad_norm": 1.4118006229400635,
      "learning_rate": 7.516778523489934e-05,
      "loss": 3.5917,
      "step": 13950
    },
    {
      "epoch": 1.8791946308724832,
      "grad_norm": 1.6000406742095947,
      "learning_rate": 7.472035794183445e-05,
      "loss": 3.6149,
      "step": 14000
    },
    {
      "epoch": 1.8791946308724832,
      "eval_loss": 3.72404408454895,
      "eval_runtime": 144.2276,
      "eval_samples_per_second": 51.266,
      "eval_steps_per_second": 6.413,
      "step": 14000
    },
    {
      "epoch": 1.8859060402684564,
      "grad_norm": 1.4080841541290283,
      "learning_rate": 7.427293064876958e-05,
      "loss": 3.5629,
      "step": 14050
    },
    {
      "epoch": 1.8926174496644297,
      "grad_norm": 1.4885884523391724,
      "learning_rate": 7.382550335570471e-05,
      "loss": 3.6047,
      "step": 14100
    },
    {
      "epoch": 1.8993288590604027,
      "grad_norm": 1.5391464233398438,
      "learning_rate": 7.337807606263982e-05,
      "loss": 3.5763,
      "step": 14150
    },
    {
      "epoch": 1.9060402684563758,
      "grad_norm": 1.4467248916625977,
      "learning_rate": 7.293064876957495e-05,
      "loss": 3.6129,
      "step": 14200
    },
    {
      "epoch": 1.912751677852349,
      "grad_norm": 1.520885705947876,
      "learning_rate": 7.248322147651007e-05,
      "loss": 3.6094,
      "step": 14250
    },
    {
      "epoch": 1.9194630872483223,
      "grad_norm": 1.5546483993530273,
      "learning_rate": 7.203579418344519e-05,
      "loss": 3.5758,
      "step": 14300
    },
    {
      "epoch": 1.9261744966442953,
      "grad_norm": 1.3711888790130615,
      "learning_rate": 7.158836689038032e-05,
      "loss": 3.6256,
      "step": 14350
    },
    {
      "epoch": 1.9328859060402683,
      "grad_norm": 1.3613412380218506,
      "learning_rate": 7.114093959731544e-05,
      "loss": 3.588,
      "step": 14400
    },
    {
      "epoch": 1.9395973154362416,
      "grad_norm": 1.5396127700805664,
      "learning_rate": 7.069351230425056e-05,
      "loss": 3.5952,
      "step": 14450
    },
    {
      "epoch": 1.9463087248322148,
      "grad_norm": 1.4332865476608276,
      "learning_rate": 7.024608501118568e-05,
      "loss": 3.5534,
      "step": 14500
    },
    {
      "epoch": 1.9530201342281879,
      "grad_norm": 1.4218310117721558,
      "learning_rate": 6.979865771812081e-05,
      "loss": 3.5552,
      "step": 14550
    },
    {
      "epoch": 1.959731543624161,
      "grad_norm": 1.3367841243743896,
      "learning_rate": 6.935123042505593e-05,
      "loss": 3.5804,
      "step": 14600
    },
    {
      "epoch": 1.9664429530201342,
      "grad_norm": 1.3674352169036865,
      "learning_rate": 6.890380313199105e-05,
      "loss": 3.5577,
      "step": 14650
    },
    {
      "epoch": 1.9731543624161074,
      "grad_norm": 1.4405218362808228,
      "learning_rate": 6.845637583892618e-05,
      "loss": 3.6349,
      "step": 14700
    },
    {
      "epoch": 1.9798657718120807,
      "grad_norm": 1.6344757080078125,
      "learning_rate": 6.800894854586131e-05,
      "loss": 3.6292,
      "step": 14750
    },
    {
      "epoch": 1.9865771812080537,
      "grad_norm": 1.4325110912322998,
      "learning_rate": 6.756152125279642e-05,
      "loss": 3.6139,
      "step": 14800
    },
    {
      "epoch": 1.9932885906040267,
      "grad_norm": 1.4477618932724,
      "learning_rate": 6.711409395973155e-05,
      "loss": 3.6376,
      "step": 14850
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.7419347763061523,
      "learning_rate": 6.666666666666667e-05,
      "loss": 3.5748,
      "step": 14900
    },
    {
      "epoch": 2.0067114093959733,
      "grad_norm": 1.4296259880065918,
      "learning_rate": 6.621923937360179e-05,
      "loss": 3.457,
      "step": 14950
    },
    {
      "epoch": 2.0134228187919465,
      "grad_norm": 1.3260093927383423,
      "learning_rate": 6.577181208053692e-05,
      "loss": 3.4389,
      "step": 15000
    },
    {
      "epoch": 2.0201342281879193,
      "grad_norm": 1.391867756843567,
      "learning_rate": 6.532438478747204e-05,
      "loss": 3.4209,
      "step": 15050
    },
    {
      "epoch": 2.0268456375838926,
      "grad_norm": 1.947696328163147,
      "learning_rate": 6.487695749440716e-05,
      "loss": 3.4269,
      "step": 15100
    },
    {
      "epoch": 2.033557046979866,
      "grad_norm": 1.431552767753601,
      "learning_rate": 6.442953020134228e-05,
      "loss": 3.5082,
      "step": 15150
    },
    {
      "epoch": 2.040268456375839,
      "grad_norm": 2.2445950508117676,
      "learning_rate": 6.398210290827741e-05,
      "loss": 3.4239,
      "step": 15200
    },
    {
      "epoch": 2.046979865771812,
      "grad_norm": 1.3714320659637451,
      "learning_rate": 6.353467561521253e-05,
      "loss": 3.4529,
      "step": 15250
    },
    {
      "epoch": 2.053691275167785,
      "grad_norm": 1.3739490509033203,
      "learning_rate": 6.308724832214765e-05,
      "loss": 3.4056,
      "step": 15300
    },
    {
      "epoch": 2.0604026845637584,
      "grad_norm": 1.421149730682373,
      "learning_rate": 6.263982102908278e-05,
      "loss": 3.4057,
      "step": 15350
    },
    {
      "epoch": 2.0671140939597317,
      "grad_norm": 1.39127516746521,
      "learning_rate": 6.21923937360179e-05,
      "loss": 3.4816,
      "step": 15400
    },
    {
      "epoch": 2.073825503355705,
      "grad_norm": 1.3910996913909912,
      "learning_rate": 6.174496644295302e-05,
      "loss": 3.4583,
      "step": 15450
    },
    {
      "epoch": 2.0805369127516777,
      "grad_norm": 1.4088225364685059,
      "learning_rate": 6.129753914988815e-05,
      "loss": 3.4938,
      "step": 15500
    },
    {
      "epoch": 2.087248322147651,
      "grad_norm": 1.4364615678787231,
      "learning_rate": 6.0850111856823265e-05,
      "loss": 3.5094,
      "step": 15550
    },
    {
      "epoch": 2.0939597315436242,
      "grad_norm": 1.3308193683624268,
      "learning_rate": 6.04026845637584e-05,
      "loss": 3.4373,
      "step": 15600
    },
    {
      "epoch": 2.1006711409395975,
      "grad_norm": 1.3824222087860107,
      "learning_rate": 5.995525727069351e-05,
      "loss": 3.4571,
      "step": 15650
    },
    {
      "epoch": 2.1073825503355703,
      "grad_norm": 1.4294573068618774,
      "learning_rate": 5.9507829977628635e-05,
      "loss": 3.4181,
      "step": 15700
    },
    {
      "epoch": 2.1140939597315436,
      "grad_norm": 1.4876999855041504,
      "learning_rate": 5.906040268456377e-05,
      "loss": 3.4775,
      "step": 15750
    },
    {
      "epoch": 2.120805369127517,
      "grad_norm": 1.3962804079055786,
      "learning_rate": 5.861297539149888e-05,
      "loss": 3.4907,
      "step": 15800
    },
    {
      "epoch": 2.12751677852349,
      "grad_norm": 1.3671129941940308,
      "learning_rate": 5.8165548098434006e-05,
      "loss": 3.4773,
      "step": 15850
    },
    {
      "epoch": 2.134228187919463,
      "grad_norm": 1.4748337268829346,
      "learning_rate": 5.771812080536914e-05,
      "loss": 3.4495,
      "step": 15900
    },
    {
      "epoch": 2.140939597315436,
      "grad_norm": 1.4803773164749146,
      "learning_rate": 5.727069351230425e-05,
      "loss": 3.4694,
      "step": 15950
    },
    {
      "epoch": 2.1476510067114094,
      "grad_norm": 1.4086521863937378,
      "learning_rate": 5.6823266219239376e-05,
      "loss": 3.4726,
      "step": 16000
    },
    {
      "epoch": 2.1476510067114094,
      "eval_loss": 3.7263221740722656,
      "eval_runtime": 144.181,
      "eval_samples_per_second": 51.283,
      "eval_steps_per_second": 6.416,
      "step": 16000
    },
    {
      "epoch": 2.1543624161073827,
      "grad_norm": 1.3029348850250244,
      "learning_rate": 5.6375838926174495e-05,
      "loss": 3.4316,
      "step": 16050
    },
    {
      "epoch": 2.1610738255033555,
      "grad_norm": 1.6102063655853271,
      "learning_rate": 5.592841163310962e-05,
      "loss": 3.4885,
      "step": 16100
    },
    {
      "epoch": 2.1677852348993287,
      "grad_norm": 1.6716564893722534,
      "learning_rate": 5.5480984340044754e-05,
      "loss": 3.4665,
      "step": 16150
    },
    {
      "epoch": 2.174496644295302,
      "grad_norm": 1.5633656978607178,
      "learning_rate": 5.5033557046979866e-05,
      "loss": 3.4857,
      "step": 16200
    },
    {
      "epoch": 2.1812080536912752,
      "grad_norm": 1.3593872785568237,
      "learning_rate": 5.458612975391499e-05,
      "loss": 3.4981,
      "step": 16250
    },
    {
      "epoch": 2.1879194630872485,
      "grad_norm": 1.3451423645019531,
      "learning_rate": 5.413870246085011e-05,
      "loss": 3.4398,
      "step": 16300
    },
    {
      "epoch": 2.1946308724832213,
      "grad_norm": 1.3430637121200562,
      "learning_rate": 5.3691275167785237e-05,
      "loss": 3.4571,
      "step": 16350
    },
    {
      "epoch": 2.2013422818791946,
      "grad_norm": 1.3010573387145996,
      "learning_rate": 5.324384787472036e-05,
      "loss": 3.4328,
      "step": 16400
    },
    {
      "epoch": 2.208053691275168,
      "grad_norm": 1.3593764305114746,
      "learning_rate": 5.279642058165548e-05,
      "loss": 3.4781,
      "step": 16450
    },
    {
      "epoch": 2.214765100671141,
      "grad_norm": 1.4913052320480347,
      "learning_rate": 5.234899328859061e-05,
      "loss": 3.4127,
      "step": 16500
    },
    {
      "epoch": 2.221476510067114,
      "grad_norm": 1.4191455841064453,
      "learning_rate": 5.1901565995525726e-05,
      "loss": 3.4592,
      "step": 16550
    },
    {
      "epoch": 2.228187919463087,
      "grad_norm": 1.404759168624878,
      "learning_rate": 5.145413870246085e-05,
      "loss": 3.4955,
      "step": 16600
    },
    {
      "epoch": 2.2348993288590604,
      "grad_norm": 1.524095058441162,
      "learning_rate": 5.100671140939598e-05,
      "loss": 3.4392,
      "step": 16650
    },
    {
      "epoch": 2.2416107382550337,
      "grad_norm": 1.4796863794326782,
      "learning_rate": 5.05592841163311e-05,
      "loss": 3.49,
      "step": 16700
    },
    {
      "epoch": 2.248322147651007,
      "grad_norm": 1.323693037033081,
      "learning_rate": 5.011185682326622e-05,
      "loss": 3.5157,
      "step": 16750
    },
    {
      "epoch": 2.2550335570469797,
      "grad_norm": 1.4366666078567505,
      "learning_rate": 4.966442953020135e-05,
      "loss": 3.4919,
      "step": 16800
    },
    {
      "epoch": 2.261744966442953,
      "grad_norm": 1.3193647861480713,
      "learning_rate": 4.921700223713647e-05,
      "loss": 3.4316,
      "step": 16850
    },
    {
      "epoch": 2.2684563758389262,
      "grad_norm": 1.4356764554977417,
      "learning_rate": 4.8769574944071586e-05,
      "loss": 3.4633,
      "step": 16900
    },
    {
      "epoch": 2.2751677852348995,
      "grad_norm": 1.3962044715881348,
      "learning_rate": 4.832214765100672e-05,
      "loss": 3.4325,
      "step": 16950
    },
    {
      "epoch": 2.2818791946308723,
      "grad_norm": 1.3315057754516602,
      "learning_rate": 4.787472035794184e-05,
      "loss": 3.4577,
      "step": 17000
    },
    {
      "epoch": 2.2885906040268456,
      "grad_norm": 1.3856488466262817,
      "learning_rate": 4.742729306487696e-05,
      "loss": 3.4433,
      "step": 17050
    },
    {
      "epoch": 2.295302013422819,
      "grad_norm": 1.5760515928268433,
      "learning_rate": 4.697986577181208e-05,
      "loss": 3.442,
      "step": 17100
    },
    {
      "epoch": 2.302013422818792,
      "grad_norm": 1.3175195455551147,
      "learning_rate": 4.65324384787472e-05,
      "loss": 3.4321,
      "step": 17150
    },
    {
      "epoch": 2.3087248322147653,
      "grad_norm": 1.3656843900680542,
      "learning_rate": 4.608501118568233e-05,
      "loss": 3.5229,
      "step": 17200
    },
    {
      "epoch": 2.315436241610738,
      "grad_norm": 1.3586742877960205,
      "learning_rate": 4.5637583892617453e-05,
      "loss": 3.4501,
      "step": 17250
    },
    {
      "epoch": 2.3221476510067114,
      "grad_norm": 1.4025006294250488,
      "learning_rate": 4.519015659955257e-05,
      "loss": 3.4691,
      "step": 17300
    },
    {
      "epoch": 2.3288590604026846,
      "grad_norm": 1.551385521888733,
      "learning_rate": 4.47427293064877e-05,
      "loss": 3.4869,
      "step": 17350
    },
    {
      "epoch": 2.335570469798658,
      "grad_norm": 1.3446428775787354,
      "learning_rate": 4.4295302013422824e-05,
      "loss": 3.5034,
      "step": 17400
    },
    {
      "epoch": 2.3422818791946307,
      "grad_norm": 1.3736869096755981,
      "learning_rate": 4.384787472035794e-05,
      "loss": 3.4228,
      "step": 17450
    },
    {
      "epoch": 2.348993288590604,
      "grad_norm": 1.4934077262878418,
      "learning_rate": 4.340044742729307e-05,
      "loss": 3.4334,
      "step": 17500
    },
    {
      "epoch": 2.3557046979865772,
      "grad_norm": 1.362192153930664,
      "learning_rate": 4.295302013422819e-05,
      "loss": 3.4479,
      "step": 17550
    },
    {
      "epoch": 2.3624161073825505,
      "grad_norm": 1.427018165588379,
      "learning_rate": 4.2505592841163314e-05,
      "loss": 3.5494,
      "step": 17600
    },
    {
      "epoch": 2.3691275167785237,
      "grad_norm": 1.3922386169433594,
      "learning_rate": 4.205816554809844e-05,
      "loss": 3.4753,
      "step": 17650
    },
    {
      "epoch": 2.3758389261744965,
      "grad_norm": 1.4108850955963135,
      "learning_rate": 4.161073825503356e-05,
      "loss": 3.4217,
      "step": 17700
    },
    {
      "epoch": 2.38255033557047,
      "grad_norm": 1.3589001893997192,
      "learning_rate": 4.1163310961968684e-05,
      "loss": 3.4634,
      "step": 17750
    },
    {
      "epoch": 2.389261744966443,
      "grad_norm": 1.3123095035552979,
      "learning_rate": 4.07158836689038e-05,
      "loss": 3.4224,
      "step": 17800
    },
    {
      "epoch": 2.395973154362416,
      "grad_norm": 1.3848495483398438,
      "learning_rate": 4.026845637583892e-05,
      "loss": 3.448,
      "step": 17850
    },
    {
      "epoch": 2.402684563758389,
      "grad_norm": 1.5255330801010132,
      "learning_rate": 3.9821029082774055e-05,
      "loss": 3.4556,
      "step": 17900
    },
    {
      "epoch": 2.4093959731543624,
      "grad_norm": 1.4086582660675049,
      "learning_rate": 3.9373601789709174e-05,
      "loss": 3.4205,
      "step": 17950
    },
    {
      "epoch": 2.4161073825503356,
      "grad_norm": 1.387269377708435,
      "learning_rate": 3.89261744966443e-05,
      "loss": 3.4579,
      "step": 18000
    },
    {
      "epoch": 2.4161073825503356,
      "eval_loss": 3.7152905464172363,
      "eval_runtime": 144.1428,
      "eval_samples_per_second": 51.296,
      "eval_steps_per_second": 6.417,
      "step": 18000
    },
    {
      "epoch": 2.422818791946309,
      "grad_norm": 1.4490413665771484,
      "learning_rate": 3.847874720357942e-05,
      "loss": 3.4782,
      "step": 18050
    },
    {
      "epoch": 2.4295302013422817,
      "grad_norm": 1.3847864866256714,
      "learning_rate": 3.8031319910514545e-05,
      "loss": 3.4031,
      "step": 18100
    },
    {
      "epoch": 2.436241610738255,
      "grad_norm": 1.3925424814224243,
      "learning_rate": 3.758389261744967e-05,
      "loss": 3.41,
      "step": 18150
    },
    {
      "epoch": 2.442953020134228,
      "grad_norm": 1.387988567352295,
      "learning_rate": 3.713646532438479e-05,
      "loss": 3.4604,
      "step": 18200
    },
    {
      "epoch": 2.4496644295302015,
      "grad_norm": 1.4328181743621826,
      "learning_rate": 3.668903803131991e-05,
      "loss": 3.4456,
      "step": 18250
    },
    {
      "epoch": 2.4563758389261743,
      "grad_norm": 1.367344856262207,
      "learning_rate": 3.6241610738255034e-05,
      "loss": 3.4858,
      "step": 18300
    },
    {
      "epoch": 2.4630872483221475,
      "grad_norm": 1.4338754415512085,
      "learning_rate": 3.579418344519016e-05,
      "loss": 3.4405,
      "step": 18350
    },
    {
      "epoch": 2.469798657718121,
      "grad_norm": 1.4267319440841675,
      "learning_rate": 3.534675615212528e-05,
      "loss": 3.4739,
      "step": 18400
    },
    {
      "epoch": 2.476510067114094,
      "grad_norm": 1.339160680770874,
      "learning_rate": 3.4899328859060405e-05,
      "loss": 3.4418,
      "step": 18450
    },
    {
      "epoch": 2.4832214765100673,
      "grad_norm": 1.347264051437378,
      "learning_rate": 3.4451901565995524e-05,
      "loss": 3.4865,
      "step": 18500
    },
    {
      "epoch": 2.48993288590604,
      "grad_norm": 1.3991851806640625,
      "learning_rate": 3.4004474272930656e-05,
      "loss": 3.481,
      "step": 18550
    },
    {
      "epoch": 2.4966442953020134,
      "grad_norm": 1.309531331062317,
      "learning_rate": 3.3557046979865775e-05,
      "loss": 3.451,
      "step": 18600
    },
    {
      "epoch": 2.5033557046979866,
      "grad_norm": 1.333095908164978,
      "learning_rate": 3.3109619686800894e-05,
      "loss": 3.4669,
      "step": 18650
    },
    {
      "epoch": 2.51006711409396,
      "grad_norm": 1.3389732837677002,
      "learning_rate": 3.266219239373602e-05,
      "loss": 3.4673,
      "step": 18700
    },
    {
      "epoch": 2.5167785234899327,
      "grad_norm": 1.296998381614685,
      "learning_rate": 3.221476510067114e-05,
      "loss": 3.4729,
      "step": 18750
    },
    {
      "epoch": 2.523489932885906,
      "grad_norm": 1.4063960313796997,
      "learning_rate": 3.1767337807606265e-05,
      "loss": 3.4599,
      "step": 18800
    },
    {
      "epoch": 2.530201342281879,
      "grad_norm": 1.4199063777923584,
      "learning_rate": 3.131991051454139e-05,
      "loss": 3.4312,
      "step": 18850
    },
    {
      "epoch": 2.5369127516778525,
      "grad_norm": 1.3672833442687988,
      "learning_rate": 3.087248322147651e-05,
      "loss": 3.4597,
      "step": 18900
    },
    {
      "epoch": 2.5436241610738257,
      "grad_norm": 1.3664242029190063,
      "learning_rate": 3.0425055928411632e-05,
      "loss": 3.4341,
      "step": 18950
    },
    {
      "epoch": 2.5503355704697985,
      "grad_norm": 1.3156280517578125,
      "learning_rate": 2.9977628635346755e-05,
      "loss": 3.4778,
      "step": 19000
    },
    {
      "epoch": 2.557046979865772,
      "grad_norm": 1.3060634136199951,
      "learning_rate": 2.9530201342281884e-05,
      "loss": 3.408,
      "step": 19050
    },
    {
      "epoch": 2.563758389261745,
      "grad_norm": 1.3005321025848389,
      "learning_rate": 2.9082774049217003e-05,
      "loss": 3.3991,
      "step": 19100
    },
    {
      "epoch": 2.570469798657718,
      "grad_norm": 1.3969286680221558,
      "learning_rate": 2.8635346756152125e-05,
      "loss": 3.4912,
      "step": 19150
    },
    {
      "epoch": 2.577181208053691,
      "grad_norm": 1.350830316543579,
      "learning_rate": 2.8187919463087248e-05,
      "loss": 3.4588,
      "step": 19200
    },
    {
      "epoch": 2.5838926174496644,
      "grad_norm": 1.3365623950958252,
      "learning_rate": 2.7740492170022377e-05,
      "loss": 3.4619,
      "step": 19250
    },
    {
      "epoch": 2.5906040268456376,
      "grad_norm": 1.3257492780685425,
      "learning_rate": 2.7293064876957496e-05,
      "loss": 3.4757,
      "step": 19300
    },
    {
      "epoch": 2.597315436241611,
      "grad_norm": 1.3881317377090454,
      "learning_rate": 2.6845637583892618e-05,
      "loss": 3.435,
      "step": 19350
    },
    {
      "epoch": 2.604026845637584,
      "grad_norm": 1.3967775106430054,
      "learning_rate": 2.639821029082774e-05,
      "loss": 3.4903,
      "step": 19400
    },
    {
      "epoch": 2.610738255033557,
      "grad_norm": 1.513430118560791,
      "learning_rate": 2.5950782997762863e-05,
      "loss": 3.4355,
      "step": 19450
    },
    {
      "epoch": 2.61744966442953,
      "grad_norm": 1.365707278251648,
      "learning_rate": 2.550335570469799e-05,
      "loss": 3.4601,
      "step": 19500
    },
    {
      "epoch": 2.6241610738255035,
      "grad_norm": 1.3250421285629272,
      "learning_rate": 2.505592841163311e-05,
      "loss": 3.4102,
      "step": 19550
    },
    {
      "epoch": 2.6308724832214763,
      "grad_norm": 1.4319475889205933,
      "learning_rate": 2.4608501118568234e-05,
      "loss": 3.4391,
      "step": 19600
    },
    {
      "epoch": 2.6375838926174495,
      "grad_norm": 1.4895823001861572,
      "learning_rate": 2.416107382550336e-05,
      "loss": 3.4853,
      "step": 19650
    },
    {
      "epoch": 2.6442953020134228,
      "grad_norm": 1.3634551763534546,
      "learning_rate": 2.371364653243848e-05,
      "loss": 3.4564,
      "step": 19700
    },
    {
      "epoch": 2.651006711409396,
      "grad_norm": 1.3033480644226074,
      "learning_rate": 2.32662192393736e-05,
      "loss": 3.4663,
      "step": 19750
    },
    {
      "epoch": 2.6577181208053693,
      "grad_norm": 1.4400330781936646,
      "learning_rate": 2.2818791946308727e-05,
      "loss": 3.46,
      "step": 19800
    },
    {
      "epoch": 2.6644295302013425,
      "grad_norm": 1.3913419246673584,
      "learning_rate": 2.237136465324385e-05,
      "loss": 3.4601,
      "step": 19850
    },
    {
      "epoch": 2.6711409395973154,
      "grad_norm": 1.3698569536209106,
      "learning_rate": 2.192393736017897e-05,
      "loss": 3.5015,
      "step": 19900
    },
    {
      "epoch": 2.6778523489932886,
      "grad_norm": 1.3788982629776,
      "learning_rate": 2.1476510067114094e-05,
      "loss": 3.473,
      "step": 19950
    },
    {
      "epoch": 2.684563758389262,
      "grad_norm": 1.35898756980896,
      "learning_rate": 2.102908277404922e-05,
      "loss": 3.4589,
      "step": 20000
    },
    {
      "epoch": 2.684563758389262,
      "eval_loss": 3.7020397186279297,
      "eval_runtime": 144.1365,
      "eval_samples_per_second": 51.299,
      "eval_steps_per_second": 6.418,
      "step": 20000
    },
    {
      "epoch": 2.6912751677852347,
      "grad_norm": 1.383773922920227,
      "learning_rate": 2.0581655480984342e-05,
      "loss": 3.4592,
      "step": 20050
    },
    {
      "epoch": 2.697986577181208,
      "grad_norm": 1.4759602546691895,
      "learning_rate": 2.013422818791946e-05,
      "loss": 3.4416,
      "step": 20100
    },
    {
      "epoch": 2.704697986577181,
      "grad_norm": 1.516384243965149,
      "learning_rate": 1.9686800894854587e-05,
      "loss": 3.4187,
      "step": 20150
    },
    {
      "epoch": 2.7114093959731544,
      "grad_norm": 1.3972305059432983,
      "learning_rate": 1.923937360178971e-05,
      "loss": 3.4699,
      "step": 20200
    },
    {
      "epoch": 2.7181208053691277,
      "grad_norm": 1.3743888139724731,
      "learning_rate": 1.8791946308724835e-05,
      "loss": 3.4943,
      "step": 20250
    },
    {
      "epoch": 2.7248322147651005,
      "grad_norm": 1.3856157064437866,
      "learning_rate": 1.8344519015659954e-05,
      "loss": 3.4133,
      "step": 20300
    },
    {
      "epoch": 2.7315436241610738,
      "grad_norm": 1.4271527528762817,
      "learning_rate": 1.789709172259508e-05,
      "loss": 3.4189,
      "step": 20350
    },
    {
      "epoch": 2.738255033557047,
      "grad_norm": 1.3398560285568237,
      "learning_rate": 1.7449664429530202e-05,
      "loss": 3.4077,
      "step": 20400
    },
    {
      "epoch": 2.7449664429530203,
      "grad_norm": 1.3171736001968384,
      "learning_rate": 1.7002237136465328e-05,
      "loss": 3.4611,
      "step": 20450
    },
    {
      "epoch": 2.751677852348993,
      "grad_norm": 1.4460121393203735,
      "learning_rate": 1.6554809843400447e-05,
      "loss": 3.4399,
      "step": 20500
    },
    {
      "epoch": 2.7583892617449663,
      "grad_norm": 1.4714581966400146,
      "learning_rate": 1.610738255033557e-05,
      "loss": 3.4483,
      "step": 20550
    },
    {
      "epoch": 2.7651006711409396,
      "grad_norm": 1.401661992073059,
      "learning_rate": 1.5659955257270695e-05,
      "loss": 3.4713,
      "step": 20600
    },
    {
      "epoch": 2.771812080536913,
      "grad_norm": 1.3285635709762573,
      "learning_rate": 1.5212527964205816e-05,
      "loss": 3.4443,
      "step": 20650
    },
    {
      "epoch": 2.778523489932886,
      "grad_norm": 1.3677481412887573,
      "learning_rate": 1.4765100671140942e-05,
      "loss": 3.4087,
      "step": 20700
    },
    {
      "epoch": 2.785234899328859,
      "grad_norm": 1.3105510473251343,
      "learning_rate": 1.4317673378076063e-05,
      "loss": 3.435,
      "step": 20750
    },
    {
      "epoch": 2.791946308724832,
      "grad_norm": 1.456135869026184,
      "learning_rate": 1.3870246085011188e-05,
      "loss": 3.4218,
      "step": 20800
    },
    {
      "epoch": 2.7986577181208054,
      "grad_norm": 1.3782716989517212,
      "learning_rate": 1.3422818791946309e-05,
      "loss": 3.4215,
      "step": 20850
    },
    {
      "epoch": 2.8053691275167782,
      "grad_norm": 1.406241536140442,
      "learning_rate": 1.2975391498881432e-05,
      "loss": 3.4347,
      "step": 20900
    },
    {
      "epoch": 2.8120805369127515,
      "grad_norm": 1.3114354610443115,
      "learning_rate": 1.2527964205816556e-05,
      "loss": 3.4145,
      "step": 20950
    },
    {
      "epoch": 2.8187919463087248,
      "grad_norm": 1.3499128818511963,
      "learning_rate": 1.208053691275168e-05,
      "loss": 3.4605,
      "step": 21000
    },
    {
      "epoch": 2.825503355704698,
      "grad_norm": 1.3757699728012085,
      "learning_rate": 1.16331096196868e-05,
      "loss": 3.4817,
      "step": 21050
    },
    {
      "epoch": 2.8322147651006713,
      "grad_norm": 1.2636085748672485,
      "learning_rate": 1.1185682326621925e-05,
      "loss": 3.403,
      "step": 21100
    },
    {
      "epoch": 2.8389261744966445,
      "grad_norm": 1.3841673135757446,
      "learning_rate": 1.0738255033557047e-05,
      "loss": 3.4478,
      "step": 21150
    },
    {
      "epoch": 2.8456375838926173,
      "grad_norm": 1.8389960527420044,
      "learning_rate": 1.0290827740492171e-05,
      "loss": 3.4374,
      "step": 21200
    },
    {
      "epoch": 2.8523489932885906,
      "grad_norm": 1.2787665128707886,
      "learning_rate": 9.843400447427293e-06,
      "loss": 3.4868,
      "step": 21250
    },
    {
      "epoch": 2.859060402684564,
      "grad_norm": 1.383015513420105,
      "learning_rate": 9.395973154362418e-06,
      "loss": 3.4514,
      "step": 21300
    },
    {
      "epoch": 2.8657718120805367,
      "grad_norm": 1.4514954090118408,
      "learning_rate": 8.94854586129754e-06,
      "loss": 3.4975,
      "step": 21350
    },
    {
      "epoch": 2.87248322147651,
      "grad_norm": 1.4523814916610718,
      "learning_rate": 8.501118568232664e-06,
      "loss": 3.4921,
      "step": 21400
    },
    {
      "epoch": 2.879194630872483,
      "grad_norm": 1.4232310056686401,
      "learning_rate": 8.053691275167785e-06,
      "loss": 3.4316,
      "step": 21450
    },
    {
      "epoch": 2.8859060402684564,
      "grad_norm": 1.485573649406433,
      "learning_rate": 7.606263982102908e-06,
      "loss": 3.3985,
      "step": 21500
    },
    {
      "epoch": 2.8926174496644297,
      "grad_norm": 1.3647847175598145,
      "learning_rate": 7.158836689038031e-06,
      "loss": 3.4787,
      "step": 21550
    },
    {
      "epoch": 2.899328859060403,
      "grad_norm": 1.2750236988067627,
      "learning_rate": 6.7114093959731546e-06,
      "loss": 3.4334,
      "step": 21600
    },
    {
      "epoch": 2.9060402684563758,
      "grad_norm": 1.419232726097107,
      "learning_rate": 6.263982102908278e-06,
      "loss": 3.5027,
      "step": 21650
    },
    {
      "epoch": 2.912751677852349,
      "grad_norm": 1.3848015069961548,
      "learning_rate": 5.8165548098434e-06,
      "loss": 3.4234,
      "step": 21700
    },
    {
      "epoch": 2.9194630872483223,
      "grad_norm": 1.3751074075698853,
      "learning_rate": 5.3691275167785235e-06,
      "loss": 3.4195,
      "step": 21750
    },
    {
      "epoch": 2.926174496644295,
      "grad_norm": 1.4722973108291626,
      "learning_rate": 4.921700223713647e-06,
      "loss": 3.4397,
      "step": 21800
    },
    {
      "epoch": 2.9328859060402683,
      "grad_norm": 1.3173731565475464,
      "learning_rate": 4.47427293064877e-06,
      "loss": 3.4386,
      "step": 21850
    },
    {
      "epoch": 2.9395973154362416,
      "grad_norm": 1.3810417652130127,
      "learning_rate": 4.026845637583892e-06,
      "loss": 3.411,
      "step": 21900
    },
    {
      "epoch": 2.946308724832215,
      "grad_norm": 1.3764430284500122,
      "learning_rate": 3.5794183445190157e-06,
      "loss": 3.3738,
      "step": 21950
    },
    {
      "epoch": 2.953020134228188,
      "grad_norm": 1.3165397644042969,
      "learning_rate": 3.131991051454139e-06,
      "loss": 3.5129,
      "step": 22000
    },
    {
      "epoch": 2.953020134228188,
      "eval_loss": 3.6929149627685547,
      "eval_runtime": 144.2526,
      "eval_samples_per_second": 51.257,
      "eval_steps_per_second": 6.412,
      "step": 22000
    }
  ],
  "logging_steps": 50,
  "max_steps": 22350,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 2000,
  "total_flos": 1.199016804530258e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
