{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.8791946308724832,
  "eval_steps": 3500,
  "global_step": 14000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006711409395973154,
      "grad_norm": 9.76291561126709,
      "learning_rate": 0.00019932885906040267,
      "loss": 6.3744,
      "step": 50
    },
    {
      "epoch": 0.013422818791946308,
      "grad_norm": 6.147958278656006,
      "learning_rate": 0.0001986577181208054,
      "loss": 5.2863,
      "step": 100
    },
    {
      "epoch": 0.020134228187919462,
      "grad_norm": 5.431545734405518,
      "learning_rate": 0.00019798657718120806,
      "loss": 5.0889,
      "step": 150
    },
    {
      "epoch": 0.026845637583892617,
      "grad_norm": 3.9404029846191406,
      "learning_rate": 0.00019731543624161075,
      "loss": 4.7939,
      "step": 200
    },
    {
      "epoch": 0.03355704697986577,
      "grad_norm": 3.8592262268066406,
      "learning_rate": 0.00019664429530201342,
      "loss": 4.5786,
      "step": 250
    },
    {
      "epoch": 0.040268456375838924,
      "grad_norm": 3.698657989501953,
      "learning_rate": 0.00019597315436241613,
      "loss": 4.4425,
      "step": 300
    },
    {
      "epoch": 0.04697986577181208,
      "grad_norm": 3.2288243770599365,
      "learning_rate": 0.0001953020134228188,
      "loss": 4.4089,
      "step": 350
    },
    {
      "epoch": 0.053691275167785234,
      "grad_norm": 2.7144501209259033,
      "learning_rate": 0.0001946308724832215,
      "loss": 4.3284,
      "step": 400
    },
    {
      "epoch": 0.06040268456375839,
      "grad_norm": 3.168605327606201,
      "learning_rate": 0.00019395973154362416,
      "loss": 4.2628,
      "step": 450
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 2.6436657905578613,
      "learning_rate": 0.00019328859060402688,
      "loss": 4.264,
      "step": 500
    },
    {
      "epoch": 0.0738255033557047,
      "grad_norm": 3.1202750205993652,
      "learning_rate": 0.00019261744966442954,
      "loss": 4.1816,
      "step": 550
    },
    {
      "epoch": 0.08053691275167785,
      "grad_norm": 4.323246479034424,
      "learning_rate": 0.0001919463087248322,
      "loss": 4.1715,
      "step": 600
    },
    {
      "epoch": 0.087248322147651,
      "grad_norm": 2.914849042892456,
      "learning_rate": 0.0001912751677852349,
      "loss": 4.1779,
      "step": 650
    },
    {
      "epoch": 0.09395973154362416,
      "grad_norm": 2.5681986808776855,
      "learning_rate": 0.0001906040268456376,
      "loss": 4.1389,
      "step": 700
    },
    {
      "epoch": 0.10067114093959731,
      "grad_norm": 2.550813674926758,
      "learning_rate": 0.00018993288590604028,
      "loss": 4.1075,
      "step": 750
    },
    {
      "epoch": 0.10738255033557047,
      "grad_norm": 2.363677978515625,
      "learning_rate": 0.00018926174496644295,
      "loss": 4.097,
      "step": 800
    },
    {
      "epoch": 0.11409395973154363,
      "grad_norm": 2.320943832397461,
      "learning_rate": 0.00018859060402684564,
      "loss": 4.0895,
      "step": 850
    },
    {
      "epoch": 0.12080536912751678,
      "grad_norm": 2.3433618545532227,
      "learning_rate": 0.00018791946308724833,
      "loss": 4.0696,
      "step": 900
    },
    {
      "epoch": 0.12751677852348994,
      "grad_norm": 2.233490467071533,
      "learning_rate": 0.00018724832214765102,
      "loss": 4.051,
      "step": 950
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 2.341381072998047,
      "learning_rate": 0.0001865771812080537,
      "loss": 4.0377,
      "step": 1000
    },
    {
      "epoch": 0.14093959731543623,
      "grad_norm": 2.9077939987182617,
      "learning_rate": 0.00018590604026845638,
      "loss": 4.0099,
      "step": 1050
    },
    {
      "epoch": 0.1476510067114094,
      "grad_norm": 2.1887967586517334,
      "learning_rate": 0.00018523489932885907,
      "loss": 4.0124,
      "step": 1100
    },
    {
      "epoch": 0.15436241610738255,
      "grad_norm": 2.6332523822784424,
      "learning_rate": 0.00018456375838926174,
      "loss": 4.0078,
      "step": 1150
    },
    {
      "epoch": 0.1610738255033557,
      "grad_norm": 2.3427298069000244,
      "learning_rate": 0.00018389261744966443,
      "loss": 4.0036,
      "step": 1200
    },
    {
      "epoch": 0.16778523489932887,
      "grad_norm": 2.49576997756958,
      "learning_rate": 0.00018322147651006712,
      "loss": 4.0017,
      "step": 1250
    },
    {
      "epoch": 0.174496644295302,
      "grad_norm": 2.27463436126709,
      "learning_rate": 0.00018255033557046981,
      "loss": 3.9747,
      "step": 1300
    },
    {
      "epoch": 0.18120805369127516,
      "grad_norm": 2.0607120990753174,
      "learning_rate": 0.00018187919463087248,
      "loss": 4.009,
      "step": 1350
    },
    {
      "epoch": 0.18791946308724833,
      "grad_norm": 2.1656742095947266,
      "learning_rate": 0.00018120805369127517,
      "loss": 3.9373,
      "step": 1400
    },
    {
      "epoch": 0.19463087248322147,
      "grad_norm": 2.2813687324523926,
      "learning_rate": 0.00018053691275167786,
      "loss": 3.9853,
      "step": 1450
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 2.083205223083496,
      "learning_rate": 0.00017986577181208056,
      "loss": 3.9077,
      "step": 1500
    },
    {
      "epoch": 0.2080536912751678,
      "grad_norm": 2.522930145263672,
      "learning_rate": 0.00017919463087248322,
      "loss": 3.944,
      "step": 1550
    },
    {
      "epoch": 0.21476510067114093,
      "grad_norm": 1.9890559911727905,
      "learning_rate": 0.0001785234899328859,
      "loss": 3.9472,
      "step": 1600
    },
    {
      "epoch": 0.2214765100671141,
      "grad_norm": 2.7502059936523438,
      "learning_rate": 0.0001778523489932886,
      "loss": 3.9943,
      "step": 1650
    },
    {
      "epoch": 0.22818791946308725,
      "grad_norm": 2.2661354541778564,
      "learning_rate": 0.0001771812080536913,
      "loss": 3.9344,
      "step": 1700
    },
    {
      "epoch": 0.2348993288590604,
      "grad_norm": 1.9702156782150269,
      "learning_rate": 0.00017651006711409396,
      "loss": 3.9075,
      "step": 1750
    },
    {
      "epoch": 0.24161073825503357,
      "grad_norm": 1.967592716217041,
      "learning_rate": 0.00017583892617449665,
      "loss": 3.9332,
      "step": 1800
    },
    {
      "epoch": 0.2483221476510067,
      "grad_norm": 1.9875054359436035,
      "learning_rate": 0.00017516778523489935,
      "loss": 3.9302,
      "step": 1850
    },
    {
      "epoch": 0.2550335570469799,
      "grad_norm": 2.074561834335327,
      "learning_rate": 0.000174496644295302,
      "loss": 3.8833,
      "step": 1900
    },
    {
      "epoch": 0.26174496644295303,
      "grad_norm": 2.250723123550415,
      "learning_rate": 0.0001738255033557047,
      "loss": 3.9434,
      "step": 1950
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 2.1008102893829346,
      "learning_rate": 0.0001731543624161074,
      "loss": 3.8837,
      "step": 2000
    },
    {
      "epoch": 0.2751677852348993,
      "grad_norm": 2.4409961700439453,
      "learning_rate": 0.0001724832214765101,
      "loss": 3.9174,
      "step": 2050
    },
    {
      "epoch": 0.28187919463087246,
      "grad_norm": 2.200366258621216,
      "learning_rate": 0.00017181208053691275,
      "loss": 3.8413,
      "step": 2100
    },
    {
      "epoch": 0.28859060402684567,
      "grad_norm": 1.953946590423584,
      "learning_rate": 0.00017114093959731544,
      "loss": 3.8873,
      "step": 2150
    },
    {
      "epoch": 0.2953020134228188,
      "grad_norm": 2.2515640258789062,
      "learning_rate": 0.00017046979865771814,
      "loss": 3.8174,
      "step": 2200
    },
    {
      "epoch": 0.30201342281879195,
      "grad_norm": 1.904086947441101,
      "learning_rate": 0.00016979865771812083,
      "loss": 3.8626,
      "step": 2250
    },
    {
      "epoch": 0.3087248322147651,
      "grad_norm": 1.9325824975967407,
      "learning_rate": 0.0001691275167785235,
      "loss": 3.93,
      "step": 2300
    },
    {
      "epoch": 0.31543624161073824,
      "grad_norm": 2.224473476409912,
      "learning_rate": 0.00016845637583892619,
      "loss": 3.8731,
      "step": 2350
    },
    {
      "epoch": 0.3221476510067114,
      "grad_norm": 1.8665449619293213,
      "learning_rate": 0.00016778523489932888,
      "loss": 3.8862,
      "step": 2400
    },
    {
      "epoch": 0.3288590604026846,
      "grad_norm": 2.212570905685425,
      "learning_rate": 0.00016711409395973154,
      "loss": 3.9278,
      "step": 2450
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 2.3300766944885254,
      "learning_rate": 0.00016644295302013423,
      "loss": 3.8391,
      "step": 2500
    },
    {
      "epoch": 0.3422818791946309,
      "grad_norm": 2.237867832183838,
      "learning_rate": 0.00016577181208053693,
      "loss": 3.8496,
      "step": 2550
    },
    {
      "epoch": 0.348993288590604,
      "grad_norm": 2.131697654724121,
      "learning_rate": 0.00016510067114093962,
      "loss": 3.8246,
      "step": 2600
    },
    {
      "epoch": 0.35570469798657717,
      "grad_norm": 2.215324878692627,
      "learning_rate": 0.00016442953020134228,
      "loss": 3.866,
      "step": 2650
    },
    {
      "epoch": 0.3624161073825503,
      "grad_norm": 2.4978621006011963,
      "learning_rate": 0.00016375838926174498,
      "loss": 3.838,
      "step": 2700
    },
    {
      "epoch": 0.3691275167785235,
      "grad_norm": 1.9225152730941772,
      "learning_rate": 0.00016308724832214767,
      "loss": 3.8938,
      "step": 2750
    },
    {
      "epoch": 0.37583892617449666,
      "grad_norm": 2.0942625999450684,
      "learning_rate": 0.00016241610738255036,
      "loss": 3.8599,
      "step": 2800
    },
    {
      "epoch": 0.3825503355704698,
      "grad_norm": 2.0202109813690186,
      "learning_rate": 0.00016174496644295302,
      "loss": 3.7686,
      "step": 2850
    },
    {
      "epoch": 0.38926174496644295,
      "grad_norm": 1.8244993686676025,
      "learning_rate": 0.0001610738255033557,
      "loss": 3.8712,
      "step": 2900
    },
    {
      "epoch": 0.3959731543624161,
      "grad_norm": 1.7788095474243164,
      "learning_rate": 0.0001604026845637584,
      "loss": 3.8156,
      "step": 2950
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 2.077664375305176,
      "learning_rate": 0.00015973154362416107,
      "loss": 3.7651,
      "step": 3000
    },
    {
      "epoch": 0.40939597315436244,
      "grad_norm": 1.8456828594207764,
      "learning_rate": 0.00015906040268456377,
      "loss": 3.8081,
      "step": 3050
    },
    {
      "epoch": 0.4161073825503356,
      "grad_norm": 1.816679835319519,
      "learning_rate": 0.00015838926174496643,
      "loss": 3.841,
      "step": 3100
    },
    {
      "epoch": 0.4228187919463087,
      "grad_norm": 2.017704963684082,
      "learning_rate": 0.00015771812080536915,
      "loss": 3.839,
      "step": 3150
    },
    {
      "epoch": 0.42953020134228187,
      "grad_norm": 1.7677229642868042,
      "learning_rate": 0.00015704697986577181,
      "loss": 3.8112,
      "step": 3200
    },
    {
      "epoch": 0.436241610738255,
      "grad_norm": 2.0390050411224365,
      "learning_rate": 0.0001563758389261745,
      "loss": 3.8483,
      "step": 3250
    },
    {
      "epoch": 0.4429530201342282,
      "grad_norm": 1.9494991302490234,
      "learning_rate": 0.0001557046979865772,
      "loss": 3.8103,
      "step": 3300
    },
    {
      "epoch": 0.44966442953020136,
      "grad_norm": 2.0421464443206787,
      "learning_rate": 0.0001550335570469799,
      "loss": 3.8279,
      "step": 3350
    },
    {
      "epoch": 0.4563758389261745,
      "grad_norm": 1.8679497241973877,
      "learning_rate": 0.00015436241610738256,
      "loss": 3.8216,
      "step": 3400
    },
    {
      "epoch": 0.46308724832214765,
      "grad_norm": 1.9072271585464478,
      "learning_rate": 0.00015369127516778522,
      "loss": 3.8172,
      "step": 3450
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 2.9276883602142334,
      "learning_rate": 0.00015302013422818794,
      "loss": 3.7794,
      "step": 3500
    },
    {
      "epoch": 0.4697986577181208,
      "eval_loss": 3.819903612136841,
      "eval_runtime": 157.4728,
      "eval_samples_per_second": 46.954,
      "eval_steps_per_second": 5.874,
      "step": 3500
    },
    {
      "epoch": 0.47651006711409394,
      "grad_norm": 1.9343053102493286,
      "learning_rate": 0.0001523489932885906,
      "loss": 3.8335,
      "step": 3550
    },
    {
      "epoch": 0.48322147651006714,
      "grad_norm": 1.9856635332107544,
      "learning_rate": 0.0001516778523489933,
      "loss": 3.7995,
      "step": 3600
    },
    {
      "epoch": 0.4899328859060403,
      "grad_norm": 1.7829244136810303,
      "learning_rate": 0.00015100671140939596,
      "loss": 3.7461,
      "step": 3650
    },
    {
      "epoch": 0.4966442953020134,
      "grad_norm": 2.0373854637145996,
      "learning_rate": 0.00015033557046979868,
      "loss": 3.7912,
      "step": 3700
    },
    {
      "epoch": 0.5033557046979866,
      "grad_norm": 2.3217556476593018,
      "learning_rate": 0.00014966442953020135,
      "loss": 3.7968,
      "step": 3750
    },
    {
      "epoch": 0.5100671140939598,
      "grad_norm": 1.9560930728912354,
      "learning_rate": 0.00014899328859060404,
      "loss": 3.7598,
      "step": 3800
    },
    {
      "epoch": 0.5167785234899329,
      "grad_norm": 1.9263492822647095,
      "learning_rate": 0.0001483221476510067,
      "loss": 3.7048,
      "step": 3850
    },
    {
      "epoch": 0.5234899328859061,
      "grad_norm": 2.1479504108428955,
      "learning_rate": 0.00014765100671140942,
      "loss": 3.7715,
      "step": 3900
    },
    {
      "epoch": 0.5302013422818792,
      "grad_norm": 1.899453043937683,
      "learning_rate": 0.0001469798657718121,
      "loss": 3.7414,
      "step": 3950
    },
    {
      "epoch": 0.5369127516778524,
      "grad_norm": 1.9471064805984497,
      "learning_rate": 0.00014630872483221478,
      "loss": 3.7737,
      "step": 4000
    },
    {
      "epoch": 0.5436241610738255,
      "grad_norm": 2.07108473777771,
      "learning_rate": 0.00014563758389261744,
      "loss": 3.8278,
      "step": 4050
    },
    {
      "epoch": 0.5503355704697986,
      "grad_norm": 2.0954501628875732,
      "learning_rate": 0.00014496644295302014,
      "loss": 3.7693,
      "step": 4100
    },
    {
      "epoch": 0.5570469798657718,
      "grad_norm": 1.6807000637054443,
      "learning_rate": 0.00014429530201342283,
      "loss": 3.7349,
      "step": 4150
    },
    {
      "epoch": 0.5637583892617449,
      "grad_norm": 1.8638263940811157,
      "learning_rate": 0.0001436241610738255,
      "loss": 3.8546,
      "step": 4200
    },
    {
      "epoch": 0.5704697986577181,
      "grad_norm": 2.1892662048339844,
      "learning_rate": 0.00014295302013422819,
      "loss": 3.7528,
      "step": 4250
    },
    {
      "epoch": 0.5771812080536913,
      "grad_norm": 2.7254104614257812,
      "learning_rate": 0.00014228187919463088,
      "loss": 3.7879,
      "step": 4300
    },
    {
      "epoch": 0.5838926174496645,
      "grad_norm": 1.7408696413040161,
      "learning_rate": 0.00014161073825503357,
      "loss": 3.7179,
      "step": 4350
    },
    {
      "epoch": 0.5906040268456376,
      "grad_norm": 1.9033234119415283,
      "learning_rate": 0.00014093959731543624,
      "loss": 3.7662,
      "step": 4400
    },
    {
      "epoch": 0.5973154362416108,
      "grad_norm": 2.1683738231658936,
      "learning_rate": 0.00014026845637583895,
      "loss": 3.7583,
      "step": 4450
    },
    {
      "epoch": 0.6040268456375839,
      "grad_norm": 1.9414604902267456,
      "learning_rate": 0.00013959731543624162,
      "loss": 3.728,
      "step": 4500
    },
    {
      "epoch": 0.610738255033557,
      "grad_norm": 1.6500420570373535,
      "learning_rate": 0.0001389261744966443,
      "loss": 3.7271,
      "step": 4550
    },
    {
      "epoch": 0.6174496644295302,
      "grad_norm": 1.9738832712173462,
      "learning_rate": 0.00013825503355704698,
      "loss": 3.7536,
      "step": 4600
    },
    {
      "epoch": 0.6241610738255033,
      "grad_norm": 1.9532337188720703,
      "learning_rate": 0.00013758389261744967,
      "loss": 3.7965,
      "step": 4650
    },
    {
      "epoch": 0.6308724832214765,
      "grad_norm": 1.915177822113037,
      "learning_rate": 0.00013691275167785236,
      "loss": 3.7685,
      "step": 4700
    },
    {
      "epoch": 0.6375838926174496,
      "grad_norm": 1.591180682182312,
      "learning_rate": 0.00013624161073825503,
      "loss": 3.7537,
      "step": 4750
    },
    {
      "epoch": 0.6442953020134228,
      "grad_norm": 1.8513423204421997,
      "learning_rate": 0.00013557046979865772,
      "loss": 3.7878,
      "step": 4800
    },
    {
      "epoch": 0.6510067114093959,
      "grad_norm": 2.123608350753784,
      "learning_rate": 0.0001348993288590604,
      "loss": 3.7452,
      "step": 4850
    },
    {
      "epoch": 0.6577181208053692,
      "grad_norm": 1.9018489122390747,
      "learning_rate": 0.0001342281879194631,
      "loss": 3.7639,
      "step": 4900
    },
    {
      "epoch": 0.6644295302013423,
      "grad_norm": 2.0423192977905273,
      "learning_rate": 0.00013355704697986577,
      "loss": 3.7226,
      "step": 4950
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 1.7277112007141113,
      "learning_rate": 0.00013288590604026846,
      "loss": 3.738,
      "step": 5000
    },
    {
      "epoch": 0.6778523489932886,
      "grad_norm": 1.8612254858016968,
      "learning_rate": 0.00013221476510067115,
      "loss": 3.7708,
      "step": 5050
    },
    {
      "epoch": 0.6845637583892618,
      "grad_norm": 2.0442166328430176,
      "learning_rate": 0.00013154362416107384,
      "loss": 3.7755,
      "step": 5100
    },
    {
      "epoch": 0.6912751677852349,
      "grad_norm": 1.8707512617111206,
      "learning_rate": 0.0001308724832214765,
      "loss": 3.7641,
      "step": 5150
    },
    {
      "epoch": 0.697986577181208,
      "grad_norm": 1.8773969411849976,
      "learning_rate": 0.0001302013422818792,
      "loss": 3.7439,
      "step": 5200
    },
    {
      "epoch": 0.7046979865771812,
      "grad_norm": 1.8990896940231323,
      "learning_rate": 0.0001295302013422819,
      "loss": 3.7345,
      "step": 5250
    },
    {
      "epoch": 0.7114093959731543,
      "grad_norm": 1.754605770111084,
      "learning_rate": 0.00012885906040268456,
      "loss": 3.7968,
      "step": 5300
    },
    {
      "epoch": 0.7181208053691275,
      "grad_norm": 1.8087323904037476,
      "learning_rate": 0.00012818791946308725,
      "loss": 3.6892,
      "step": 5350
    },
    {
      "epoch": 0.7248322147651006,
      "grad_norm": 1.7670347690582275,
      "learning_rate": 0.00012751677852348994,
      "loss": 3.7949,
      "step": 5400
    },
    {
      "epoch": 0.7315436241610739,
      "grad_norm": 1.7536414861679077,
      "learning_rate": 0.00012684563758389263,
      "loss": 3.7019,
      "step": 5450
    },
    {
      "epoch": 0.738255033557047,
      "grad_norm": 1.8770837783813477,
      "learning_rate": 0.0001261744966442953,
      "loss": 3.7809,
      "step": 5500
    },
    {
      "epoch": 0.7449664429530202,
      "grad_norm": 1.7503114938735962,
      "learning_rate": 0.000125503355704698,
      "loss": 3.7593,
      "step": 5550
    },
    {
      "epoch": 0.7516778523489933,
      "grad_norm": 1.8459446430206299,
      "learning_rate": 0.00012483221476510068,
      "loss": 3.7182,
      "step": 5600
    },
    {
      "epoch": 0.7583892617449665,
      "grad_norm": 1.7982112169265747,
      "learning_rate": 0.00012416107382550337,
      "loss": 3.7317,
      "step": 5650
    },
    {
      "epoch": 0.7651006711409396,
      "grad_norm": 1.994579792022705,
      "learning_rate": 0.00012348993288590604,
      "loss": 3.7181,
      "step": 5700
    },
    {
      "epoch": 0.7718120805369127,
      "grad_norm": 1.7562785148620605,
      "learning_rate": 0.00012281879194630873,
      "loss": 3.7201,
      "step": 5750
    },
    {
      "epoch": 0.7785234899328859,
      "grad_norm": 1.8677860498428345,
      "learning_rate": 0.00012214765100671142,
      "loss": 3.7205,
      "step": 5800
    },
    {
      "epoch": 0.785234899328859,
      "grad_norm": 1.5928508043289185,
      "learning_rate": 0.00012147651006711409,
      "loss": 3.6837,
      "step": 5850
    },
    {
      "epoch": 0.7919463087248322,
      "grad_norm": 1.8394744396209717,
      "learning_rate": 0.0001208053691275168,
      "loss": 3.738,
      "step": 5900
    },
    {
      "epoch": 0.7986577181208053,
      "grad_norm": 1.6827718019485474,
      "learning_rate": 0.00012013422818791946,
      "loss": 3.7434,
      "step": 5950
    },
    {
      "epoch": 0.8053691275167785,
      "grad_norm": 1.8026622533798218,
      "learning_rate": 0.00011946308724832216,
      "loss": 3.7436,
      "step": 6000
    },
    {
      "epoch": 0.8120805369127517,
      "grad_norm": 1.93060302734375,
      "learning_rate": 0.00011879194630872483,
      "loss": 3.6676,
      "step": 6050
    },
    {
      "epoch": 0.8187919463087249,
      "grad_norm": 1.832889199256897,
      "learning_rate": 0.00011812080536912754,
      "loss": 3.7323,
      "step": 6100
    },
    {
      "epoch": 0.825503355704698,
      "grad_norm": 1.95548415184021,
      "learning_rate": 0.0001174496644295302,
      "loss": 3.6927,
      "step": 6150
    },
    {
      "epoch": 0.8322147651006712,
      "grad_norm": 1.7701032161712646,
      "learning_rate": 0.0001167785234899329,
      "loss": 3.6612,
      "step": 6200
    },
    {
      "epoch": 0.8389261744966443,
      "grad_norm": 1.810914158821106,
      "learning_rate": 0.00011610738255033557,
      "loss": 3.6991,
      "step": 6250
    },
    {
      "epoch": 0.8456375838926175,
      "grad_norm": 1.7224668264389038,
      "learning_rate": 0.00011543624161073828,
      "loss": 3.6973,
      "step": 6300
    },
    {
      "epoch": 0.8523489932885906,
      "grad_norm": 1.694105863571167,
      "learning_rate": 0.00011476510067114094,
      "loss": 3.7326,
      "step": 6350
    },
    {
      "epoch": 0.8590604026845637,
      "grad_norm": 1.7891417741775513,
      "learning_rate": 0.00011409395973154362,
      "loss": 3.6984,
      "step": 6400
    },
    {
      "epoch": 0.8657718120805369,
      "grad_norm": 1.8742501735687256,
      "learning_rate": 0.00011342281879194631,
      "loss": 3.6438,
      "step": 6450
    },
    {
      "epoch": 0.87248322147651,
      "grad_norm": 1.6098870038986206,
      "learning_rate": 0.00011275167785234899,
      "loss": 3.7155,
      "step": 6500
    },
    {
      "epoch": 0.8791946308724832,
      "grad_norm": 1.648972749710083,
      "learning_rate": 0.00011208053691275168,
      "loss": 3.7358,
      "step": 6550
    },
    {
      "epoch": 0.8859060402684564,
      "grad_norm": 1.773345708847046,
      "learning_rate": 0.00011140939597315436,
      "loss": 3.6344,
      "step": 6600
    },
    {
      "epoch": 0.8926174496644296,
      "grad_norm": 1.688447117805481,
      "learning_rate": 0.00011073825503355705,
      "loss": 3.6634,
      "step": 6650
    },
    {
      "epoch": 0.8993288590604027,
      "grad_norm": 1.6986675262451172,
      "learning_rate": 0.00011006711409395973,
      "loss": 3.6835,
      "step": 6700
    },
    {
      "epoch": 0.9060402684563759,
      "grad_norm": 1.9521842002868652,
      "learning_rate": 0.00010939597315436242,
      "loss": 3.6643,
      "step": 6750
    },
    {
      "epoch": 0.912751677852349,
      "grad_norm": 1.933103084564209,
      "learning_rate": 0.0001087248322147651,
      "loss": 3.704,
      "step": 6800
    },
    {
      "epoch": 0.9194630872483222,
      "grad_norm": 1.788414716720581,
      "learning_rate": 0.0001080536912751678,
      "loss": 3.6997,
      "step": 6850
    },
    {
      "epoch": 0.9261744966442953,
      "grad_norm": 1.689279556274414,
      "learning_rate": 0.00010738255033557047,
      "loss": 3.6812,
      "step": 6900
    },
    {
      "epoch": 0.9328859060402684,
      "grad_norm": 1.7318956851959229,
      "learning_rate": 0.00010671140939597315,
      "loss": 3.6802,
      "step": 6950
    },
    {
      "epoch": 0.9395973154362416,
      "grad_norm": 1.7405134439468384,
      "learning_rate": 0.00010604026845637584,
      "loss": 3.7215,
      "step": 7000
    },
    {
      "epoch": 0.9395973154362416,
      "eval_loss": 3.7047133445739746,
      "eval_runtime": 157.6355,
      "eval_samples_per_second": 46.906,
      "eval_steps_per_second": 5.868,
      "step": 7000
    },
    {
      "epoch": 0.9463087248322147,
      "grad_norm": 1.810361623764038,
      "learning_rate": 0.00010536912751677852,
      "loss": 3.6291,
      "step": 7050
    },
    {
      "epoch": 0.9530201342281879,
      "grad_norm": 1.6685844659805298,
      "learning_rate": 0.00010469798657718121,
      "loss": 3.6741,
      "step": 7100
    },
    {
      "epoch": 0.959731543624161,
      "grad_norm": 1.8416473865509033,
      "learning_rate": 0.00010402684563758389,
      "loss": 3.6464,
      "step": 7150
    },
    {
      "epoch": 0.9664429530201343,
      "grad_norm": 1.5847303867340088,
      "learning_rate": 0.00010335570469798659,
      "loss": 3.6842,
      "step": 7200
    },
    {
      "epoch": 0.9731543624161074,
      "grad_norm": 2.1048407554626465,
      "learning_rate": 0.00010268456375838926,
      "loss": 3.704,
      "step": 7250
    },
    {
      "epoch": 0.9798657718120806,
      "grad_norm": 1.5975180864334106,
      "learning_rate": 0.00010201342281879196,
      "loss": 3.6201,
      "step": 7300
    },
    {
      "epoch": 0.9865771812080537,
      "grad_norm": 1.8556488752365112,
      "learning_rate": 0.00010134228187919463,
      "loss": 3.6583,
      "step": 7350
    },
    {
      "epoch": 0.9932885906040269,
      "grad_norm": 1.8127460479736328,
      "learning_rate": 0.00010067114093959733,
      "loss": 3.672,
      "step": 7400
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.795249581336975,
      "learning_rate": 0.0001,
      "loss": 3.6786,
      "step": 7450
    },
    {
      "epoch": 1.0067114093959733,
      "grad_norm": 1.6772247552871704,
      "learning_rate": 9.93288590604027e-05,
      "loss": 3.5089,
      "step": 7500
    },
    {
      "epoch": 1.0134228187919463,
      "grad_norm": 1.5670970678329468,
      "learning_rate": 9.865771812080538e-05,
      "loss": 3.5366,
      "step": 7550
    },
    {
      "epoch": 1.0201342281879195,
      "grad_norm": 1.585089921951294,
      "learning_rate": 9.798657718120807e-05,
      "loss": 3.5319,
      "step": 7600
    },
    {
      "epoch": 1.0268456375838926,
      "grad_norm": 1.984673261642456,
      "learning_rate": 9.731543624161075e-05,
      "loss": 3.5461,
      "step": 7650
    },
    {
      "epoch": 1.0335570469798658,
      "grad_norm": 1.6301310062408447,
      "learning_rate": 9.664429530201344e-05,
      "loss": 3.5117,
      "step": 7700
    },
    {
      "epoch": 1.0402684563758389,
      "grad_norm": 1.8808112144470215,
      "learning_rate": 9.59731543624161e-05,
      "loss": 3.4954,
      "step": 7750
    },
    {
      "epoch": 1.0469798657718121,
      "grad_norm": 1.5190138816833496,
      "learning_rate": 9.53020134228188e-05,
      "loss": 3.5601,
      "step": 7800
    },
    {
      "epoch": 1.0536912751677852,
      "grad_norm": 1.5802603960037231,
      "learning_rate": 9.463087248322147e-05,
      "loss": 3.5674,
      "step": 7850
    },
    {
      "epoch": 1.0604026845637584,
      "grad_norm": 1.6342467069625854,
      "learning_rate": 9.395973154362417e-05,
      "loss": 3.5583,
      "step": 7900
    },
    {
      "epoch": 1.0671140939597314,
      "grad_norm": 1.664076566696167,
      "learning_rate": 9.328859060402684e-05,
      "loss": 3.5457,
      "step": 7950
    },
    {
      "epoch": 1.0738255033557047,
      "grad_norm": 1.6527574062347412,
      "learning_rate": 9.261744966442954e-05,
      "loss": 3.5525,
      "step": 8000
    },
    {
      "epoch": 1.0805369127516777,
      "grad_norm": 1.5612707138061523,
      "learning_rate": 9.194630872483221e-05,
      "loss": 3.5397,
      "step": 8050
    },
    {
      "epoch": 1.087248322147651,
      "grad_norm": 1.6974948644638062,
      "learning_rate": 9.127516778523491e-05,
      "loss": 3.5097,
      "step": 8100
    },
    {
      "epoch": 1.0939597315436242,
      "grad_norm": 1.5697314739227295,
      "learning_rate": 9.060402684563759e-05,
      "loss": 3.5232,
      "step": 8150
    },
    {
      "epoch": 1.1006711409395973,
      "grad_norm": 1.555533766746521,
      "learning_rate": 8.993288590604028e-05,
      "loss": 3.5006,
      "step": 8200
    },
    {
      "epoch": 1.1073825503355705,
      "grad_norm": 1.4820719957351685,
      "learning_rate": 8.926174496644296e-05,
      "loss": 3.5233,
      "step": 8250
    },
    {
      "epoch": 1.1140939597315436,
      "grad_norm": 1.801612377166748,
      "learning_rate": 8.859060402684565e-05,
      "loss": 3.5933,
      "step": 8300
    },
    {
      "epoch": 1.1208053691275168,
      "grad_norm": 1.595752477645874,
      "learning_rate": 8.791946308724833e-05,
      "loss": 3.5555,
      "step": 8350
    },
    {
      "epoch": 1.1275167785234899,
      "grad_norm": 1.8374154567718506,
      "learning_rate": 8.7248322147651e-05,
      "loss": 3.515,
      "step": 8400
    },
    {
      "epoch": 1.1342281879194631,
      "grad_norm": 1.474866271018982,
      "learning_rate": 8.65771812080537e-05,
      "loss": 3.5201,
      "step": 8450
    },
    {
      "epoch": 1.1409395973154361,
      "grad_norm": 1.6267457008361816,
      "learning_rate": 8.590604026845638e-05,
      "loss": 3.5937,
      "step": 8500
    },
    {
      "epoch": 1.1476510067114094,
      "grad_norm": 1.5331722497940063,
      "learning_rate": 8.523489932885907e-05,
      "loss": 3.529,
      "step": 8550
    },
    {
      "epoch": 1.1543624161073827,
      "grad_norm": 1.6845853328704834,
      "learning_rate": 8.456375838926175e-05,
      "loss": 3.5259,
      "step": 8600
    },
    {
      "epoch": 1.1610738255033557,
      "grad_norm": 1.7547121047973633,
      "learning_rate": 8.389261744966444e-05,
      "loss": 3.5232,
      "step": 8650
    },
    {
      "epoch": 1.167785234899329,
      "grad_norm": 1.6804486513137817,
      "learning_rate": 8.322147651006712e-05,
      "loss": 3.5023,
      "step": 8700
    },
    {
      "epoch": 1.174496644295302,
      "grad_norm": 1.6720974445343018,
      "learning_rate": 8.255033557046981e-05,
      "loss": 3.495,
      "step": 8750
    },
    {
      "epoch": 1.1812080536912752,
      "grad_norm": 1.8330912590026855,
      "learning_rate": 8.187919463087249e-05,
      "loss": 3.506,
      "step": 8800
    },
    {
      "epoch": 1.1879194630872483,
      "grad_norm": 1.6916418075561523,
      "learning_rate": 8.120805369127518e-05,
      "loss": 3.5049,
      "step": 8850
    },
    {
      "epoch": 1.1946308724832215,
      "grad_norm": 1.631360411643982,
      "learning_rate": 8.053691275167784e-05,
      "loss": 3.4764,
      "step": 8900
    },
    {
      "epoch": 1.2013422818791946,
      "grad_norm": 1.6502691507339478,
      "learning_rate": 7.986577181208054e-05,
      "loss": 3.5431,
      "step": 8950
    },
    {
      "epoch": 1.2080536912751678,
      "grad_norm": 1.6160414218902588,
      "learning_rate": 7.919463087248322e-05,
      "loss": 3.5258,
      "step": 9000
    },
    {
      "epoch": 1.2147651006711409,
      "grad_norm": 1.789989709854126,
      "learning_rate": 7.852348993288591e-05,
      "loss": 3.496,
      "step": 9050
    },
    {
      "epoch": 1.221476510067114,
      "grad_norm": 1.8269708156585693,
      "learning_rate": 7.78523489932886e-05,
      "loss": 3.51,
      "step": 9100
    },
    {
      "epoch": 1.2281879194630871,
      "grad_norm": 1.6918151378631592,
      "learning_rate": 7.718120805369128e-05,
      "loss": 3.5295,
      "step": 9150
    },
    {
      "epoch": 1.2348993288590604,
      "grad_norm": 1.5626118183135986,
      "learning_rate": 7.651006711409397e-05,
      "loss": 3.4862,
      "step": 9200
    },
    {
      "epoch": 1.2416107382550337,
      "grad_norm": 1.6347813606262207,
      "learning_rate": 7.583892617449665e-05,
      "loss": 3.4976,
      "step": 9250
    },
    {
      "epoch": 1.2483221476510067,
      "grad_norm": 1.6891601085662842,
      "learning_rate": 7.516778523489934e-05,
      "loss": 3.5309,
      "step": 9300
    },
    {
      "epoch": 1.25503355704698,
      "grad_norm": 1.7273950576782227,
      "learning_rate": 7.449664429530202e-05,
      "loss": 3.5324,
      "step": 9350
    },
    {
      "epoch": 1.261744966442953,
      "grad_norm": 1.8590378761291504,
      "learning_rate": 7.382550335570471e-05,
      "loss": 3.5406,
      "step": 9400
    },
    {
      "epoch": 1.2684563758389262,
      "grad_norm": 1.7113752365112305,
      "learning_rate": 7.315436241610739e-05,
      "loss": 3.5114,
      "step": 9450
    },
    {
      "epoch": 1.2751677852348993,
      "grad_norm": 1.7116502523422241,
      "learning_rate": 7.248322147651007e-05,
      "loss": 3.5262,
      "step": 9500
    },
    {
      "epoch": 1.2818791946308725,
      "grad_norm": 1.7560157775878906,
      "learning_rate": 7.181208053691275e-05,
      "loss": 3.5005,
      "step": 9550
    },
    {
      "epoch": 1.2885906040268456,
      "grad_norm": 1.5261934995651245,
      "learning_rate": 7.114093959731544e-05,
      "loss": 3.5034,
      "step": 9600
    },
    {
      "epoch": 1.2953020134228188,
      "grad_norm": 1.5996999740600586,
      "learning_rate": 7.046979865771812e-05,
      "loss": 3.5119,
      "step": 9650
    },
    {
      "epoch": 1.302013422818792,
      "grad_norm": 1.6691954135894775,
      "learning_rate": 6.979865771812081e-05,
      "loss": 3.4661,
      "step": 9700
    },
    {
      "epoch": 1.308724832214765,
      "grad_norm": 1.6707172393798828,
      "learning_rate": 6.912751677852349e-05,
      "loss": 3.5448,
      "step": 9750
    },
    {
      "epoch": 1.3154362416107381,
      "grad_norm": 1.7001508474349976,
      "learning_rate": 6.845637583892618e-05,
      "loss": 3.4571,
      "step": 9800
    },
    {
      "epoch": 1.3221476510067114,
      "grad_norm": 1.6750668287277222,
      "learning_rate": 6.778523489932886e-05,
      "loss": 3.5337,
      "step": 9850
    },
    {
      "epoch": 1.3288590604026846,
      "grad_norm": 1.5876479148864746,
      "learning_rate": 6.711409395973155e-05,
      "loss": 3.4975,
      "step": 9900
    },
    {
      "epoch": 1.3355704697986577,
      "grad_norm": 1.6236827373504639,
      "learning_rate": 6.644295302013423e-05,
      "loss": 3.4861,
      "step": 9950
    },
    {
      "epoch": 1.342281879194631,
      "grad_norm": 1.5184366703033447,
      "learning_rate": 6.577181208053692e-05,
      "loss": 3.4764,
      "step": 10000
    },
    {
      "epoch": 1.348993288590604,
      "grad_norm": 1.837461233139038,
      "learning_rate": 6.51006711409396e-05,
      "loss": 3.4885,
      "step": 10050
    },
    {
      "epoch": 1.3557046979865772,
      "grad_norm": 1.6792209148406982,
      "learning_rate": 6.442953020134228e-05,
      "loss": 3.5083,
      "step": 10100
    },
    {
      "epoch": 1.3624161073825503,
      "grad_norm": 1.7687416076660156,
      "learning_rate": 6.375838926174497e-05,
      "loss": 3.4971,
      "step": 10150
    },
    {
      "epoch": 1.3691275167785235,
      "grad_norm": 1.547205924987793,
      "learning_rate": 6.308724832214765e-05,
      "loss": 3.4881,
      "step": 10200
    },
    {
      "epoch": 1.3758389261744965,
      "grad_norm": 1.5196295976638794,
      "learning_rate": 6.241610738255034e-05,
      "loss": 3.5766,
      "step": 10250
    },
    {
      "epoch": 1.3825503355704698,
      "grad_norm": 1.7716776132583618,
      "learning_rate": 6.174496644295302e-05,
      "loss": 3.4959,
      "step": 10300
    },
    {
      "epoch": 1.389261744966443,
      "grad_norm": 1.638798475265503,
      "learning_rate": 6.107382550335571e-05,
      "loss": 3.5503,
      "step": 10350
    },
    {
      "epoch": 1.395973154362416,
      "grad_norm": 1.7274184226989746,
      "learning_rate": 6.04026845637584e-05,
      "loss": 3.4822,
      "step": 10400
    },
    {
      "epoch": 1.4026845637583891,
      "grad_norm": 1.6604434251785278,
      "learning_rate": 5.973154362416108e-05,
      "loss": 3.4673,
      "step": 10450
    },
    {
      "epoch": 1.4093959731543624,
      "grad_norm": 1.6848317384719849,
      "learning_rate": 5.906040268456377e-05,
      "loss": 3.5256,
      "step": 10500
    },
    {
      "epoch": 1.4093959731543624,
      "eval_loss": 3.6587510108947754,
      "eval_runtime": 157.5264,
      "eval_samples_per_second": 46.938,
      "eval_steps_per_second": 5.872,
      "step": 10500
    },
    {
      "epoch": 1.4161073825503356,
      "grad_norm": 1.7273012399673462,
      "learning_rate": 5.838926174496645e-05,
      "loss": 3.4907,
      "step": 10550
    },
    {
      "epoch": 1.4228187919463087,
      "grad_norm": 1.760393738746643,
      "learning_rate": 5.771812080536914e-05,
      "loss": 3.4849,
      "step": 10600
    },
    {
      "epoch": 1.429530201342282,
      "grad_norm": 1.6159547567367554,
      "learning_rate": 5.704697986577181e-05,
      "loss": 3.4924,
      "step": 10650
    },
    {
      "epoch": 1.436241610738255,
      "grad_norm": 1.6727755069732666,
      "learning_rate": 5.6375838926174495e-05,
      "loss": 3.4559,
      "step": 10700
    },
    {
      "epoch": 1.4429530201342282,
      "grad_norm": 1.4687386751174927,
      "learning_rate": 5.570469798657718e-05,
      "loss": 3.4186,
      "step": 10750
    },
    {
      "epoch": 1.4496644295302015,
      "grad_norm": 1.5882593393325806,
      "learning_rate": 5.5033557046979866e-05,
      "loss": 3.4653,
      "step": 10800
    },
    {
      "epoch": 1.4563758389261745,
      "grad_norm": 1.6050992012023926,
      "learning_rate": 5.436241610738255e-05,
      "loss": 3.536,
      "step": 10850
    },
    {
      "epoch": 1.4630872483221475,
      "grad_norm": 1.7316681146621704,
      "learning_rate": 5.3691275167785237e-05,
      "loss": 3.5544,
      "step": 10900
    },
    {
      "epoch": 1.4697986577181208,
      "grad_norm": 1.4816292524337769,
      "learning_rate": 5.302013422818792e-05,
      "loss": 3.534,
      "step": 10950
    },
    {
      "epoch": 1.476510067114094,
      "grad_norm": 1.6182646751403809,
      "learning_rate": 5.234899328859061e-05,
      "loss": 3.4632,
      "step": 11000
    },
    {
      "epoch": 1.483221476510067,
      "grad_norm": 1.70271897315979,
      "learning_rate": 5.167785234899329e-05,
      "loss": 3.5418,
      "step": 11050
    },
    {
      "epoch": 1.4899328859060403,
      "grad_norm": 1.614667296409607,
      "learning_rate": 5.100671140939598e-05,
      "loss": 3.5098,
      "step": 11100
    },
    {
      "epoch": 1.4966442953020134,
      "grad_norm": 1.7727981805801392,
      "learning_rate": 5.033557046979866e-05,
      "loss": 3.5048,
      "step": 11150
    },
    {
      "epoch": 1.5033557046979866,
      "grad_norm": 1.868232250213623,
      "learning_rate": 4.966442953020135e-05,
      "loss": 3.5431,
      "step": 11200
    },
    {
      "epoch": 1.5100671140939599,
      "grad_norm": 1.4901870489120483,
      "learning_rate": 4.8993288590604034e-05,
      "loss": 3.4838,
      "step": 11250
    },
    {
      "epoch": 1.516778523489933,
      "grad_norm": 1.7952704429626465,
      "learning_rate": 4.832214765100672e-05,
      "loss": 3.4768,
      "step": 11300
    },
    {
      "epoch": 1.523489932885906,
      "grad_norm": 1.5511270761489868,
      "learning_rate": 4.76510067114094e-05,
      "loss": 3.4884,
      "step": 11350
    },
    {
      "epoch": 1.5302013422818792,
      "grad_norm": 1.6255027055740356,
      "learning_rate": 4.697986577181208e-05,
      "loss": 3.4885,
      "step": 11400
    },
    {
      "epoch": 1.5369127516778525,
      "grad_norm": 1.5847183465957642,
      "learning_rate": 4.630872483221477e-05,
      "loss": 3.521,
      "step": 11450
    },
    {
      "epoch": 1.5436241610738255,
      "grad_norm": 1.603010654449463,
      "learning_rate": 4.5637583892617453e-05,
      "loss": 3.5452,
      "step": 11500
    },
    {
      "epoch": 1.5503355704697985,
      "grad_norm": 1.5895144939422607,
      "learning_rate": 4.496644295302014e-05,
      "loss": 3.5373,
      "step": 11550
    },
    {
      "epoch": 1.5570469798657718,
      "grad_norm": 1.6598740816116333,
      "learning_rate": 4.4295302013422824e-05,
      "loss": 3.455,
      "step": 11600
    },
    {
      "epoch": 1.563758389261745,
      "grad_norm": 1.571210265159607,
      "learning_rate": 4.36241610738255e-05,
      "loss": 3.4703,
      "step": 11650
    },
    {
      "epoch": 1.570469798657718,
      "grad_norm": 1.5403891801834106,
      "learning_rate": 4.295302013422819e-05,
      "loss": 3.5011,
      "step": 11700
    },
    {
      "epoch": 1.5771812080536913,
      "grad_norm": 1.5460262298583984,
      "learning_rate": 4.228187919463087e-05,
      "loss": 3.5142,
      "step": 11750
    },
    {
      "epoch": 1.5838926174496644,
      "grad_norm": 1.6007267236709595,
      "learning_rate": 4.161073825503356e-05,
      "loss": 3.5247,
      "step": 11800
    },
    {
      "epoch": 1.5906040268456376,
      "grad_norm": 1.6835743188858032,
      "learning_rate": 4.0939597315436244e-05,
      "loss": 3.4959,
      "step": 11850
    },
    {
      "epoch": 1.5973154362416109,
      "grad_norm": 1.5206178426742554,
      "learning_rate": 4.026845637583892e-05,
      "loss": 3.5,
      "step": 11900
    },
    {
      "epoch": 1.604026845637584,
      "grad_norm": 1.8968535661697388,
      "learning_rate": 3.959731543624161e-05,
      "loss": 3.4545,
      "step": 11950
    },
    {
      "epoch": 1.610738255033557,
      "grad_norm": 1.822359561920166,
      "learning_rate": 3.89261744966443e-05,
      "loss": 3.4201,
      "step": 12000
    },
    {
      "epoch": 1.6174496644295302,
      "grad_norm": 1.536906123161316,
      "learning_rate": 3.8255033557046985e-05,
      "loss": 3.5263,
      "step": 12050
    },
    {
      "epoch": 1.6241610738255035,
      "grad_norm": 1.4844920635223389,
      "learning_rate": 3.758389261744967e-05,
      "loss": 3.4591,
      "step": 12100
    },
    {
      "epoch": 1.6308724832214765,
      "grad_norm": 1.6623713970184326,
      "learning_rate": 3.6912751677852356e-05,
      "loss": 3.4744,
      "step": 12150
    },
    {
      "epoch": 1.6375838926174495,
      "grad_norm": 1.517615556716919,
      "learning_rate": 3.6241610738255034e-05,
      "loss": 3.5203,
      "step": 12200
    },
    {
      "epoch": 1.6442953020134228,
      "grad_norm": 1.618048071861267,
      "learning_rate": 3.557046979865772e-05,
      "loss": 3.455,
      "step": 12250
    },
    {
      "epoch": 1.651006711409396,
      "grad_norm": 1.6573059558868408,
      "learning_rate": 3.4899328859060405e-05,
      "loss": 3.487,
      "step": 12300
    },
    {
      "epoch": 1.6577181208053693,
      "grad_norm": 1.5335503816604614,
      "learning_rate": 3.422818791946309e-05,
      "loss": 3.5249,
      "step": 12350
    },
    {
      "epoch": 1.6644295302013423,
      "grad_norm": 1.6136585474014282,
      "learning_rate": 3.3557046979865775e-05,
      "loss": 3.5175,
      "step": 12400
    },
    {
      "epoch": 1.6711409395973154,
      "grad_norm": 1.6459460258483887,
      "learning_rate": 3.288590604026846e-05,
      "loss": 3.5053,
      "step": 12450
    },
    {
      "epoch": 1.6778523489932886,
      "grad_norm": 1.6190588474273682,
      "learning_rate": 3.221476510067114e-05,
      "loss": 3.5567,
      "step": 12500
    },
    {
      "epoch": 1.6845637583892619,
      "grad_norm": 1.5673518180847168,
      "learning_rate": 3.1543624161073825e-05,
      "loss": 3.4663,
      "step": 12550
    },
    {
      "epoch": 1.691275167785235,
      "grad_norm": 1.5292575359344482,
      "learning_rate": 3.087248322147651e-05,
      "loss": 3.467,
      "step": 12600
    },
    {
      "epoch": 1.697986577181208,
      "grad_norm": 1.7313216924667358,
      "learning_rate": 3.02013422818792e-05,
      "loss": 3.472,
      "step": 12650
    },
    {
      "epoch": 1.7046979865771812,
      "grad_norm": 1.6088407039642334,
      "learning_rate": 2.9530201342281884e-05,
      "loss": 3.4999,
      "step": 12700
    },
    {
      "epoch": 1.7114093959731544,
      "grad_norm": 1.7493579387664795,
      "learning_rate": 2.885906040268457e-05,
      "loss": 3.4999,
      "step": 12750
    },
    {
      "epoch": 1.7181208053691275,
      "grad_norm": 1.5320335626602173,
      "learning_rate": 2.8187919463087248e-05,
      "loss": 3.4424,
      "step": 12800
    },
    {
      "epoch": 1.7248322147651005,
      "grad_norm": 1.5287050008773804,
      "learning_rate": 2.7516778523489933e-05,
      "loss": 3.4694,
      "step": 12850
    },
    {
      "epoch": 1.7315436241610738,
      "grad_norm": 1.756906509399414,
      "learning_rate": 2.6845637583892618e-05,
      "loss": 3.518,
      "step": 12900
    },
    {
      "epoch": 1.738255033557047,
      "grad_norm": 1.6873823404312134,
      "learning_rate": 2.6174496644295304e-05,
      "loss": 3.4734,
      "step": 12950
    },
    {
      "epoch": 1.7449664429530203,
      "grad_norm": 1.7049816846847534,
      "learning_rate": 2.550335570469799e-05,
      "loss": 3.5242,
      "step": 13000
    },
    {
      "epoch": 1.7516778523489933,
      "grad_norm": 1.7059725522994995,
      "learning_rate": 2.4832214765100674e-05,
      "loss": 3.4686,
      "step": 13050
    },
    {
      "epoch": 1.7583892617449663,
      "grad_norm": 1.638928771018982,
      "learning_rate": 2.416107382550336e-05,
      "loss": 3.4523,
      "step": 13100
    },
    {
      "epoch": 1.7651006711409396,
      "grad_norm": 1.64641535282135,
      "learning_rate": 2.348993288590604e-05,
      "loss": 3.4646,
      "step": 13150
    },
    {
      "epoch": 1.7718120805369129,
      "grad_norm": 1.6039409637451172,
      "learning_rate": 2.2818791946308727e-05,
      "loss": 3.4324,
      "step": 13200
    },
    {
      "epoch": 1.778523489932886,
      "grad_norm": 1.6225621700286865,
      "learning_rate": 2.2147651006711412e-05,
      "loss": 3.4788,
      "step": 13250
    },
    {
      "epoch": 1.785234899328859,
      "grad_norm": 1.5514132976531982,
      "learning_rate": 2.1476510067114094e-05,
      "loss": 3.5118,
      "step": 13300
    },
    {
      "epoch": 1.7919463087248322,
      "grad_norm": 1.580020785331726,
      "learning_rate": 2.080536912751678e-05,
      "loss": 3.5322,
      "step": 13350
    },
    {
      "epoch": 1.7986577181208054,
      "grad_norm": 1.5681813955307007,
      "learning_rate": 2.013422818791946e-05,
      "loss": 3.4891,
      "step": 13400
    },
    {
      "epoch": 1.8053691275167785,
      "grad_norm": 1.5459489822387695,
      "learning_rate": 1.946308724832215e-05,
      "loss": 3.474,
      "step": 13450
    },
    {
      "epoch": 1.8120805369127517,
      "grad_norm": 1.419686198234558,
      "learning_rate": 1.8791946308724835e-05,
      "loss": 3.469,
      "step": 13500
    },
    {
      "epoch": 1.8187919463087248,
      "grad_norm": 1.543858528137207,
      "learning_rate": 1.8120805369127517e-05,
      "loss": 3.4872,
      "step": 13550
    },
    {
      "epoch": 1.825503355704698,
      "grad_norm": 1.745296597480774,
      "learning_rate": 1.7449664429530202e-05,
      "loss": 3.4573,
      "step": 13600
    },
    {
      "epoch": 1.8322147651006713,
      "grad_norm": 1.472774863243103,
      "learning_rate": 1.6778523489932888e-05,
      "loss": 3.4718,
      "step": 13650
    },
    {
      "epoch": 1.8389261744966443,
      "grad_norm": 1.5474226474761963,
      "learning_rate": 1.610738255033557e-05,
      "loss": 3.4944,
      "step": 13700
    },
    {
      "epoch": 1.8456375838926173,
      "grad_norm": 1.6456965208053589,
      "learning_rate": 1.5436241610738255e-05,
      "loss": 3.5285,
      "step": 13750
    },
    {
      "epoch": 1.8523489932885906,
      "grad_norm": 1.6498017311096191,
      "learning_rate": 1.4765100671140942e-05,
      "loss": 3.4569,
      "step": 13800
    },
    {
      "epoch": 1.8590604026845639,
      "grad_norm": 1.5467504262924194,
      "learning_rate": 1.4093959731543624e-05,
      "loss": 3.4639,
      "step": 13850
    },
    {
      "epoch": 1.8657718120805369,
      "grad_norm": 1.4848510026931763,
      "learning_rate": 1.3422818791946309e-05,
      "loss": 3.4389,
      "step": 13900
    },
    {
      "epoch": 1.87248322147651,
      "grad_norm": 1.5809725522994995,
      "learning_rate": 1.2751677852348994e-05,
      "loss": 3.46,
      "step": 13950
    },
    {
      "epoch": 1.8791946308724832,
      "grad_norm": 1.6101207733154297,
      "learning_rate": 1.208053691275168e-05,
      "loss": 3.4841,
      "step": 14000
    },
    {
      "epoch": 1.8791946308724832,
      "eval_loss": 3.6194987297058105,
      "eval_runtime": 157.2237,
      "eval_samples_per_second": 47.029,
      "eval_steps_per_second": 5.883,
      "step": 14000
    }
  ],
  "logging_steps": 50,
  "max_steps": 14900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 3500,
  "total_flos": 7.63014409837609e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
