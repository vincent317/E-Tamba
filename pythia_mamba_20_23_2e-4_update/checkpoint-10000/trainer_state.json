{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9227071716977504,
  "eval_steps": 5000,
  "global_step": 10000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01,
      "grad_norm": 1.0761053562164307,
      "learning_rate": 0.00019903864641415114,
      "loss": 12.2703,
      "step": 50
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.639334499835968,
      "learning_rate": 0.00019807729282830225,
      "loss": 5.7722,
      "step": 100
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.5162156224250793,
      "learning_rate": 0.00019711593924245338,
      "loss": 5.1244,
      "step": 150
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.3849436044692993,
      "learning_rate": 0.00019615458565660452,
      "loss": 4.9347,
      "step": 200
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.37167486548423767,
      "learning_rate": 0.00019519323207075565,
      "loss": 4.8156,
      "step": 250
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.3494596779346466,
      "learning_rate": 0.00019423187848490676,
      "loss": 4.7258,
      "step": 300
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.3432973623275757,
      "learning_rate": 0.0001932705248990579,
      "loss": 4.6545,
      "step": 350
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.7180005311965942,
      "learning_rate": 0.000192309171313209,
      "loss": 4.6242,
      "step": 400
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.38967305421829224,
      "learning_rate": 0.00019134781772736013,
      "loss": 4.5851,
      "step": 450
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.3197270929813385,
      "learning_rate": 0.00019038646414151127,
      "loss": 4.5365,
      "step": 500
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.37121719121932983,
      "learning_rate": 0.0001894251105556624,
      "loss": 4.5363,
      "step": 550
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.42906275391578674,
      "learning_rate": 0.0001884637569698135,
      "loss": 4.5096,
      "step": 600
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.3132118880748749,
      "learning_rate": 0.00018750240338396464,
      "loss": 4.4701,
      "step": 650
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.3507443368434906,
      "learning_rate": 0.00018654104979811575,
      "loss": 4.4432,
      "step": 700
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.3885868489742279,
      "learning_rate": 0.00018557969621226688,
      "loss": 4.4322,
      "step": 750
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.3975401818752289,
      "learning_rate": 0.000184618342626418,
      "loss": 4.4063,
      "step": 800
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.41002345085144043,
      "learning_rate": 0.00018365698904056915,
      "loss": 4.3862,
      "step": 850
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.36799708008766174,
      "learning_rate": 0.00018269563545472025,
      "loss": 4.3743,
      "step": 900
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.3999990224838257,
      "learning_rate": 0.0001817342818688714,
      "loss": 4.3433,
      "step": 950
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.3548630475997925,
      "learning_rate": 0.0001807729282830225,
      "loss": 4.339,
      "step": 1000
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.4022258222103119,
      "learning_rate": 0.00017981157469717363,
      "loss": 4.3158,
      "step": 1050
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.3360045552253723,
      "learning_rate": 0.00017885022111132474,
      "loss": 4.2895,
      "step": 1100
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.4591478705406189,
      "learning_rate": 0.0001778888675254759,
      "loss": 4.298,
      "step": 1150
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.36591678857803345,
      "learning_rate": 0.000176927513939627,
      "loss": 4.3006,
      "step": 1200
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.3495497405529022,
      "learning_rate": 0.00017596616035377814,
      "loss": 4.2907,
      "step": 1250
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.34594935178756714,
      "learning_rate": 0.00017500480676792924,
      "loss": 4.2518,
      "step": 1300
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.32440927624702454,
      "learning_rate": 0.00017404345318208038,
      "loss": 4.2586,
      "step": 1350
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.3621368408203125,
      "learning_rate": 0.00017308209959623148,
      "loss": 4.2811,
      "step": 1400
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.3388976752758026,
      "learning_rate": 0.00017212074601038262,
      "loss": 4.246,
      "step": 1450
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.3534735441207886,
      "learning_rate": 0.00017115939242453375,
      "loss": 4.2513,
      "step": 1500
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.33655014634132385,
      "learning_rate": 0.00017019803883868489,
      "loss": 4.224,
      "step": 1550
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.32447418570518494,
      "learning_rate": 0.000169236685252836,
      "loss": 4.2365,
      "step": 1600
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.3853776156902313,
      "learning_rate": 0.00016827533166698713,
      "loss": 4.2069,
      "step": 1650
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.3714156150817871,
      "learning_rate": 0.00016731397808113823,
      "loss": 4.2341,
      "step": 1700
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.34729012846946716,
      "learning_rate": 0.00016635262449528937,
      "loss": 4.2142,
      "step": 1750
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.35995766520500183,
      "learning_rate": 0.0001653912709094405,
      "loss": 4.196,
      "step": 1800
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.33667874336242676,
      "learning_rate": 0.00016442991732359163,
      "loss": 4.1983,
      "step": 1850
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.34494590759277344,
      "learning_rate": 0.00016346856373774274,
      "loss": 4.2049,
      "step": 1900
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.6549867987632751,
      "learning_rate": 0.00016250721015189387,
      "loss": 4.1974,
      "step": 1950
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.35763823986053467,
      "learning_rate": 0.000161545856566045,
      "loss": 4.1875,
      "step": 2000
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.3719845712184906,
      "learning_rate": 0.00016058450298019611,
      "loss": 4.2016,
      "step": 2050
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.390328586101532,
      "learning_rate": 0.00015962314939434725,
      "loss": 4.1776,
      "step": 2100
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.34528350830078125,
      "learning_rate": 0.00015866179580849838,
      "loss": 4.1695,
      "step": 2150
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.3470586836338043,
      "learning_rate": 0.00015770044222264952,
      "loss": 4.1674,
      "step": 2200
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.41419652104377747,
      "learning_rate": 0.00015673908863680062,
      "loss": 4.1664,
      "step": 2250
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.4008471369743347,
      "learning_rate": 0.00015577773505095176,
      "loss": 4.1514,
      "step": 2300
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.4852149784564972,
      "learning_rate": 0.00015481638146510286,
      "loss": 4.1238,
      "step": 2350
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.3604600429534912,
      "learning_rate": 0.000153855027879254,
      "loss": 4.1536,
      "step": 2400
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.34047624468803406,
      "learning_rate": 0.00015289367429340513,
      "loss": 4.1451,
      "step": 2450
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.3446938097476959,
      "learning_rate": 0.00015193232070755626,
      "loss": 4.1268,
      "step": 2500
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.35720083117485046,
      "learning_rate": 0.00015097096712170737,
      "loss": 4.1203,
      "step": 2550
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.3874739408493042,
      "learning_rate": 0.0001500096135358585,
      "loss": 4.155,
      "step": 2600
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.3485708236694336,
      "learning_rate": 0.0001490482599500096,
      "loss": 4.1353,
      "step": 2650
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.4150541126728058,
      "learning_rate": 0.00014808690636416074,
      "loss": 4.1346,
      "step": 2700
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.3554892838001251,
      "learning_rate": 0.00014712555277831185,
      "loss": 4.1237,
      "step": 2750
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.35080087184906006,
      "learning_rate": 0.000146164199192463,
      "loss": 4.1351,
      "step": 2800
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.354196697473526,
      "learning_rate": 0.00014520284560661412,
      "loss": 4.1245,
      "step": 2850
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.4444771111011505,
      "learning_rate": 0.00014424149202076525,
      "loss": 4.1121,
      "step": 2900
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.3527624309062958,
      "learning_rate": 0.00014328013843491636,
      "loss": 4.1272,
      "step": 2950
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.3451567590236664,
      "learning_rate": 0.0001423187848490675,
      "loss": 4.103,
      "step": 3000
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.37507736682891846,
      "learning_rate": 0.0001413574312632186,
      "loss": 4.1329,
      "step": 3050
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.34419888257980347,
      "learning_rate": 0.00014039607767736976,
      "loss": 4.0886,
      "step": 3100
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.3465399742126465,
      "learning_rate": 0.00013943472409152087,
      "loss": 4.1282,
      "step": 3150
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.36114686727523804,
      "learning_rate": 0.000138473370505672,
      "loss": 4.0793,
      "step": 3200
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.3817354440689087,
      "learning_rate": 0.0001375120169198231,
      "loss": 4.1171,
      "step": 3250
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.38165584206581116,
      "learning_rate": 0.00013655066333397424,
      "loss": 4.1029,
      "step": 3300
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.4837908446788788,
      "learning_rate": 0.00013558930974812535,
      "loss": 4.1007,
      "step": 3350
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.3582858145236969,
      "learning_rate": 0.00013462795616227648,
      "loss": 4.0926,
      "step": 3400
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.36884820461273193,
      "learning_rate": 0.00013366660257642762,
      "loss": 4.0888,
      "step": 3450
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.35536107420921326,
      "learning_rate": 0.00013270524899057875,
      "loss": 4.0958,
      "step": 3500
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.37632691860198975,
      "learning_rate": 0.00013174389540472986,
      "loss": 4.0673,
      "step": 3550
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.37494945526123047,
      "learning_rate": 0.000130782541818881,
      "loss": 4.073,
      "step": 3600
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.3468009829521179,
      "learning_rate": 0.0001298211882330321,
      "loss": 4.087,
      "step": 3650
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.36169368028640747,
      "learning_rate": 0.00012885983464718323,
      "loss": 4.0633,
      "step": 3700
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.3386775553226471,
      "learning_rate": 0.00012789848106133436,
      "loss": 4.0854,
      "step": 3750
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.4464569389820099,
      "learning_rate": 0.0001269371274754855,
      "loss": 4.0669,
      "step": 3800
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.3487553000450134,
      "learning_rate": 0.0001259757738896366,
      "loss": 4.0607,
      "step": 3850
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.3636760711669922,
      "learning_rate": 0.00012501442030378774,
      "loss": 4.073,
      "step": 3900
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.34341973066329956,
      "learning_rate": 0.00012405306671793887,
      "loss": 4.0717,
      "step": 3950
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.3438337743282318,
      "learning_rate": 0.00012309171313208998,
      "loss": 4.0677,
      "step": 4000
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.3654229938983917,
      "learning_rate": 0.0001221303595462411,
      "loss": 4.0728,
      "step": 4050
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.3589645326137543,
      "learning_rate": 0.00012116900596039225,
      "loss": 4.0541,
      "step": 4100
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.36159446835517883,
      "learning_rate": 0.00012020765237454337,
      "loss": 4.0444,
      "step": 4150
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.4326479434967041,
      "learning_rate": 0.00011924629878869449,
      "loss": 4.078,
      "step": 4200
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.37138986587524414,
      "learning_rate": 0.0001182849452028456,
      "loss": 4.0649,
      "step": 4250
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.37474194169044495,
      "learning_rate": 0.00011732359161699673,
      "loss": 4.0451,
      "step": 4300
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.3771936297416687,
      "learning_rate": 0.00011636223803114785,
      "loss": 4.0472,
      "step": 4350
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.3596436381340027,
      "learning_rate": 0.000115400884445299,
      "loss": 4.0368,
      "step": 4400
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.373518168926239,
      "learning_rate": 0.00011443953085945011,
      "loss": 4.0404,
      "step": 4450
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.34516286849975586,
      "learning_rate": 0.00011347817727360123,
      "loss": 4.0461,
      "step": 4500
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.3621443212032318,
      "learning_rate": 0.00011251682368775235,
      "loss": 4.0492,
      "step": 4550
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.34863021969795227,
      "learning_rate": 0.00011155547010190348,
      "loss": 4.0567,
      "step": 4600
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.36528900265693665,
      "learning_rate": 0.00011059411651605461,
      "loss": 4.0421,
      "step": 4650
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.4197595417499542,
      "learning_rate": 0.00010963276293020574,
      "loss": 4.0189,
      "step": 4700
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.354596346616745,
      "learning_rate": 0.00010867140934435686,
      "loss": 4.0258,
      "step": 4750
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.3808305263519287,
      "learning_rate": 0.00010771005575850798,
      "loss": 4.0354,
      "step": 4800
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.3323865234851837,
      "learning_rate": 0.0001067487021726591,
      "loss": 4.0246,
      "step": 4850
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.356784462928772,
      "learning_rate": 0.00010578734858681024,
      "loss": 4.0141,
      "step": 4900
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.41858378052711487,
      "learning_rate": 0.00010482599500096136,
      "loss": 4.0181,
      "step": 4950
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.3484872579574585,
      "learning_rate": 0.00010386464141511248,
      "loss": 4.0386,
      "step": 5000
    },
    {
      "epoch": 0.96,
      "eval_loss": 4.043379783630371,
      "eval_runtime": 371.6621,
      "eval_samples_per_second": 86.638,
      "eval_steps_per_second": 1.356,
      "step": 5000
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.3853408396244049,
      "learning_rate": 0.00010290328782926361,
      "loss": 4.0073,
      "step": 5050
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.38172101974487305,
      "learning_rate": 0.00010194193424341474,
      "loss": 4.0231,
      "step": 5100
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.3658083379268646,
      "learning_rate": 0.00010098058065756587,
      "loss": 4.0164,
      "step": 5150
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.33731961250305176,
      "learning_rate": 0.00010001922707171699,
      "loss": 4.019,
      "step": 5200
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.3577200770378113,
      "learning_rate": 9.90578734858681e-05,
      "loss": 3.996,
      "step": 5250
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.35941529273986816,
      "learning_rate": 9.809651990001924e-05,
      "loss": 4.0231,
      "step": 5300
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.31852859258651733,
      "learning_rate": 9.713516631417036e-05,
      "loss": 3.9907,
      "step": 5350
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.38656851649284363,
      "learning_rate": 9.617381272832148e-05,
      "loss": 4.0146,
      "step": 5400
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.3495696187019348,
      "learning_rate": 9.521245914247261e-05,
      "loss": 3.9934,
      "step": 5450
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.3612021505832672,
      "learning_rate": 9.425110555662373e-05,
      "loss": 3.9935,
      "step": 5500
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.34885746240615845,
      "learning_rate": 9.328975197077485e-05,
      "loss": 4.0022,
      "step": 5550
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.33940330147743225,
      "learning_rate": 9.232839838492599e-05,
      "loss": 4.0057,
      "step": 5600
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.3594762086868286,
      "learning_rate": 9.136704479907711e-05,
      "loss": 3.9965,
      "step": 5650
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.38273727893829346,
      "learning_rate": 9.040569121322823e-05,
      "loss": 4.0032,
      "step": 5700
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.3388049602508545,
      "learning_rate": 8.944433762737936e-05,
      "loss": 4.0047,
      "step": 5750
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.3319057524204254,
      "learning_rate": 8.848298404153048e-05,
      "loss": 4.0093,
      "step": 5800
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.3413621783256531,
      "learning_rate": 8.75216304556816e-05,
      "loss": 3.979,
      "step": 5850
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.35982581973075867,
      "learning_rate": 8.656027686983272e-05,
      "loss": 3.9999,
      "step": 5900
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.3706766963005066,
      "learning_rate": 8.559892328398386e-05,
      "loss": 3.9842,
      "step": 5950
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.34569716453552246,
      "learning_rate": 8.463756969813498e-05,
      "loss": 3.9887,
      "step": 6000
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.35762742161750793,
      "learning_rate": 8.36762161122861e-05,
      "loss": 4.0121,
      "step": 6050
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.3622974157333374,
      "learning_rate": 8.271486252643723e-05,
      "loss": 4.0003,
      "step": 6100
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.38261669874191284,
      "learning_rate": 8.175350894058835e-05,
      "loss": 3.982,
      "step": 6150
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.39674830436706543,
      "learning_rate": 8.079215535473947e-05,
      "loss": 4.001,
      "step": 6200
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.4052533209323883,
      "learning_rate": 7.98308017688906e-05,
      "loss": 3.9742,
      "step": 6250
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.3810446262359619,
      "learning_rate": 7.886944818304172e-05,
      "loss": 3.9795,
      "step": 6300
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.41575849056243896,
      "learning_rate": 7.790809459719284e-05,
      "loss": 3.9893,
      "step": 6350
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.35546037554740906,
      "learning_rate": 7.694674101134398e-05,
      "loss": 3.9932,
      "step": 6400
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.364665150642395,
      "learning_rate": 7.59853874254951e-05,
      "loss": 3.9999,
      "step": 6450
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.3864067494869232,
      "learning_rate": 7.502403383964622e-05,
      "loss": 3.9872,
      "step": 6500
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.35797521471977234,
      "learning_rate": 7.406268025379734e-05,
      "loss": 3.968,
      "step": 6550
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.3459186255931854,
      "learning_rate": 7.310132666794847e-05,
      "loss": 3.978,
      "step": 6600
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.35620057582855225,
      "learning_rate": 7.21399730820996e-05,
      "loss": 3.9845,
      "step": 6650
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.48376962542533875,
      "learning_rate": 7.117861949625071e-05,
      "loss": 3.9808,
      "step": 6700
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.3696092367172241,
      "learning_rate": 7.021726591040185e-05,
      "loss": 3.9864,
      "step": 6750
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.3824962079524994,
      "learning_rate": 6.925591232455297e-05,
      "loss": 3.9733,
      "step": 6800
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.36529314517974854,
      "learning_rate": 6.82945587387041e-05,
      "loss": 3.9827,
      "step": 6850
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.3509184420108795,
      "learning_rate": 6.733320515285522e-05,
      "loss": 3.9675,
      "step": 6900
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.3759908974170685,
      "learning_rate": 6.637185156700636e-05,
      "loss": 3.9793,
      "step": 6950
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.33829033374786377,
      "learning_rate": 6.541049798115748e-05,
      "loss": 3.9751,
      "step": 7000
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.36404427886009216,
      "learning_rate": 6.444914439530861e-05,
      "loss": 3.9746,
      "step": 7050
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.3366362452507019,
      "learning_rate": 6.348779080945973e-05,
      "loss": 3.952,
      "step": 7100
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.35519784688949585,
      "learning_rate": 6.252643722361085e-05,
      "loss": 3.9701,
      "step": 7150
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.34269261360168457,
      "learning_rate": 6.156508363776197e-05,
      "loss": 3.9634,
      "step": 7200
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.34248918294906616,
      "learning_rate": 6.06037300519131e-05,
      "loss": 3.9622,
      "step": 7250
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.35942745208740234,
      "learning_rate": 5.964237646606422e-05,
      "loss": 3.9662,
      "step": 7300
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.33748313784599304,
      "learning_rate": 5.8681022880215344e-05,
      "loss": 3.9713,
      "step": 7350
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.340019166469574,
      "learning_rate": 5.771966929436647e-05,
      "loss": 3.9695,
      "step": 7400
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.3640153706073761,
      "learning_rate": 5.67583157085176e-05,
      "loss": 3.9555,
      "step": 7450
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.3592172861099243,
      "learning_rate": 5.579696212266872e-05,
      "loss": 3.9698,
      "step": 7500
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.33017972111701965,
      "learning_rate": 5.483560853681985e-05,
      "loss": 3.9647,
      "step": 7550
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.3609580993652344,
      "learning_rate": 5.387425495097097e-05,
      "loss": 3.9633,
      "step": 7600
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.3670366406440735,
      "learning_rate": 5.291290136512209e-05,
      "loss": 3.9683,
      "step": 7650
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.3592860996723175,
      "learning_rate": 5.1951547779273226e-05,
      "loss": 3.9555,
      "step": 7700
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.3723936975002289,
      "learning_rate": 5.0990194193424346e-05,
      "loss": 3.9588,
      "step": 7750
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.3987171947956085,
      "learning_rate": 5.0028840607575466e-05,
      "loss": 3.9777,
      "step": 7800
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.356121689081192,
      "learning_rate": 4.9067487021726593e-05,
      "loss": 3.9647,
      "step": 7850
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.35553959012031555,
      "learning_rate": 4.810613343587772e-05,
      "loss": 3.9575,
      "step": 7900
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.3675002455711365,
      "learning_rate": 4.714477985002884e-05,
      "loss": 3.9701,
      "step": 7950
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.36919495463371277,
      "learning_rate": 4.618342626417997e-05,
      "loss": 3.9485,
      "step": 8000
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.39221325516700745,
      "learning_rate": 4.522207267833109e-05,
      "loss": 3.9592,
      "step": 8050
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.34648358821868896,
      "learning_rate": 4.4260719092482215e-05,
      "loss": 3.9657,
      "step": 8100
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.35863804817199707,
      "learning_rate": 4.329936550663334e-05,
      "loss": 3.9631,
      "step": 8150
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.3821229636669159,
      "learning_rate": 4.233801192078446e-05,
      "loss": 3.9541,
      "step": 8200
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.3537881374359131,
      "learning_rate": 4.137665833493559e-05,
      "loss": 3.9405,
      "step": 8250
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.3661949932575226,
      "learning_rate": 4.0415304749086716e-05,
      "loss": 3.9697,
      "step": 8300
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.36545267701148987,
      "learning_rate": 3.945395116323784e-05,
      "loss": 3.9671,
      "step": 8350
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.34564846754074097,
      "learning_rate": 3.849259757738897e-05,
      "loss": 3.9748,
      "step": 8400
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.3512830138206482,
      "learning_rate": 3.753124399154009e-05,
      "loss": 3.957,
      "step": 8450
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.3503740429878235,
      "learning_rate": 3.656989040569122e-05,
      "loss": 3.9596,
      "step": 8500
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.3436318337917328,
      "learning_rate": 3.5608536819842344e-05,
      "loss": 3.9691,
      "step": 8550
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.3385608494281769,
      "learning_rate": 3.4647183233993464e-05,
      "loss": 3.955,
      "step": 8600
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.3635123074054718,
      "learning_rate": 3.368582964814459e-05,
      "loss": 3.9576,
      "step": 8650
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.3618251085281372,
      "learning_rate": 3.272447606229571e-05,
      "loss": 3.931,
      "step": 8700
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.36855778098106384,
      "learning_rate": 3.176312247644684e-05,
      "loss": 3.9583,
      "step": 8750
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.37196794152259827,
      "learning_rate": 3.0801768890597965e-05,
      "loss": 3.9757,
      "step": 8800
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.36010676622390747,
      "learning_rate": 2.9840415304749086e-05,
      "loss": 3.9658,
      "step": 8850
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.35312944650650024,
      "learning_rate": 2.8879061718900213e-05,
      "loss": 3.9431,
      "step": 8900
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.33537495136260986,
      "learning_rate": 2.791770813305134e-05,
      "loss": 3.9662,
      "step": 8950
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.39557960629463196,
      "learning_rate": 2.695635454720246e-05,
      "loss": 3.9537,
      "step": 9000
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.4150066673755646,
      "learning_rate": 2.5995000961353587e-05,
      "loss": 3.9391,
      "step": 9050
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.34217485785484314,
      "learning_rate": 2.503364737550471e-05,
      "loss": 3.9587,
      "step": 9100
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.37305381894111633,
      "learning_rate": 2.4072293789655837e-05,
      "loss": 3.9525,
      "step": 9150
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.36626482009887695,
      "learning_rate": 2.311094020380696e-05,
      "loss": 3.9663,
      "step": 9200
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.3694387376308441,
      "learning_rate": 2.2149586617958088e-05,
      "loss": 3.966,
      "step": 9250
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.34506407380104065,
      "learning_rate": 2.118823303210921e-05,
      "loss": 3.9622,
      "step": 9300
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.35332992672920227,
      "learning_rate": 2.0226879446260335e-05,
      "loss": 3.9331,
      "step": 9350
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.35995742678642273,
      "learning_rate": 1.926552586041146e-05,
      "loss": 3.9529,
      "step": 9400
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.3670289218425751,
      "learning_rate": 1.8304172274562582e-05,
      "loss": 3.958,
      "step": 9450
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.3691064715385437,
      "learning_rate": 1.734281868871371e-05,
      "loss": 3.9557,
      "step": 9500
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.3656086027622223,
      "learning_rate": 1.6381465102864836e-05,
      "loss": 3.9672,
      "step": 9550
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.3554143011569977,
      "learning_rate": 1.542011151701596e-05,
      "loss": 3.9624,
      "step": 9600
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.35245469212532043,
      "learning_rate": 1.4458757931167084e-05,
      "loss": 3.961,
      "step": 9650
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.35529419779777527,
      "learning_rate": 1.349740434531821e-05,
      "loss": 3.9514,
      "step": 9700
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.3910422623157501,
      "learning_rate": 1.2536050759469334e-05,
      "loss": 3.9515,
      "step": 9750
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.3479691445827484,
      "learning_rate": 1.1574697173620458e-05,
      "loss": 3.9656,
      "step": 9800
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.3490928113460541,
      "learning_rate": 1.0613343587771583e-05,
      "loss": 3.9548,
      "step": 9850
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.37779751420021057,
      "learning_rate": 9.651990001922708e-06,
      "loss": 3.9253,
      "step": 9900
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.3703550100326538,
      "learning_rate": 8.690636416073834e-06,
      "loss": 3.9639,
      "step": 9950
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.31956467032432556,
      "learning_rate": 7.729282830224957e-06,
      "loss": 3.9604,
      "step": 10000
    },
    {
      "epoch": 1.92,
      "eval_loss": 3.9772706031799316,
      "eval_runtime": 372.1044,
      "eval_samples_per_second": 86.535,
      "eval_steps_per_second": 1.354,
      "step": 10000
    }
  ],
  "logging_steps": 50,
  "max_steps": 10402,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 5000,
  "total_flos": 8.14149089993687e+16,
  "train_batch_size": 48,
  "trial_name": null,
  "trial_params": null
}
