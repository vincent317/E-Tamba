{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda:0'\n",
    "base_path = './'\n",
    "\n",
    "def load_json_gz(filename):\n",
    "    with gzip.open(filename, 'r') as f:\n",
    "        i = 0\n",
    "        ret = []\n",
    "        for json_line in f:\n",
    "            if i == 10000:\n",
    "                return ret\n",
    "            data = json.loads(json_line)\n",
    "            text = data['text']\n",
    "            if len(text) > 2000:\n",
    "                ret.append(text)\n",
    "                i += 1\n",
    "\n",
    "# Load 10000 strings from C4 dataset: https://huggingface.co/datasets/allenai/c4/tree/main/en\n",
    "strings = load_json_gz(base_path + 'c4-validation.00000-of-00008.json.gz')\n",
    "\n",
    "def copy_task(batch_size=64, batches=10, model=None, tokenizer=None, token_max_len=25, shuffle=False):\n",
    "    string_idx = 0\n",
    "    success_copies = 0\n",
    "    for _ in tqdm(range(batches)):\n",
    "        cur_batch = []\n",
    "        for count in range(batch_size):\n",
    "            cur_batch.append(strings[count + string_idx])\n",
    "        outputs = tokenizer(cur_batch, return_tensors=\"pt\", truncation=True, max_length=token_max_len).to(device)\n",
    "        input_ids = outputs['input_ids']\n",
    "        attn_masks = outputs['attention_mask']\n",
    "        if shuffle:\n",
    "            col_perm = torch.randperm(input_ids.size(1))\n",
    "            input_ids = input_ids[:, col_perm]\n",
    "        input_ids = torch.cat([input_ids, input_ids], dim=1)\n",
    "        input_ids = torch.cat([input_ids, input_ids[:, 0:1]], dim=1)\n",
    "        output_ids = model.generate(input_ids, max_new_tokens = token_max_len-1)\n",
    "        for count in range(batch_size):\n",
    "            gold_token_len = (input_ids.shape[1]-1) // 2\n",
    "            if torch.equal(input_ids[count][:gold_token_len], output_ids[count][gold_token_len*2:]):\n",
    "                success_copies += 1\n",
    "        string_idx += batch_size\n",
    "    return success_copies / (batch_size * batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "name_phone_pairs = []\n",
    "with open(base_path + 'phonebook.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line[-1] == ',':\n",
    "            line = line[:-1]\n",
    "        pair = ast.literal_eval(line)\n",
    "        name_phone_pairs.append((pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "device = 'cuda'\n",
    "softmax = nn.Softmax(dim=2)\n",
    "\n",
    "def phone_book_task(batch_size=64, batches=5, book_size=20, model=None, tokenizer=None):\n",
    "    book = ''\n",
    "    success_lookups = 0\n",
    "    for i in range(book_size):\n",
    "        name = name_phone_pairs[i][0]\n",
    "        phone = name_phone_pairs[i][1]\n",
    "        book = book + name + ': ' + phone + '.\\n'\n",
    "    book += 'Liam: 436-725-2906\\nOlivia: 192-311-5790\\n\\n'\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(batches)):\n",
    "            cur_batch = []\n",
    "            gt_numbers = []\n",
    "            max_num_tokens = 30\n",
    "            for _ in range(batch_size):\n",
    "                query_pair_idx = random.randint(2, book_size)\n",
    "                query = book + name_phone_pairs[query_pair_idx][0] + ':'\n",
    "\n",
    "                gt_numbers.append(name_phone_pairs[query_pair_idx][1])\n",
    "                cur_batch.append(query)\n",
    "\n",
    "            input_ids = tokenizer(cur_batch, return_tensors=\"pt\", padding=True).to(device)[\"input_ids\"]\n",
    "            for i in range(max_num_tokens-1):\n",
    "                bs, seq_len = input_ids.size()\n",
    "                mask = torch.ones(bs, seq_len).to('cuda')\n",
    "                logits = model(input_ids=input_ids, attention_mask=mask, labels=None)['logits'] # bs, seq_len, vocab_size\n",
    "                next_token = torch.unsqueeze(torch.argmax(softmax(logits), dim=-1)[:, -1], 1)\n",
    "                input_ids = torch.cat((input_ids, next_token), dim=-1) # bs, seq_len, 1\n",
    "            for count in range(batch_size):\n",
    "                true_number = gt_numbers[count]\n",
    "                output_answer = tokenizer.decode(input_ids[count])\n",
    "                if output_answer.count(true_number) > 1:\n",
    "                    success_lookups += 1\n",
    "    return success_lookups / (batch_size * batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hangruic/anaconda3/envs/ssm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hangruic/anaconda3/envs/ssm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn as nn\n",
    "\n",
    "from modeling_mamba_transformer import MambaTransformerForLM, MambaTransformerConfig\n",
    "# check_point_path = base_path + 'checkpoint-4900/model.safetensors'\n",
    "# check_point_path = '/home/hangruic/webllm-test/base_1.4b_1024len_12_12_1_2_epochs/checkpoint-7400/model.safetensors'\n",
    "check_point_path = '/home/hangruic/webllm-test/sft_2_epoch_100_length_2000_samples_1.4b_ck4900_new/checkpoint-260/model.safetensors'\n",
    "model = MambaTransformerForLM(MambaTransformerConfig(),\n",
    "                              pretrained_pythia_name='EleutherAI/pythia-1.4b',\n",
    "                              pretrained_mamba_name='state-spaces/mamba-1.4b-hf',\n",
    "                              first_transformer_layers=12,\n",
    "                              mamba_start_layer=36,\n",
    "                              mamba_end_layer=47,\n",
    "                              check_point_path=check_point_path).to(device)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-1.4b', padding_side='left')\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "model.eval()\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "batches = 5\n",
    "batch_size = 8\n",
    "shuffle = True\n",
    "token_max_len = 25\n",
    "\n",
    "softmax = nn.Softmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:14<00:00, 26.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "string_idx = 0\n",
    "success_copies = 0\n",
    "token_max_len = 150\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(batches)):\n",
    "        cur_batch = []\n",
    "        for count in range(batch_size):\n",
    "            cur_batch.append(strings[count + string_idx])\n",
    "        outputs = tokenizer(cur_batch, return_tensors=\"pt\", truncation=True, max_length=token_max_len).to(device)\n",
    "        input_ids = outputs['input_ids']\n",
    "        attn_masks = outputs['attention_mask']\n",
    "        if shuffle:\n",
    "            col_perm = torch.randperm(input_ids.size(1))\n",
    "            input_ids = input_ids[:, col_perm]\n",
    "        input_ids = torch.cat([input_ids, input_ids], dim=1)\n",
    "        input_ids = torch.cat([input_ids, input_ids[:, 0:1]], dim=1)\n",
    "        output_ids = input_ids\n",
    "        for i in range(token_max_len-1):\n",
    "            bs, seq_len = output_ids.size()\n",
    "            mask = torch.ones(bs, seq_len).to('cuda')\n",
    "            logits = model(input_ids=output_ids, attention_mask=mask, labels=None)['logits'] # bs, seq_len, vocab_size\n",
    "            next_token = torch.unsqueeze(torch.argmax(softmax(logits), dim=-1)[:, -1], 1)\n",
    "            output_ids = torch.cat((output_ids, next_token), dim=-1) # bs, seq_len, 1\n",
    "\n",
    "        for count in range(batch_size):\n",
    "            gold_token_len = (input_ids.shape[1]-1) // 2\n",
    "            if torch.equal(input_ids[count][:gold_token_len], output_ids[count][gold_token_len*2:]):\n",
    "                success_copies += 1\n",
    "        string_idx += batch_size\n",
    "print(success_copies / (batch_size * batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_sft_result = [0.9, 0.825, 0.75, 0.6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f0decf89dea8b8cce19d7a5cdb4ace579cf830b296cd71e2c61ad7931d4bb70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
