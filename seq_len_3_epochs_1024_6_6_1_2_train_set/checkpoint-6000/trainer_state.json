{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8104275005065172,
  "eval_steps": 2000,
  "global_step": 6000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006753562504220977,
      "grad_norm": 4.797351360321045,
      "learning_rate": 0.0001995497320905939,
      "loss": 5.7047,
      "step": 50
    },
    {
      "epoch": 0.013507125008441954,
      "grad_norm": 15.181062698364258,
      "learning_rate": 0.00019909946418118782,
      "loss": 4.9323,
      "step": 100
    },
    {
      "epoch": 0.02026068751266293,
      "grad_norm": 5.193338871002197,
      "learning_rate": 0.00019864919627178172,
      "loss": 4.727,
      "step": 150
    },
    {
      "epoch": 0.027014250016883908,
      "grad_norm": 3.2154667377471924,
      "learning_rate": 0.00019819892836237562,
      "loss": 4.5039,
      "step": 200
    },
    {
      "epoch": 0.03376781252110488,
      "grad_norm": 2.4547903537750244,
      "learning_rate": 0.00019774866045296953,
      "loss": 4.3972,
      "step": 250
    },
    {
      "epoch": 0.04052137502532586,
      "grad_norm": 2.793060064315796,
      "learning_rate": 0.00019729839254356343,
      "loss": 4.291,
      "step": 300
    },
    {
      "epoch": 0.04727493752954683,
      "grad_norm": 2.1846165657043457,
      "learning_rate": 0.00019684812463415733,
      "loss": 4.2086,
      "step": 350
    },
    {
      "epoch": 0.054028500033767815,
      "grad_norm": 2.285574197769165,
      "learning_rate": 0.00019639785672475124,
      "loss": 4.1779,
      "step": 400
    },
    {
      "epoch": 0.06078206253798879,
      "grad_norm": 2.1679940223693848,
      "learning_rate": 0.00019594758881534514,
      "loss": 4.1111,
      "step": 450
    },
    {
      "epoch": 0.06753562504220977,
      "grad_norm": 2.131826877593994,
      "learning_rate": 0.00019549732090593904,
      "loss": 4.0626,
      "step": 500
    },
    {
      "epoch": 0.07428918754643074,
      "grad_norm": 2.2510268688201904,
      "learning_rate": 0.00019504705299653295,
      "loss": 4.0706,
      "step": 550
    },
    {
      "epoch": 0.08104275005065172,
      "grad_norm": 1.887534260749817,
      "learning_rate": 0.00019459678508712685,
      "loss": 4.0381,
      "step": 600
    },
    {
      "epoch": 0.08779631255487269,
      "grad_norm": 2.1040523052215576,
      "learning_rate": 0.00019414651717772076,
      "loss": 4.0174,
      "step": 650
    },
    {
      "epoch": 0.09454987505909367,
      "grad_norm": 1.8049144744873047,
      "learning_rate": 0.00019369624926831466,
      "loss": 3.9909,
      "step": 700
    },
    {
      "epoch": 0.10130343756331465,
      "grad_norm": 1.6007177829742432,
      "learning_rate": 0.00019324598135890856,
      "loss": 3.9624,
      "step": 750
    },
    {
      "epoch": 0.10805700006753563,
      "grad_norm": 1.710479497909546,
      "learning_rate": 0.00019279571344950247,
      "loss": 3.9792,
      "step": 800
    },
    {
      "epoch": 0.1148105625717566,
      "grad_norm": 1.973121166229248,
      "learning_rate": 0.00019234544554009637,
      "loss": 3.9295,
      "step": 850
    },
    {
      "epoch": 0.12156412507597758,
      "grad_norm": 1.5859814882278442,
      "learning_rate": 0.00019189517763069027,
      "loss": 3.9739,
      "step": 900
    },
    {
      "epoch": 0.12831768758019854,
      "grad_norm": 2.0452563762664795,
      "learning_rate": 0.00019144490972128418,
      "loss": 3.9342,
      "step": 950
    },
    {
      "epoch": 0.13507125008441953,
      "grad_norm": 1.8033288717269897,
      "learning_rate": 0.00019099464181187808,
      "loss": 3.9422,
      "step": 1000
    },
    {
      "epoch": 0.14182481258864052,
      "grad_norm": 1.5852797031402588,
      "learning_rate": 0.00019054437390247198,
      "loss": 3.9252,
      "step": 1050
    },
    {
      "epoch": 0.14857837509286148,
      "grad_norm": 1.552024006843567,
      "learning_rate": 0.00019009410599306589,
      "loss": 3.9236,
      "step": 1100
    },
    {
      "epoch": 0.15533193759708247,
      "grad_norm": 2.1475131511688232,
      "learning_rate": 0.0001896438380836598,
      "loss": 3.8821,
      "step": 1150
    },
    {
      "epoch": 0.16208550010130343,
      "grad_norm": 1.7658400535583496,
      "learning_rate": 0.0001891935701742537,
      "loss": 3.9026,
      "step": 1200
    },
    {
      "epoch": 0.16883906260552442,
      "grad_norm": 1.560481071472168,
      "learning_rate": 0.0001887433022648476,
      "loss": 3.8758,
      "step": 1250
    },
    {
      "epoch": 0.17559262510974538,
      "grad_norm": 1.5494674444198608,
      "learning_rate": 0.0001882930343554415,
      "loss": 3.8715,
      "step": 1300
    },
    {
      "epoch": 0.18234618761396637,
      "grad_norm": 1.6300110816955566,
      "learning_rate": 0.0001878427664460354,
      "loss": 3.8603,
      "step": 1350
    },
    {
      "epoch": 0.18909975011818733,
      "grad_norm": 1.714105248451233,
      "learning_rate": 0.0001873924985366293,
      "loss": 3.8602,
      "step": 1400
    },
    {
      "epoch": 0.19585331262240832,
      "grad_norm": 1.8106303215026855,
      "learning_rate": 0.0001869422306272232,
      "loss": 3.8603,
      "step": 1450
    },
    {
      "epoch": 0.2026068751266293,
      "grad_norm": 1.6805721521377563,
      "learning_rate": 0.00018649196271781711,
      "loss": 3.8406,
      "step": 1500
    },
    {
      "epoch": 0.20936043763085027,
      "grad_norm": 1.5509318113327026,
      "learning_rate": 0.00018604169480841102,
      "loss": 3.8378,
      "step": 1550
    },
    {
      "epoch": 0.21611400013507126,
      "grad_norm": 1.5121545791625977,
      "learning_rate": 0.00018559142689900492,
      "loss": 3.8215,
      "step": 1600
    },
    {
      "epoch": 0.22286756263929222,
      "grad_norm": 1.778485655784607,
      "learning_rate": 0.00018514115898959882,
      "loss": 3.8705,
      "step": 1650
    },
    {
      "epoch": 0.2296211251435132,
      "grad_norm": 1.732348918914795,
      "learning_rate": 0.00018469089108019273,
      "loss": 3.8651,
      "step": 1700
    },
    {
      "epoch": 0.23637468764773417,
      "grad_norm": 1.3877474069595337,
      "learning_rate": 0.00018424062317078663,
      "loss": 3.833,
      "step": 1750
    },
    {
      "epoch": 0.24312825015195516,
      "grad_norm": 1.6984002590179443,
      "learning_rate": 0.00018379035526138054,
      "loss": 3.7895,
      "step": 1800
    },
    {
      "epoch": 0.24988181265617612,
      "grad_norm": 1.7828930616378784,
      "learning_rate": 0.00018334008735197444,
      "loss": 3.8151,
      "step": 1850
    },
    {
      "epoch": 0.2566353751603971,
      "grad_norm": 1.5281524658203125,
      "learning_rate": 0.00018288981944256834,
      "loss": 3.8541,
      "step": 1900
    },
    {
      "epoch": 0.2633889376646181,
      "grad_norm": 1.577389121055603,
      "learning_rate": 0.00018243955153316225,
      "loss": 3.8296,
      "step": 1950
    },
    {
      "epoch": 0.27014250016883906,
      "grad_norm": 1.5177416801452637,
      "learning_rate": 0.00018198928362375612,
      "loss": 3.8045,
      "step": 2000
    },
    {
      "epoch": 0.27014250016883906,
      "eval_loss": 3.8133201599121094,
      "eval_runtime": 159.712,
      "eval_samples_per_second": 46.296,
      "eval_steps_per_second": 5.792,
      "step": 2000
    },
    {
      "epoch": 0.27689606267306005,
      "grad_norm": 1.6765632629394531,
      "learning_rate": 0.00018153901571435005,
      "loss": 3.792,
      "step": 2050
    },
    {
      "epoch": 0.28364962517728104,
      "grad_norm": 1.5799857378005981,
      "learning_rate": 0.00018108874780494396,
      "loss": 3.7752,
      "step": 2100
    },
    {
      "epoch": 0.290403187681502,
      "grad_norm": 1.6844377517700195,
      "learning_rate": 0.00018063847989553786,
      "loss": 3.809,
      "step": 2150
    },
    {
      "epoch": 0.29715675018572296,
      "grad_norm": 3.014256000518799,
      "learning_rate": 0.00018018821198613176,
      "loss": 3.7871,
      "step": 2200
    },
    {
      "epoch": 0.30391031268994395,
      "grad_norm": 1.465437650680542,
      "learning_rate": 0.00017973794407672567,
      "loss": 3.822,
      "step": 2250
    },
    {
      "epoch": 0.31066387519416494,
      "grad_norm": 1.410904884338379,
      "learning_rate": 0.00017928767616731957,
      "loss": 3.7745,
      "step": 2300
    },
    {
      "epoch": 0.3174174376983859,
      "grad_norm": 1.4237570762634277,
      "learning_rate": 0.00017883740825791347,
      "loss": 3.7633,
      "step": 2350
    },
    {
      "epoch": 0.32417100020260686,
      "grad_norm": 1.537359595298767,
      "learning_rate": 0.00017838714034850738,
      "loss": 3.7977,
      "step": 2400
    },
    {
      "epoch": 0.33092456270682785,
      "grad_norm": 1.5716197490692139,
      "learning_rate": 0.00017793687243910125,
      "loss": 3.7861,
      "step": 2450
    },
    {
      "epoch": 0.33767812521104884,
      "grad_norm": 1.57917320728302,
      "learning_rate": 0.00017748660452969516,
      "loss": 3.7189,
      "step": 2500
    },
    {
      "epoch": 0.34443168771526983,
      "grad_norm": 1.6173279285430908,
      "learning_rate": 0.0001770363366202891,
      "loss": 3.7881,
      "step": 2550
    },
    {
      "epoch": 0.35118525021949076,
      "grad_norm": 1.536758303642273,
      "learning_rate": 0.000176586068710883,
      "loss": 3.768,
      "step": 2600
    },
    {
      "epoch": 0.35793881272371175,
      "grad_norm": 1.4064674377441406,
      "learning_rate": 0.0001761358008014769,
      "loss": 3.7485,
      "step": 2650
    },
    {
      "epoch": 0.36469237522793274,
      "grad_norm": 1.4671449661254883,
      "learning_rate": 0.0001756855328920708,
      "loss": 3.7543,
      "step": 2700
    },
    {
      "epoch": 0.37144593773215373,
      "grad_norm": 1.6473393440246582,
      "learning_rate": 0.0001752352649826647,
      "loss": 3.7357,
      "step": 2750
    },
    {
      "epoch": 0.37819950023637466,
      "grad_norm": 4.014481067657471,
      "learning_rate": 0.0001747849970732586,
      "loss": 3.7691,
      "step": 2800
    },
    {
      "epoch": 0.38495306274059565,
      "grad_norm": 1.5310049057006836,
      "learning_rate": 0.00017433472916385248,
      "loss": 3.7281,
      "step": 2850
    },
    {
      "epoch": 0.39170662524481664,
      "grad_norm": 1.6480664014816284,
      "learning_rate": 0.00017388446125444639,
      "loss": 3.7534,
      "step": 2900
    },
    {
      "epoch": 0.39846018774903763,
      "grad_norm": 1.3171087503433228,
      "learning_rate": 0.0001734341933450403,
      "loss": 3.7541,
      "step": 2950
    },
    {
      "epoch": 0.4052137502532586,
      "grad_norm": 1.364017128944397,
      "learning_rate": 0.00017298392543563422,
      "loss": 3.7331,
      "step": 3000
    },
    {
      "epoch": 0.41196731275747955,
      "grad_norm": 1.5110623836517334,
      "learning_rate": 0.00017253365752622812,
      "loss": 3.7255,
      "step": 3050
    },
    {
      "epoch": 0.41872087526170054,
      "grad_norm": 1.3015849590301514,
      "learning_rate": 0.00017208338961682203,
      "loss": 3.7416,
      "step": 3100
    },
    {
      "epoch": 0.42547443776592153,
      "grad_norm": 1.3021365404129028,
      "learning_rate": 0.00017163312170741593,
      "loss": 3.7383,
      "step": 3150
    },
    {
      "epoch": 0.4322280002701425,
      "grad_norm": 1.3766306638717651,
      "learning_rate": 0.00017118285379800983,
      "loss": 3.6871,
      "step": 3200
    },
    {
      "epoch": 0.43898156277436345,
      "grad_norm": 1.3684347867965698,
      "learning_rate": 0.00017073258588860374,
      "loss": 3.7271,
      "step": 3250
    },
    {
      "epoch": 0.44573512527858444,
      "grad_norm": 1.3719110488891602,
      "learning_rate": 0.0001702823179791976,
      "loss": 3.7355,
      "step": 3300
    },
    {
      "epoch": 0.45248868778280543,
      "grad_norm": 1.498680591583252,
      "learning_rate": 0.00016983205006979152,
      "loss": 3.728,
      "step": 3350
    },
    {
      "epoch": 0.4592422502870264,
      "grad_norm": 1.466575026512146,
      "learning_rate": 0.00016938178216038542,
      "loss": 3.6906,
      "step": 3400
    },
    {
      "epoch": 0.4659958127912474,
      "grad_norm": 1.3103606700897217,
      "learning_rate": 0.00016893151425097935,
      "loss": 3.7358,
      "step": 3450
    },
    {
      "epoch": 0.47274937529546834,
      "grad_norm": 1.420717716217041,
      "learning_rate": 0.00016848124634157325,
      "loss": 3.7224,
      "step": 3500
    },
    {
      "epoch": 0.47950293779968933,
      "grad_norm": 1.706626057624817,
      "learning_rate": 0.00016803097843216716,
      "loss": 3.7238,
      "step": 3550
    },
    {
      "epoch": 0.4862565003039103,
      "grad_norm": 1.5862548351287842,
      "learning_rate": 0.00016758071052276106,
      "loss": 3.7105,
      "step": 3600
    },
    {
      "epoch": 0.4930100628081313,
      "grad_norm": 1.3878703117370605,
      "learning_rate": 0.00016713044261335496,
      "loss": 3.6743,
      "step": 3650
    },
    {
      "epoch": 0.49976362531235224,
      "grad_norm": 1.3619742393493652,
      "learning_rate": 0.00016668017470394887,
      "loss": 3.7034,
      "step": 3700
    },
    {
      "epoch": 0.5065171878165733,
      "grad_norm": 1.425737977027893,
      "learning_rate": 0.00016622990679454274,
      "loss": 3.7375,
      "step": 3750
    },
    {
      "epoch": 0.5132707503207942,
      "grad_norm": 1.3887169361114502,
      "learning_rate": 0.00016577963888513665,
      "loss": 3.6894,
      "step": 3800
    },
    {
      "epoch": 0.5200243128250152,
      "grad_norm": 1.3541208505630493,
      "learning_rate": 0.00016532937097573055,
      "loss": 3.6659,
      "step": 3850
    },
    {
      "epoch": 0.5267778753292361,
      "grad_norm": 1.2554895877838135,
      "learning_rate": 0.00016487910306632448,
      "loss": 3.709,
      "step": 3900
    },
    {
      "epoch": 0.5335314378334571,
      "grad_norm": 1.4929972887039185,
      "learning_rate": 0.00016442883515691839,
      "loss": 3.686,
      "step": 3950
    },
    {
      "epoch": 0.5402850003376781,
      "grad_norm": 1.4398239850997925,
      "learning_rate": 0.0001639785672475123,
      "loss": 3.6977,
      "step": 4000
    },
    {
      "epoch": 0.5402850003376781,
      "eval_loss": 3.7073452472686768,
      "eval_runtime": 159.8807,
      "eval_samples_per_second": 46.247,
      "eval_steps_per_second": 5.786,
      "step": 4000
    },
    {
      "epoch": 0.5470385628418991,
      "grad_norm": 1.3161286115646362,
      "learning_rate": 0.0001635282993381062,
      "loss": 3.6512,
      "step": 4050
    },
    {
      "epoch": 0.5537921253461201,
      "grad_norm": 1.4418870210647583,
      "learning_rate": 0.0001630780314287001,
      "loss": 3.6923,
      "step": 4100
    },
    {
      "epoch": 0.5605456878503411,
      "grad_norm": 1.4249507188796997,
      "learning_rate": 0.00016262776351929397,
      "loss": 3.6516,
      "step": 4150
    },
    {
      "epoch": 0.5672992503545621,
      "grad_norm": 1.5296441316604614,
      "learning_rate": 0.00016217749560988788,
      "loss": 3.6847,
      "step": 4200
    },
    {
      "epoch": 0.574052812858783,
      "grad_norm": 1.6135449409484863,
      "learning_rate": 0.00016172722770048178,
      "loss": 3.6627,
      "step": 4250
    },
    {
      "epoch": 0.580806375363004,
      "grad_norm": 1.4726654291152954,
      "learning_rate": 0.00016127695979107568,
      "loss": 3.692,
      "step": 4300
    },
    {
      "epoch": 0.5875599378672249,
      "grad_norm": 1.2898463010787964,
      "learning_rate": 0.0001608266918816696,
      "loss": 3.668,
      "step": 4350
    },
    {
      "epoch": 0.5943135003714459,
      "grad_norm": 1.2962331771850586,
      "learning_rate": 0.00016037642397226352,
      "loss": 3.698,
      "step": 4400
    },
    {
      "epoch": 0.6010670628756669,
      "grad_norm": 1.3446184396743774,
      "learning_rate": 0.00015992615606285742,
      "loss": 3.6836,
      "step": 4450
    },
    {
      "epoch": 0.6078206253798879,
      "grad_norm": 1.277993083000183,
      "learning_rate": 0.00015947588815345132,
      "loss": 3.6699,
      "step": 4500
    },
    {
      "epoch": 0.6145741878841089,
      "grad_norm": 1.3769346475601196,
      "learning_rate": 0.00015902562024404523,
      "loss": 3.6867,
      "step": 4550
    },
    {
      "epoch": 0.6213277503883299,
      "grad_norm": 1.3811522722244263,
      "learning_rate": 0.0001585753523346391,
      "loss": 3.6302,
      "step": 4600
    },
    {
      "epoch": 0.6280813128925509,
      "grad_norm": 1.3310779333114624,
      "learning_rate": 0.000158125084425233,
      "loss": 3.6923,
      "step": 4650
    },
    {
      "epoch": 0.6348348753967717,
      "grad_norm": 1.2519299983978271,
      "learning_rate": 0.0001576748165158269,
      "loss": 3.6687,
      "step": 4700
    },
    {
      "epoch": 0.6415884379009927,
      "grad_norm": 1.3973385095596313,
      "learning_rate": 0.00015722454860642081,
      "loss": 3.6477,
      "step": 4750
    },
    {
      "epoch": 0.6483420004052137,
      "grad_norm": 1.449474811553955,
      "learning_rate": 0.00015677428069701474,
      "loss": 3.652,
      "step": 4800
    },
    {
      "epoch": 0.6550955629094347,
      "grad_norm": 1.3876686096191406,
      "learning_rate": 0.00015632401278760865,
      "loss": 3.6443,
      "step": 4850
    },
    {
      "epoch": 0.6618491254136557,
      "grad_norm": 1.314900279045105,
      "learning_rate": 0.00015587374487820255,
      "loss": 3.6551,
      "step": 4900
    },
    {
      "epoch": 0.6686026879178767,
      "grad_norm": 1.2933001518249512,
      "learning_rate": 0.00015542347696879646,
      "loss": 3.6468,
      "step": 4950
    },
    {
      "epoch": 0.6753562504220977,
      "grad_norm": 1.3287744522094727,
      "learning_rate": 0.00015497320905939033,
      "loss": 3.6716,
      "step": 5000
    },
    {
      "epoch": 0.6821098129263187,
      "grad_norm": 1.3460630178451538,
      "learning_rate": 0.00015452294114998424,
      "loss": 3.6599,
      "step": 5050
    },
    {
      "epoch": 0.6888633754305397,
      "grad_norm": 1.2755683660507202,
      "learning_rate": 0.00015407267324057814,
      "loss": 3.6412,
      "step": 5100
    },
    {
      "epoch": 0.6956169379347605,
      "grad_norm": 1.3904697895050049,
      "learning_rate": 0.00015362240533117204,
      "loss": 3.63,
      "step": 5150
    },
    {
      "epoch": 0.7023705004389815,
      "grad_norm": 1.2476608753204346,
      "learning_rate": 0.00015317213742176595,
      "loss": 3.6374,
      "step": 5200
    },
    {
      "epoch": 0.7091240629432025,
      "grad_norm": 1.2476223707199097,
      "learning_rate": 0.00015272186951235988,
      "loss": 3.6499,
      "step": 5250
    },
    {
      "epoch": 0.7158776254474235,
      "grad_norm": 1.3357038497924805,
      "learning_rate": 0.00015227160160295378,
      "loss": 3.6323,
      "step": 5300
    },
    {
      "epoch": 0.7226311879516445,
      "grad_norm": 1.5791229009628296,
      "learning_rate": 0.00015182133369354768,
      "loss": 3.6389,
      "step": 5350
    },
    {
      "epoch": 0.7293847504558655,
      "grad_norm": 1.6427063941955566,
      "learning_rate": 0.0001513710657841416,
      "loss": 3.6754,
      "step": 5400
    },
    {
      "epoch": 0.7361383129600865,
      "grad_norm": 1.3833472728729248,
      "learning_rate": 0.00015092079787473546,
      "loss": 3.6442,
      "step": 5450
    },
    {
      "epoch": 0.7428918754643075,
      "grad_norm": 1.4378949403762817,
      "learning_rate": 0.00015047052996532937,
      "loss": 3.666,
      "step": 5500
    },
    {
      "epoch": 0.7496454379685284,
      "grad_norm": 1.4182040691375732,
      "learning_rate": 0.00015002026205592327,
      "loss": 3.6259,
      "step": 5550
    },
    {
      "epoch": 0.7563990004727493,
      "grad_norm": 1.3786461353302002,
      "learning_rate": 0.00014956999414651717,
      "loss": 3.6317,
      "step": 5600
    },
    {
      "epoch": 0.7631525629769703,
      "grad_norm": 1.2590128183364868,
      "learning_rate": 0.00014911972623711108,
      "loss": 3.6525,
      "step": 5650
    },
    {
      "epoch": 0.7699061254811913,
      "grad_norm": 1.2519959211349487,
      "learning_rate": 0.000148669458327705,
      "loss": 3.6238,
      "step": 5700
    },
    {
      "epoch": 0.7766596879854123,
      "grad_norm": 1.2755470275878906,
      "learning_rate": 0.0001482191904182989,
      "loss": 3.634,
      "step": 5750
    },
    {
      "epoch": 0.7834132504896333,
      "grad_norm": 1.3027544021606445,
      "learning_rate": 0.00014776892250889281,
      "loss": 3.6199,
      "step": 5800
    },
    {
      "epoch": 0.7901668129938543,
      "grad_norm": 1.4003950357437134,
      "learning_rate": 0.00014731865459948672,
      "loss": 3.6258,
      "step": 5850
    },
    {
      "epoch": 0.7969203754980753,
      "grad_norm": 1.1184207201004028,
      "learning_rate": 0.0001468683866900806,
      "loss": 3.5848,
      "step": 5900
    },
    {
      "epoch": 0.8036739380022963,
      "grad_norm": 1.389952301979065,
      "learning_rate": 0.0001464181187806745,
      "loss": 3.6466,
      "step": 5950
    },
    {
      "epoch": 0.8104275005065172,
      "grad_norm": 2.5175700187683105,
      "learning_rate": 0.0001459678508712684,
      "loss": 3.6367,
      "step": 6000
    },
    {
      "epoch": 0.8104275005065172,
      "eval_loss": 3.6482694149017334,
      "eval_runtime": 159.6265,
      "eval_samples_per_second": 46.321,
      "eval_steps_per_second": 5.795,
      "step": 6000
    }
  ],
  "logging_steps": 50,
  "max_steps": 22209,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 2000,
  "total_flos": 6.5402403028992e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
