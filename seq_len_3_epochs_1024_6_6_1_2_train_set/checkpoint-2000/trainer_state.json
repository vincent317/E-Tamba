{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.27014250016883906,
  "eval_steps": 2000,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006753562504220977,
      "grad_norm": 4.797351360321045,
      "learning_rate": 0.0001995497320905939,
      "loss": 5.7047,
      "step": 50
    },
    {
      "epoch": 0.013507125008441954,
      "grad_norm": 15.181062698364258,
      "learning_rate": 0.00019909946418118782,
      "loss": 4.9323,
      "step": 100
    },
    {
      "epoch": 0.02026068751266293,
      "grad_norm": 5.193338871002197,
      "learning_rate": 0.00019864919627178172,
      "loss": 4.727,
      "step": 150
    },
    {
      "epoch": 0.027014250016883908,
      "grad_norm": 3.2154667377471924,
      "learning_rate": 0.00019819892836237562,
      "loss": 4.5039,
      "step": 200
    },
    {
      "epoch": 0.03376781252110488,
      "grad_norm": 2.4547903537750244,
      "learning_rate": 0.00019774866045296953,
      "loss": 4.3972,
      "step": 250
    },
    {
      "epoch": 0.04052137502532586,
      "grad_norm": 2.793060064315796,
      "learning_rate": 0.00019729839254356343,
      "loss": 4.291,
      "step": 300
    },
    {
      "epoch": 0.04727493752954683,
      "grad_norm": 2.1846165657043457,
      "learning_rate": 0.00019684812463415733,
      "loss": 4.2086,
      "step": 350
    },
    {
      "epoch": 0.054028500033767815,
      "grad_norm": 2.285574197769165,
      "learning_rate": 0.00019639785672475124,
      "loss": 4.1779,
      "step": 400
    },
    {
      "epoch": 0.06078206253798879,
      "grad_norm": 2.1679940223693848,
      "learning_rate": 0.00019594758881534514,
      "loss": 4.1111,
      "step": 450
    },
    {
      "epoch": 0.06753562504220977,
      "grad_norm": 2.131826877593994,
      "learning_rate": 0.00019549732090593904,
      "loss": 4.0626,
      "step": 500
    },
    {
      "epoch": 0.07428918754643074,
      "grad_norm": 2.2510268688201904,
      "learning_rate": 0.00019504705299653295,
      "loss": 4.0706,
      "step": 550
    },
    {
      "epoch": 0.08104275005065172,
      "grad_norm": 1.887534260749817,
      "learning_rate": 0.00019459678508712685,
      "loss": 4.0381,
      "step": 600
    },
    {
      "epoch": 0.08779631255487269,
      "grad_norm": 2.1040523052215576,
      "learning_rate": 0.00019414651717772076,
      "loss": 4.0174,
      "step": 650
    },
    {
      "epoch": 0.09454987505909367,
      "grad_norm": 1.8049144744873047,
      "learning_rate": 0.00019369624926831466,
      "loss": 3.9909,
      "step": 700
    },
    {
      "epoch": 0.10130343756331465,
      "grad_norm": 1.6007177829742432,
      "learning_rate": 0.00019324598135890856,
      "loss": 3.9624,
      "step": 750
    },
    {
      "epoch": 0.10805700006753563,
      "grad_norm": 1.710479497909546,
      "learning_rate": 0.00019279571344950247,
      "loss": 3.9792,
      "step": 800
    },
    {
      "epoch": 0.1148105625717566,
      "grad_norm": 1.973121166229248,
      "learning_rate": 0.00019234544554009637,
      "loss": 3.9295,
      "step": 850
    },
    {
      "epoch": 0.12156412507597758,
      "grad_norm": 1.5859814882278442,
      "learning_rate": 0.00019189517763069027,
      "loss": 3.9739,
      "step": 900
    },
    {
      "epoch": 0.12831768758019854,
      "grad_norm": 2.0452563762664795,
      "learning_rate": 0.00019144490972128418,
      "loss": 3.9342,
      "step": 950
    },
    {
      "epoch": 0.13507125008441953,
      "grad_norm": 1.8033288717269897,
      "learning_rate": 0.00019099464181187808,
      "loss": 3.9422,
      "step": 1000
    },
    {
      "epoch": 0.14182481258864052,
      "grad_norm": 1.5852797031402588,
      "learning_rate": 0.00019054437390247198,
      "loss": 3.9252,
      "step": 1050
    },
    {
      "epoch": 0.14857837509286148,
      "grad_norm": 1.552024006843567,
      "learning_rate": 0.00019009410599306589,
      "loss": 3.9236,
      "step": 1100
    },
    {
      "epoch": 0.15533193759708247,
      "grad_norm": 2.1475131511688232,
      "learning_rate": 0.0001896438380836598,
      "loss": 3.8821,
      "step": 1150
    },
    {
      "epoch": 0.16208550010130343,
      "grad_norm": 1.7658400535583496,
      "learning_rate": 0.0001891935701742537,
      "loss": 3.9026,
      "step": 1200
    },
    {
      "epoch": 0.16883906260552442,
      "grad_norm": 1.560481071472168,
      "learning_rate": 0.0001887433022648476,
      "loss": 3.8758,
      "step": 1250
    },
    {
      "epoch": 0.17559262510974538,
      "grad_norm": 1.5494674444198608,
      "learning_rate": 0.0001882930343554415,
      "loss": 3.8715,
      "step": 1300
    },
    {
      "epoch": 0.18234618761396637,
      "grad_norm": 1.6300110816955566,
      "learning_rate": 0.0001878427664460354,
      "loss": 3.8603,
      "step": 1350
    },
    {
      "epoch": 0.18909975011818733,
      "grad_norm": 1.714105248451233,
      "learning_rate": 0.0001873924985366293,
      "loss": 3.8602,
      "step": 1400
    },
    {
      "epoch": 0.19585331262240832,
      "grad_norm": 1.8106303215026855,
      "learning_rate": 0.0001869422306272232,
      "loss": 3.8603,
      "step": 1450
    },
    {
      "epoch": 0.2026068751266293,
      "grad_norm": 1.6805721521377563,
      "learning_rate": 0.00018649196271781711,
      "loss": 3.8406,
      "step": 1500
    },
    {
      "epoch": 0.20936043763085027,
      "grad_norm": 1.5509318113327026,
      "learning_rate": 0.00018604169480841102,
      "loss": 3.8378,
      "step": 1550
    },
    {
      "epoch": 0.21611400013507126,
      "grad_norm": 1.5121545791625977,
      "learning_rate": 0.00018559142689900492,
      "loss": 3.8215,
      "step": 1600
    },
    {
      "epoch": 0.22286756263929222,
      "grad_norm": 1.778485655784607,
      "learning_rate": 0.00018514115898959882,
      "loss": 3.8705,
      "step": 1650
    },
    {
      "epoch": 0.2296211251435132,
      "grad_norm": 1.732348918914795,
      "learning_rate": 0.00018469089108019273,
      "loss": 3.8651,
      "step": 1700
    },
    {
      "epoch": 0.23637468764773417,
      "grad_norm": 1.3877474069595337,
      "learning_rate": 0.00018424062317078663,
      "loss": 3.833,
      "step": 1750
    },
    {
      "epoch": 0.24312825015195516,
      "grad_norm": 1.6984002590179443,
      "learning_rate": 0.00018379035526138054,
      "loss": 3.7895,
      "step": 1800
    },
    {
      "epoch": 0.24988181265617612,
      "grad_norm": 1.7828930616378784,
      "learning_rate": 0.00018334008735197444,
      "loss": 3.8151,
      "step": 1850
    },
    {
      "epoch": 0.2566353751603971,
      "grad_norm": 1.5281524658203125,
      "learning_rate": 0.00018288981944256834,
      "loss": 3.8541,
      "step": 1900
    },
    {
      "epoch": 0.2633889376646181,
      "grad_norm": 1.577389121055603,
      "learning_rate": 0.00018243955153316225,
      "loss": 3.8296,
      "step": 1950
    },
    {
      "epoch": 0.27014250016883906,
      "grad_norm": 1.5177416801452637,
      "learning_rate": 0.00018198928362375612,
      "loss": 3.8045,
      "step": 2000
    },
    {
      "epoch": 0.27014250016883906,
      "eval_loss": 3.8133201599121094,
      "eval_runtime": 159.712,
      "eval_samples_per_second": 46.296,
      "eval_steps_per_second": 5.792,
      "step": 2000
    }
  ],
  "logging_steps": 50,
  "max_steps": 22209,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 2000,
  "total_flos": 2.1800801009664e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
