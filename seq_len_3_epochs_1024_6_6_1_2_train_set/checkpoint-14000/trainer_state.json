{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.8909975011818734,
  "eval_steps": 2000,
  "global_step": 14000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006753562504220977,
      "grad_norm": 4.797351360321045,
      "learning_rate": 0.0001995497320905939,
      "loss": 5.7047,
      "step": 50
    },
    {
      "epoch": 0.013507125008441954,
      "grad_norm": 15.181062698364258,
      "learning_rate": 0.00019909946418118782,
      "loss": 4.9323,
      "step": 100
    },
    {
      "epoch": 0.02026068751266293,
      "grad_norm": 5.193338871002197,
      "learning_rate": 0.00019864919627178172,
      "loss": 4.727,
      "step": 150
    },
    {
      "epoch": 0.027014250016883908,
      "grad_norm": 3.2154667377471924,
      "learning_rate": 0.00019819892836237562,
      "loss": 4.5039,
      "step": 200
    },
    {
      "epoch": 0.03376781252110488,
      "grad_norm": 2.4547903537750244,
      "learning_rate": 0.00019774866045296953,
      "loss": 4.3972,
      "step": 250
    },
    {
      "epoch": 0.04052137502532586,
      "grad_norm": 2.793060064315796,
      "learning_rate": 0.00019729839254356343,
      "loss": 4.291,
      "step": 300
    },
    {
      "epoch": 0.04727493752954683,
      "grad_norm": 2.1846165657043457,
      "learning_rate": 0.00019684812463415733,
      "loss": 4.2086,
      "step": 350
    },
    {
      "epoch": 0.054028500033767815,
      "grad_norm": 2.285574197769165,
      "learning_rate": 0.00019639785672475124,
      "loss": 4.1779,
      "step": 400
    },
    {
      "epoch": 0.06078206253798879,
      "grad_norm": 2.1679940223693848,
      "learning_rate": 0.00019594758881534514,
      "loss": 4.1111,
      "step": 450
    },
    {
      "epoch": 0.06753562504220977,
      "grad_norm": 2.131826877593994,
      "learning_rate": 0.00019549732090593904,
      "loss": 4.0626,
      "step": 500
    },
    {
      "epoch": 0.07428918754643074,
      "grad_norm": 2.2510268688201904,
      "learning_rate": 0.00019504705299653295,
      "loss": 4.0706,
      "step": 550
    },
    {
      "epoch": 0.08104275005065172,
      "grad_norm": 1.887534260749817,
      "learning_rate": 0.00019459678508712685,
      "loss": 4.0381,
      "step": 600
    },
    {
      "epoch": 0.08779631255487269,
      "grad_norm": 2.1040523052215576,
      "learning_rate": 0.00019414651717772076,
      "loss": 4.0174,
      "step": 650
    },
    {
      "epoch": 0.09454987505909367,
      "grad_norm": 1.8049144744873047,
      "learning_rate": 0.00019369624926831466,
      "loss": 3.9909,
      "step": 700
    },
    {
      "epoch": 0.10130343756331465,
      "grad_norm": 1.6007177829742432,
      "learning_rate": 0.00019324598135890856,
      "loss": 3.9624,
      "step": 750
    },
    {
      "epoch": 0.10805700006753563,
      "grad_norm": 1.710479497909546,
      "learning_rate": 0.00019279571344950247,
      "loss": 3.9792,
      "step": 800
    },
    {
      "epoch": 0.1148105625717566,
      "grad_norm": 1.973121166229248,
      "learning_rate": 0.00019234544554009637,
      "loss": 3.9295,
      "step": 850
    },
    {
      "epoch": 0.12156412507597758,
      "grad_norm": 1.5859814882278442,
      "learning_rate": 0.00019189517763069027,
      "loss": 3.9739,
      "step": 900
    },
    {
      "epoch": 0.12831768758019854,
      "grad_norm": 2.0452563762664795,
      "learning_rate": 0.00019144490972128418,
      "loss": 3.9342,
      "step": 950
    },
    {
      "epoch": 0.13507125008441953,
      "grad_norm": 1.8033288717269897,
      "learning_rate": 0.00019099464181187808,
      "loss": 3.9422,
      "step": 1000
    },
    {
      "epoch": 0.14182481258864052,
      "grad_norm": 1.5852797031402588,
      "learning_rate": 0.00019054437390247198,
      "loss": 3.9252,
      "step": 1050
    },
    {
      "epoch": 0.14857837509286148,
      "grad_norm": 1.552024006843567,
      "learning_rate": 0.00019009410599306589,
      "loss": 3.9236,
      "step": 1100
    },
    {
      "epoch": 0.15533193759708247,
      "grad_norm": 2.1475131511688232,
      "learning_rate": 0.0001896438380836598,
      "loss": 3.8821,
      "step": 1150
    },
    {
      "epoch": 0.16208550010130343,
      "grad_norm": 1.7658400535583496,
      "learning_rate": 0.0001891935701742537,
      "loss": 3.9026,
      "step": 1200
    },
    {
      "epoch": 0.16883906260552442,
      "grad_norm": 1.560481071472168,
      "learning_rate": 0.0001887433022648476,
      "loss": 3.8758,
      "step": 1250
    },
    {
      "epoch": 0.17559262510974538,
      "grad_norm": 1.5494674444198608,
      "learning_rate": 0.0001882930343554415,
      "loss": 3.8715,
      "step": 1300
    },
    {
      "epoch": 0.18234618761396637,
      "grad_norm": 1.6300110816955566,
      "learning_rate": 0.0001878427664460354,
      "loss": 3.8603,
      "step": 1350
    },
    {
      "epoch": 0.18909975011818733,
      "grad_norm": 1.714105248451233,
      "learning_rate": 0.0001873924985366293,
      "loss": 3.8602,
      "step": 1400
    },
    {
      "epoch": 0.19585331262240832,
      "grad_norm": 1.8106303215026855,
      "learning_rate": 0.0001869422306272232,
      "loss": 3.8603,
      "step": 1450
    },
    {
      "epoch": 0.2026068751266293,
      "grad_norm": 1.6805721521377563,
      "learning_rate": 0.00018649196271781711,
      "loss": 3.8406,
      "step": 1500
    },
    {
      "epoch": 0.20936043763085027,
      "grad_norm": 1.5509318113327026,
      "learning_rate": 0.00018604169480841102,
      "loss": 3.8378,
      "step": 1550
    },
    {
      "epoch": 0.21611400013507126,
      "grad_norm": 1.5121545791625977,
      "learning_rate": 0.00018559142689900492,
      "loss": 3.8215,
      "step": 1600
    },
    {
      "epoch": 0.22286756263929222,
      "grad_norm": 1.778485655784607,
      "learning_rate": 0.00018514115898959882,
      "loss": 3.8705,
      "step": 1650
    },
    {
      "epoch": 0.2296211251435132,
      "grad_norm": 1.732348918914795,
      "learning_rate": 0.00018469089108019273,
      "loss": 3.8651,
      "step": 1700
    },
    {
      "epoch": 0.23637468764773417,
      "grad_norm": 1.3877474069595337,
      "learning_rate": 0.00018424062317078663,
      "loss": 3.833,
      "step": 1750
    },
    {
      "epoch": 0.24312825015195516,
      "grad_norm": 1.6984002590179443,
      "learning_rate": 0.00018379035526138054,
      "loss": 3.7895,
      "step": 1800
    },
    {
      "epoch": 0.24988181265617612,
      "grad_norm": 1.7828930616378784,
      "learning_rate": 0.00018334008735197444,
      "loss": 3.8151,
      "step": 1850
    },
    {
      "epoch": 0.2566353751603971,
      "grad_norm": 1.5281524658203125,
      "learning_rate": 0.00018288981944256834,
      "loss": 3.8541,
      "step": 1900
    },
    {
      "epoch": 0.2633889376646181,
      "grad_norm": 1.577389121055603,
      "learning_rate": 0.00018243955153316225,
      "loss": 3.8296,
      "step": 1950
    },
    {
      "epoch": 0.27014250016883906,
      "grad_norm": 1.5177416801452637,
      "learning_rate": 0.00018198928362375612,
      "loss": 3.8045,
      "step": 2000
    },
    {
      "epoch": 0.27014250016883906,
      "eval_loss": 3.8133201599121094,
      "eval_runtime": 159.712,
      "eval_samples_per_second": 46.296,
      "eval_steps_per_second": 5.792,
      "step": 2000
    },
    {
      "epoch": 0.27689606267306005,
      "grad_norm": 1.6765632629394531,
      "learning_rate": 0.00018153901571435005,
      "loss": 3.792,
      "step": 2050
    },
    {
      "epoch": 0.28364962517728104,
      "grad_norm": 1.5799857378005981,
      "learning_rate": 0.00018108874780494396,
      "loss": 3.7752,
      "step": 2100
    },
    {
      "epoch": 0.290403187681502,
      "grad_norm": 1.6844377517700195,
      "learning_rate": 0.00018063847989553786,
      "loss": 3.809,
      "step": 2150
    },
    {
      "epoch": 0.29715675018572296,
      "grad_norm": 3.014256000518799,
      "learning_rate": 0.00018018821198613176,
      "loss": 3.7871,
      "step": 2200
    },
    {
      "epoch": 0.30391031268994395,
      "grad_norm": 1.465437650680542,
      "learning_rate": 0.00017973794407672567,
      "loss": 3.822,
      "step": 2250
    },
    {
      "epoch": 0.31066387519416494,
      "grad_norm": 1.410904884338379,
      "learning_rate": 0.00017928767616731957,
      "loss": 3.7745,
      "step": 2300
    },
    {
      "epoch": 0.3174174376983859,
      "grad_norm": 1.4237570762634277,
      "learning_rate": 0.00017883740825791347,
      "loss": 3.7633,
      "step": 2350
    },
    {
      "epoch": 0.32417100020260686,
      "grad_norm": 1.537359595298767,
      "learning_rate": 0.00017838714034850738,
      "loss": 3.7977,
      "step": 2400
    },
    {
      "epoch": 0.33092456270682785,
      "grad_norm": 1.5716197490692139,
      "learning_rate": 0.00017793687243910125,
      "loss": 3.7861,
      "step": 2450
    },
    {
      "epoch": 0.33767812521104884,
      "grad_norm": 1.57917320728302,
      "learning_rate": 0.00017748660452969516,
      "loss": 3.7189,
      "step": 2500
    },
    {
      "epoch": 0.34443168771526983,
      "grad_norm": 1.6173279285430908,
      "learning_rate": 0.0001770363366202891,
      "loss": 3.7881,
      "step": 2550
    },
    {
      "epoch": 0.35118525021949076,
      "grad_norm": 1.536758303642273,
      "learning_rate": 0.000176586068710883,
      "loss": 3.768,
      "step": 2600
    },
    {
      "epoch": 0.35793881272371175,
      "grad_norm": 1.4064674377441406,
      "learning_rate": 0.0001761358008014769,
      "loss": 3.7485,
      "step": 2650
    },
    {
      "epoch": 0.36469237522793274,
      "grad_norm": 1.4671449661254883,
      "learning_rate": 0.0001756855328920708,
      "loss": 3.7543,
      "step": 2700
    },
    {
      "epoch": 0.37144593773215373,
      "grad_norm": 1.6473393440246582,
      "learning_rate": 0.0001752352649826647,
      "loss": 3.7357,
      "step": 2750
    },
    {
      "epoch": 0.37819950023637466,
      "grad_norm": 4.014481067657471,
      "learning_rate": 0.0001747849970732586,
      "loss": 3.7691,
      "step": 2800
    },
    {
      "epoch": 0.38495306274059565,
      "grad_norm": 1.5310049057006836,
      "learning_rate": 0.00017433472916385248,
      "loss": 3.7281,
      "step": 2850
    },
    {
      "epoch": 0.39170662524481664,
      "grad_norm": 1.6480664014816284,
      "learning_rate": 0.00017388446125444639,
      "loss": 3.7534,
      "step": 2900
    },
    {
      "epoch": 0.39846018774903763,
      "grad_norm": 1.3171087503433228,
      "learning_rate": 0.0001734341933450403,
      "loss": 3.7541,
      "step": 2950
    },
    {
      "epoch": 0.4052137502532586,
      "grad_norm": 1.364017128944397,
      "learning_rate": 0.00017298392543563422,
      "loss": 3.7331,
      "step": 3000
    },
    {
      "epoch": 0.41196731275747955,
      "grad_norm": 1.5110623836517334,
      "learning_rate": 0.00017253365752622812,
      "loss": 3.7255,
      "step": 3050
    },
    {
      "epoch": 0.41872087526170054,
      "grad_norm": 1.3015849590301514,
      "learning_rate": 0.00017208338961682203,
      "loss": 3.7416,
      "step": 3100
    },
    {
      "epoch": 0.42547443776592153,
      "grad_norm": 1.3021365404129028,
      "learning_rate": 0.00017163312170741593,
      "loss": 3.7383,
      "step": 3150
    },
    {
      "epoch": 0.4322280002701425,
      "grad_norm": 1.3766306638717651,
      "learning_rate": 0.00017118285379800983,
      "loss": 3.6871,
      "step": 3200
    },
    {
      "epoch": 0.43898156277436345,
      "grad_norm": 1.3684347867965698,
      "learning_rate": 0.00017073258588860374,
      "loss": 3.7271,
      "step": 3250
    },
    {
      "epoch": 0.44573512527858444,
      "grad_norm": 1.3719110488891602,
      "learning_rate": 0.0001702823179791976,
      "loss": 3.7355,
      "step": 3300
    },
    {
      "epoch": 0.45248868778280543,
      "grad_norm": 1.498680591583252,
      "learning_rate": 0.00016983205006979152,
      "loss": 3.728,
      "step": 3350
    },
    {
      "epoch": 0.4592422502870264,
      "grad_norm": 1.466575026512146,
      "learning_rate": 0.00016938178216038542,
      "loss": 3.6906,
      "step": 3400
    },
    {
      "epoch": 0.4659958127912474,
      "grad_norm": 1.3103606700897217,
      "learning_rate": 0.00016893151425097935,
      "loss": 3.7358,
      "step": 3450
    },
    {
      "epoch": 0.47274937529546834,
      "grad_norm": 1.420717716217041,
      "learning_rate": 0.00016848124634157325,
      "loss": 3.7224,
      "step": 3500
    },
    {
      "epoch": 0.47950293779968933,
      "grad_norm": 1.706626057624817,
      "learning_rate": 0.00016803097843216716,
      "loss": 3.7238,
      "step": 3550
    },
    {
      "epoch": 0.4862565003039103,
      "grad_norm": 1.5862548351287842,
      "learning_rate": 0.00016758071052276106,
      "loss": 3.7105,
      "step": 3600
    },
    {
      "epoch": 0.4930100628081313,
      "grad_norm": 1.3878703117370605,
      "learning_rate": 0.00016713044261335496,
      "loss": 3.6743,
      "step": 3650
    },
    {
      "epoch": 0.49976362531235224,
      "grad_norm": 1.3619742393493652,
      "learning_rate": 0.00016668017470394887,
      "loss": 3.7034,
      "step": 3700
    },
    {
      "epoch": 0.5065171878165733,
      "grad_norm": 1.425737977027893,
      "learning_rate": 0.00016622990679454274,
      "loss": 3.7375,
      "step": 3750
    },
    {
      "epoch": 0.5132707503207942,
      "grad_norm": 1.3887169361114502,
      "learning_rate": 0.00016577963888513665,
      "loss": 3.6894,
      "step": 3800
    },
    {
      "epoch": 0.5200243128250152,
      "grad_norm": 1.3541208505630493,
      "learning_rate": 0.00016532937097573055,
      "loss": 3.6659,
      "step": 3850
    },
    {
      "epoch": 0.5267778753292361,
      "grad_norm": 1.2554895877838135,
      "learning_rate": 0.00016487910306632448,
      "loss": 3.709,
      "step": 3900
    },
    {
      "epoch": 0.5335314378334571,
      "grad_norm": 1.4929972887039185,
      "learning_rate": 0.00016442883515691839,
      "loss": 3.686,
      "step": 3950
    },
    {
      "epoch": 0.5402850003376781,
      "grad_norm": 1.4398239850997925,
      "learning_rate": 0.0001639785672475123,
      "loss": 3.6977,
      "step": 4000
    },
    {
      "epoch": 0.5402850003376781,
      "eval_loss": 3.7073452472686768,
      "eval_runtime": 159.8807,
      "eval_samples_per_second": 46.247,
      "eval_steps_per_second": 5.786,
      "step": 4000
    },
    {
      "epoch": 0.5470385628418991,
      "grad_norm": 1.3161286115646362,
      "learning_rate": 0.0001635282993381062,
      "loss": 3.6512,
      "step": 4050
    },
    {
      "epoch": 0.5537921253461201,
      "grad_norm": 1.4418870210647583,
      "learning_rate": 0.0001630780314287001,
      "loss": 3.6923,
      "step": 4100
    },
    {
      "epoch": 0.5605456878503411,
      "grad_norm": 1.4249507188796997,
      "learning_rate": 0.00016262776351929397,
      "loss": 3.6516,
      "step": 4150
    },
    {
      "epoch": 0.5672992503545621,
      "grad_norm": 1.5296441316604614,
      "learning_rate": 0.00016217749560988788,
      "loss": 3.6847,
      "step": 4200
    },
    {
      "epoch": 0.574052812858783,
      "grad_norm": 1.6135449409484863,
      "learning_rate": 0.00016172722770048178,
      "loss": 3.6627,
      "step": 4250
    },
    {
      "epoch": 0.580806375363004,
      "grad_norm": 1.4726654291152954,
      "learning_rate": 0.00016127695979107568,
      "loss": 3.692,
      "step": 4300
    },
    {
      "epoch": 0.5875599378672249,
      "grad_norm": 1.2898463010787964,
      "learning_rate": 0.0001608266918816696,
      "loss": 3.668,
      "step": 4350
    },
    {
      "epoch": 0.5943135003714459,
      "grad_norm": 1.2962331771850586,
      "learning_rate": 0.00016037642397226352,
      "loss": 3.698,
      "step": 4400
    },
    {
      "epoch": 0.6010670628756669,
      "grad_norm": 1.3446184396743774,
      "learning_rate": 0.00015992615606285742,
      "loss": 3.6836,
      "step": 4450
    },
    {
      "epoch": 0.6078206253798879,
      "grad_norm": 1.277993083000183,
      "learning_rate": 0.00015947588815345132,
      "loss": 3.6699,
      "step": 4500
    },
    {
      "epoch": 0.6145741878841089,
      "grad_norm": 1.3769346475601196,
      "learning_rate": 0.00015902562024404523,
      "loss": 3.6867,
      "step": 4550
    },
    {
      "epoch": 0.6213277503883299,
      "grad_norm": 1.3811522722244263,
      "learning_rate": 0.0001585753523346391,
      "loss": 3.6302,
      "step": 4600
    },
    {
      "epoch": 0.6280813128925509,
      "grad_norm": 1.3310779333114624,
      "learning_rate": 0.000158125084425233,
      "loss": 3.6923,
      "step": 4650
    },
    {
      "epoch": 0.6348348753967717,
      "grad_norm": 1.2519299983978271,
      "learning_rate": 0.0001576748165158269,
      "loss": 3.6687,
      "step": 4700
    },
    {
      "epoch": 0.6415884379009927,
      "grad_norm": 1.3973385095596313,
      "learning_rate": 0.00015722454860642081,
      "loss": 3.6477,
      "step": 4750
    },
    {
      "epoch": 0.6483420004052137,
      "grad_norm": 1.449474811553955,
      "learning_rate": 0.00015677428069701474,
      "loss": 3.652,
      "step": 4800
    },
    {
      "epoch": 0.6550955629094347,
      "grad_norm": 1.3876686096191406,
      "learning_rate": 0.00015632401278760865,
      "loss": 3.6443,
      "step": 4850
    },
    {
      "epoch": 0.6618491254136557,
      "grad_norm": 1.314900279045105,
      "learning_rate": 0.00015587374487820255,
      "loss": 3.6551,
      "step": 4900
    },
    {
      "epoch": 0.6686026879178767,
      "grad_norm": 1.2933001518249512,
      "learning_rate": 0.00015542347696879646,
      "loss": 3.6468,
      "step": 4950
    },
    {
      "epoch": 0.6753562504220977,
      "grad_norm": 1.3287744522094727,
      "learning_rate": 0.00015497320905939033,
      "loss": 3.6716,
      "step": 5000
    },
    {
      "epoch": 0.6821098129263187,
      "grad_norm": 1.3460630178451538,
      "learning_rate": 0.00015452294114998424,
      "loss": 3.6599,
      "step": 5050
    },
    {
      "epoch": 0.6888633754305397,
      "grad_norm": 1.2755683660507202,
      "learning_rate": 0.00015407267324057814,
      "loss": 3.6412,
      "step": 5100
    },
    {
      "epoch": 0.6956169379347605,
      "grad_norm": 1.3904697895050049,
      "learning_rate": 0.00015362240533117204,
      "loss": 3.63,
      "step": 5150
    },
    {
      "epoch": 0.7023705004389815,
      "grad_norm": 1.2476608753204346,
      "learning_rate": 0.00015317213742176595,
      "loss": 3.6374,
      "step": 5200
    },
    {
      "epoch": 0.7091240629432025,
      "grad_norm": 1.2476223707199097,
      "learning_rate": 0.00015272186951235988,
      "loss": 3.6499,
      "step": 5250
    },
    {
      "epoch": 0.7158776254474235,
      "grad_norm": 1.3357038497924805,
      "learning_rate": 0.00015227160160295378,
      "loss": 3.6323,
      "step": 5300
    },
    {
      "epoch": 0.7226311879516445,
      "grad_norm": 1.5791229009628296,
      "learning_rate": 0.00015182133369354768,
      "loss": 3.6389,
      "step": 5350
    },
    {
      "epoch": 0.7293847504558655,
      "grad_norm": 1.6427063941955566,
      "learning_rate": 0.0001513710657841416,
      "loss": 3.6754,
      "step": 5400
    },
    {
      "epoch": 0.7361383129600865,
      "grad_norm": 1.3833472728729248,
      "learning_rate": 0.00015092079787473546,
      "loss": 3.6442,
      "step": 5450
    },
    {
      "epoch": 0.7428918754643075,
      "grad_norm": 1.4378949403762817,
      "learning_rate": 0.00015047052996532937,
      "loss": 3.666,
      "step": 5500
    },
    {
      "epoch": 0.7496454379685284,
      "grad_norm": 1.4182040691375732,
      "learning_rate": 0.00015002026205592327,
      "loss": 3.6259,
      "step": 5550
    },
    {
      "epoch": 0.7563990004727493,
      "grad_norm": 1.3786461353302002,
      "learning_rate": 0.00014956999414651717,
      "loss": 3.6317,
      "step": 5600
    },
    {
      "epoch": 0.7631525629769703,
      "grad_norm": 1.2590128183364868,
      "learning_rate": 0.00014911972623711108,
      "loss": 3.6525,
      "step": 5650
    },
    {
      "epoch": 0.7699061254811913,
      "grad_norm": 1.2519959211349487,
      "learning_rate": 0.000148669458327705,
      "loss": 3.6238,
      "step": 5700
    },
    {
      "epoch": 0.7766596879854123,
      "grad_norm": 1.2755470275878906,
      "learning_rate": 0.0001482191904182989,
      "loss": 3.634,
      "step": 5750
    },
    {
      "epoch": 0.7834132504896333,
      "grad_norm": 1.3027544021606445,
      "learning_rate": 0.00014776892250889281,
      "loss": 3.6199,
      "step": 5800
    },
    {
      "epoch": 0.7901668129938543,
      "grad_norm": 1.4003950357437134,
      "learning_rate": 0.00014731865459948672,
      "loss": 3.6258,
      "step": 5850
    },
    {
      "epoch": 0.7969203754980753,
      "grad_norm": 1.1184207201004028,
      "learning_rate": 0.0001468683866900806,
      "loss": 3.5848,
      "step": 5900
    },
    {
      "epoch": 0.8036739380022963,
      "grad_norm": 1.389952301979065,
      "learning_rate": 0.0001464181187806745,
      "loss": 3.6466,
      "step": 5950
    },
    {
      "epoch": 0.8104275005065172,
      "grad_norm": 2.5175700187683105,
      "learning_rate": 0.0001459678508712684,
      "loss": 3.6367,
      "step": 6000
    },
    {
      "epoch": 0.8104275005065172,
      "eval_loss": 3.6482694149017334,
      "eval_runtime": 159.6265,
      "eval_samples_per_second": 46.321,
      "eval_steps_per_second": 5.795,
      "step": 6000
    },
    {
      "epoch": 0.8171810630107381,
      "grad_norm": 1.2936817407608032,
      "learning_rate": 0.0001455175829618623,
      "loss": 3.6421,
      "step": 6050
    },
    {
      "epoch": 0.8239346255149591,
      "grad_norm": 1.4023462533950806,
      "learning_rate": 0.0001450673150524562,
      "loss": 3.6079,
      "step": 6100
    },
    {
      "epoch": 0.8306881880191801,
      "grad_norm": 1.351050615310669,
      "learning_rate": 0.00014461704714305014,
      "loss": 3.6205,
      "step": 6150
    },
    {
      "epoch": 0.8374417505234011,
      "grad_norm": 1.3294957876205444,
      "learning_rate": 0.00014416677923364404,
      "loss": 3.6307,
      "step": 6200
    },
    {
      "epoch": 0.8441953130276221,
      "grad_norm": 1.3081169128417969,
      "learning_rate": 0.00014371651132423795,
      "loss": 3.6109,
      "step": 6250
    },
    {
      "epoch": 0.8509488755318431,
      "grad_norm": 1.2370671033859253,
      "learning_rate": 0.00014326624341483182,
      "loss": 3.6036,
      "step": 6300
    },
    {
      "epoch": 0.857702438036064,
      "grad_norm": 1.233775019645691,
      "learning_rate": 0.00014281597550542573,
      "loss": 3.6636,
      "step": 6350
    },
    {
      "epoch": 0.864456000540285,
      "grad_norm": 1.3236181735992432,
      "learning_rate": 0.00014236570759601963,
      "loss": 3.5767,
      "step": 6400
    },
    {
      "epoch": 0.871209563044506,
      "grad_norm": 1.3009051084518433,
      "learning_rate": 0.00014191543968661353,
      "loss": 3.6069,
      "step": 6450
    },
    {
      "epoch": 0.8779631255487269,
      "grad_norm": 1.2888799905776978,
      "learning_rate": 0.00014146517177720744,
      "loss": 3.6249,
      "step": 6500
    },
    {
      "epoch": 0.8847166880529479,
      "grad_norm": 1.347568154335022,
      "learning_rate": 0.00014101490386780134,
      "loss": 3.551,
      "step": 6550
    },
    {
      "epoch": 0.8914702505571689,
      "grad_norm": 1.255020260810852,
      "learning_rate": 0.00014056463595839524,
      "loss": 3.6175,
      "step": 6600
    },
    {
      "epoch": 0.8982238130613899,
      "grad_norm": 1.283096194267273,
      "learning_rate": 0.00014011436804898917,
      "loss": 3.6277,
      "step": 6650
    },
    {
      "epoch": 0.9049773755656109,
      "grad_norm": 1.362763524055481,
      "learning_rate": 0.00013966410013958308,
      "loss": 3.6081,
      "step": 6700
    },
    {
      "epoch": 0.9117309380698319,
      "grad_norm": 1.3236017227172852,
      "learning_rate": 0.00013921383223017695,
      "loss": 3.6163,
      "step": 6750
    },
    {
      "epoch": 0.9184845005740528,
      "grad_norm": 1.2329596281051636,
      "learning_rate": 0.00013876356432077086,
      "loss": 3.6297,
      "step": 6800
    },
    {
      "epoch": 0.9252380630782738,
      "grad_norm": 1.2077208757400513,
      "learning_rate": 0.00013831329641136476,
      "loss": 3.6012,
      "step": 6850
    },
    {
      "epoch": 0.9319916255824948,
      "grad_norm": 1.3872253894805908,
      "learning_rate": 0.00013786302850195866,
      "loss": 3.5948,
      "step": 6900
    },
    {
      "epoch": 0.9387451880867157,
      "grad_norm": 1.3401249647140503,
      "learning_rate": 0.00013741276059255257,
      "loss": 3.5802,
      "step": 6950
    },
    {
      "epoch": 0.9454987505909367,
      "grad_norm": 1.2614299058914185,
      "learning_rate": 0.00013696249268314647,
      "loss": 3.5765,
      "step": 7000
    },
    {
      "epoch": 0.9522523130951577,
      "grad_norm": 1.1430917978286743,
      "learning_rate": 0.00013651222477374038,
      "loss": 3.5941,
      "step": 7050
    },
    {
      "epoch": 0.9590058755993787,
      "grad_norm": 1.3438173532485962,
      "learning_rate": 0.0001360619568643343,
      "loss": 3.5887,
      "step": 7100
    },
    {
      "epoch": 0.9657594381035997,
      "grad_norm": 1.3574961423873901,
      "learning_rate": 0.00013561168895492818,
      "loss": 3.5974,
      "step": 7150
    },
    {
      "epoch": 0.9725130006078206,
      "grad_norm": 1.157500982284546,
      "learning_rate": 0.00013516142104552209,
      "loss": 3.6338,
      "step": 7200
    },
    {
      "epoch": 0.9792665631120416,
      "grad_norm": 1.2824013233184814,
      "learning_rate": 0.000134711153136116,
      "loss": 3.5715,
      "step": 7250
    },
    {
      "epoch": 0.9860201256162626,
      "grad_norm": 1.3808598518371582,
      "learning_rate": 0.0001342608852267099,
      "loss": 3.5835,
      "step": 7300
    },
    {
      "epoch": 0.9927736881204836,
      "grad_norm": 1.3301664590835571,
      "learning_rate": 0.0001338106173173038,
      "loss": 3.6192,
      "step": 7350
    },
    {
      "epoch": 0.9995272506247045,
      "grad_norm": 1.1675714254379272,
      "learning_rate": 0.0001333603494078977,
      "loss": 3.6001,
      "step": 7400
    },
    {
      "epoch": 1.0062808131289256,
      "grad_norm": 1.2712936401367188,
      "learning_rate": 0.0001329100814984916,
      "loss": 3.4786,
      "step": 7450
    },
    {
      "epoch": 1.0130343756331466,
      "grad_norm": 2.006174087524414,
      "learning_rate": 0.0001324598135890855,
      "loss": 3.5186,
      "step": 7500
    },
    {
      "epoch": 1.0197879381373676,
      "grad_norm": 1.2320533990859985,
      "learning_rate": 0.00013200954567967944,
      "loss": 3.4896,
      "step": 7550
    },
    {
      "epoch": 1.0265415006415883,
      "grad_norm": 1.2705402374267578,
      "learning_rate": 0.0001315592777702733,
      "loss": 3.4501,
      "step": 7600
    },
    {
      "epoch": 1.0332950631458093,
      "grad_norm": 1.2035890817642212,
      "learning_rate": 0.00013110900986086722,
      "loss": 3.5034,
      "step": 7650
    },
    {
      "epoch": 1.0400486256500303,
      "grad_norm": 1.1925008296966553,
      "learning_rate": 0.00013065874195146112,
      "loss": 3.5065,
      "step": 7700
    },
    {
      "epoch": 1.0468021881542513,
      "grad_norm": 1.2856203317642212,
      "learning_rate": 0.00013020847404205502,
      "loss": 3.5063,
      "step": 7750
    },
    {
      "epoch": 1.0535557506584723,
      "grad_norm": 1.5457279682159424,
      "learning_rate": 0.00012975820613264893,
      "loss": 3.5215,
      "step": 7800
    },
    {
      "epoch": 1.0603093131626933,
      "grad_norm": 1.3602020740509033,
      "learning_rate": 0.00012930793822324283,
      "loss": 3.5005,
      "step": 7850
    },
    {
      "epoch": 1.0670628756669143,
      "grad_norm": 1.1513519287109375,
      "learning_rate": 0.00012885767031383673,
      "loss": 3.4626,
      "step": 7900
    },
    {
      "epoch": 1.0738164381711353,
      "grad_norm": 1.4063835144042969,
      "learning_rate": 0.00012840740240443064,
      "loss": 3.4991,
      "step": 7950
    },
    {
      "epoch": 1.0805700006753562,
      "grad_norm": 1.2238065004348755,
      "learning_rate": 0.00012795713449502457,
      "loss": 3.4878,
      "step": 8000
    },
    {
      "epoch": 1.0805700006753562,
      "eval_loss": 3.6134092807769775,
      "eval_runtime": 159.7996,
      "eval_samples_per_second": 46.27,
      "eval_steps_per_second": 5.789,
      "step": 8000
    },
    {
      "epoch": 1.0873235631795772,
      "grad_norm": 1.4145734310150146,
      "learning_rate": 0.00012750686658561844,
      "loss": 3.482,
      "step": 8050
    },
    {
      "epoch": 1.0940771256837982,
      "grad_norm": 1.153913140296936,
      "learning_rate": 0.00012705659867621235,
      "loss": 3.4395,
      "step": 8100
    },
    {
      "epoch": 1.1008306881880192,
      "grad_norm": 1.2333894968032837,
      "learning_rate": 0.00012660633076680625,
      "loss": 3.4742,
      "step": 8150
    },
    {
      "epoch": 1.1075842506922402,
      "grad_norm": 1.4572712182998657,
      "learning_rate": 0.00012615606285740016,
      "loss": 3.4714,
      "step": 8200
    },
    {
      "epoch": 1.1143378131964612,
      "grad_norm": 1.1956005096435547,
      "learning_rate": 0.00012570579494799406,
      "loss": 3.4945,
      "step": 8250
    },
    {
      "epoch": 1.1210913757006822,
      "grad_norm": 1.239667296409607,
      "learning_rate": 0.00012525552703858796,
      "loss": 3.4595,
      "step": 8300
    },
    {
      "epoch": 1.1278449382049032,
      "grad_norm": 1.3055598735809326,
      "learning_rate": 0.00012480525912918187,
      "loss": 3.4872,
      "step": 8350
    },
    {
      "epoch": 1.1345985007091242,
      "grad_norm": 1.3681546449661255,
      "learning_rate": 0.00012435499121977577,
      "loss": 3.4626,
      "step": 8400
    },
    {
      "epoch": 1.1413520632133451,
      "grad_norm": 1.2295156717300415,
      "learning_rate": 0.00012390472331036967,
      "loss": 3.4656,
      "step": 8450
    },
    {
      "epoch": 1.148105625717566,
      "grad_norm": 1.2994314432144165,
      "learning_rate": 0.00012345445540096358,
      "loss": 3.4863,
      "step": 8500
    },
    {
      "epoch": 1.154859188221787,
      "grad_norm": 1.392669677734375,
      "learning_rate": 0.00012300418749155748,
      "loss": 3.5076,
      "step": 8550
    },
    {
      "epoch": 1.161612750726008,
      "grad_norm": 1.1496925354003906,
      "learning_rate": 0.00012255391958215138,
      "loss": 3.5371,
      "step": 8600
    },
    {
      "epoch": 1.1683663132302289,
      "grad_norm": 1.201375961303711,
      "learning_rate": 0.0001221036516727453,
      "loss": 3.4775,
      "step": 8650
    },
    {
      "epoch": 1.1751198757344499,
      "grad_norm": 1.1933215856552124,
      "learning_rate": 0.00012165338376333919,
      "loss": 3.5014,
      "step": 8700
    },
    {
      "epoch": 1.1818734382386709,
      "grad_norm": 1.221785306930542,
      "learning_rate": 0.0001212031158539331,
      "loss": 3.4914,
      "step": 8750
    },
    {
      "epoch": 1.1886270007428918,
      "grad_norm": 1.2205344438552856,
      "learning_rate": 0.000120752847944527,
      "loss": 3.4702,
      "step": 8800
    },
    {
      "epoch": 1.1953805632471128,
      "grad_norm": 1.3494948148727417,
      "learning_rate": 0.00012030258003512089,
      "loss": 3.4642,
      "step": 8850
    },
    {
      "epoch": 1.2021341257513338,
      "grad_norm": 1.137389063835144,
      "learning_rate": 0.00011985231212571482,
      "loss": 3.4684,
      "step": 8900
    },
    {
      "epoch": 1.2088876882555548,
      "grad_norm": 1.1846920251846313,
      "learning_rate": 0.00011940204421630872,
      "loss": 3.4697,
      "step": 8950
    },
    {
      "epoch": 1.2156412507597758,
      "grad_norm": 1.2314590215682983,
      "learning_rate": 0.00011895177630690261,
      "loss": 3.4822,
      "step": 9000
    },
    {
      "epoch": 1.2223948132639968,
      "grad_norm": 1.1945892572402954,
      "learning_rate": 0.00011850150839749651,
      "loss": 3.4748,
      "step": 9050
    },
    {
      "epoch": 1.2291483757682178,
      "grad_norm": 1.2081423997879028,
      "learning_rate": 0.00011805124048809042,
      "loss": 3.4686,
      "step": 9100
    },
    {
      "epoch": 1.2359019382724388,
      "grad_norm": 1.3030065298080444,
      "learning_rate": 0.00011760097257868432,
      "loss": 3.4585,
      "step": 9150
    },
    {
      "epoch": 1.2426555007766598,
      "grad_norm": 1.1546599864959717,
      "learning_rate": 0.00011715070466927823,
      "loss": 3.479,
      "step": 9200
    },
    {
      "epoch": 1.2494090632808807,
      "grad_norm": 1.243806004524231,
      "learning_rate": 0.00011670043675987213,
      "loss": 3.4786,
      "step": 9250
    },
    {
      "epoch": 1.2561626257851017,
      "grad_norm": 1.402078628540039,
      "learning_rate": 0.00011625016885046602,
      "loss": 3.4934,
      "step": 9300
    },
    {
      "epoch": 1.2629161882893225,
      "grad_norm": 1.1788606643676758,
      "learning_rate": 0.00011579990094105995,
      "loss": 3.5034,
      "step": 9350
    },
    {
      "epoch": 1.2696697507935437,
      "grad_norm": 1.1940710544586182,
      "learning_rate": 0.00011534963303165385,
      "loss": 3.4882,
      "step": 9400
    },
    {
      "epoch": 1.2764233132977645,
      "grad_norm": 1.1765316724777222,
      "learning_rate": 0.00011489936512224774,
      "loss": 3.4994,
      "step": 9450
    },
    {
      "epoch": 1.2831768758019855,
      "grad_norm": 1.2376821041107178,
      "learning_rate": 0.00011444909721284165,
      "loss": 3.4866,
      "step": 9500
    },
    {
      "epoch": 1.2899304383062065,
      "grad_norm": 1.1515047550201416,
      "learning_rate": 0.00011399882930343555,
      "loss": 3.4946,
      "step": 9550
    },
    {
      "epoch": 1.2966840008104275,
      "grad_norm": 1.1407310962677002,
      "learning_rate": 0.00011354856139402945,
      "loss": 3.4688,
      "step": 9600
    },
    {
      "epoch": 1.3034375633146484,
      "grad_norm": 1.2244267463684082,
      "learning_rate": 0.00011309829348462336,
      "loss": 3.4822,
      "step": 9650
    },
    {
      "epoch": 1.3101911258188694,
      "grad_norm": 1.2450419664382935,
      "learning_rate": 0.00011264802557521725,
      "loss": 3.4457,
      "step": 9700
    },
    {
      "epoch": 1.3169446883230904,
      "grad_norm": 1.202599048614502,
      "learning_rate": 0.00011219775766581115,
      "loss": 3.4858,
      "step": 9750
    },
    {
      "epoch": 1.3236982508273114,
      "grad_norm": 1.2120839357376099,
      "learning_rate": 0.00011174748975640508,
      "loss": 3.427,
      "step": 9800
    },
    {
      "epoch": 1.3304518133315324,
      "grad_norm": 1.157388687133789,
      "learning_rate": 0.00011129722184699898,
      "loss": 3.5004,
      "step": 9850
    },
    {
      "epoch": 1.3372053758357534,
      "grad_norm": 1.2109462022781372,
      "learning_rate": 0.00011084695393759287,
      "loss": 3.4982,
      "step": 9900
    },
    {
      "epoch": 1.3439589383399744,
      "grad_norm": 2.1288375854492188,
      "learning_rate": 0.00011039668602818678,
      "loss": 3.4782,
      "step": 9950
    },
    {
      "epoch": 1.3507125008441954,
      "grad_norm": 1.1794404983520508,
      "learning_rate": 0.00010994641811878068,
      "loss": 3.4799,
      "step": 10000
    },
    {
      "epoch": 1.3507125008441954,
      "eval_loss": 3.5846760272979736,
      "eval_runtime": 159.6089,
      "eval_samples_per_second": 46.326,
      "eval_steps_per_second": 5.795,
      "step": 10000
    },
    {
      "epoch": 1.3574660633484164,
      "grad_norm": 1.1706565618515015,
      "learning_rate": 0.00010949615020937458,
      "loss": 3.4995,
      "step": 10050
    },
    {
      "epoch": 1.3642196258526373,
      "grad_norm": 1.1133564710617065,
      "learning_rate": 0.00010904588229996849,
      "loss": 3.4951,
      "step": 10100
    },
    {
      "epoch": 1.3709731883568583,
      "grad_norm": 1.1387799978256226,
      "learning_rate": 0.00010859561439056238,
      "loss": 3.4545,
      "step": 10150
    },
    {
      "epoch": 1.377726750861079,
      "grad_norm": 1.317315697669983,
      "learning_rate": 0.00010814534648115628,
      "loss": 3.4797,
      "step": 10200
    },
    {
      "epoch": 1.3844803133653003,
      "grad_norm": 1.1459002494812012,
      "learning_rate": 0.00010769507857175021,
      "loss": 3.4694,
      "step": 10250
    },
    {
      "epoch": 1.391233875869521,
      "grad_norm": 1.2264615297317505,
      "learning_rate": 0.0001072448106623441,
      "loss": 3.4871,
      "step": 10300
    },
    {
      "epoch": 1.3979874383737423,
      "grad_norm": 1.2030742168426514,
      "learning_rate": 0.000106794542752938,
      "loss": 3.5127,
      "step": 10350
    },
    {
      "epoch": 1.404741000877963,
      "grad_norm": 1.3182792663574219,
      "learning_rate": 0.00010634427484353191,
      "loss": 3.5013,
      "step": 10400
    },
    {
      "epoch": 1.411494563382184,
      "grad_norm": 1.1204378604888916,
      "learning_rate": 0.00010589400693412581,
      "loss": 3.4884,
      "step": 10450
    },
    {
      "epoch": 1.418248125886405,
      "grad_norm": 1.1954288482666016,
      "learning_rate": 0.00010544373902471972,
      "loss": 3.455,
      "step": 10500
    },
    {
      "epoch": 1.425001688390626,
      "grad_norm": 1.2595511674880981,
      "learning_rate": 0.00010499347111531362,
      "loss": 3.4976,
      "step": 10550
    },
    {
      "epoch": 1.431755250894847,
      "grad_norm": 1.1855500936508179,
      "learning_rate": 0.00010454320320590751,
      "loss": 3.4731,
      "step": 10600
    },
    {
      "epoch": 1.438508813399068,
      "grad_norm": 1.1050753593444824,
      "learning_rate": 0.00010409293529650141,
      "loss": 3.4568,
      "step": 10650
    },
    {
      "epoch": 1.445262375903289,
      "grad_norm": 1.0960774421691895,
      "learning_rate": 0.00010364266738709532,
      "loss": 3.4584,
      "step": 10700
    },
    {
      "epoch": 1.45201593840751,
      "grad_norm": 1.250262975692749,
      "learning_rate": 0.00010319239947768923,
      "loss": 3.4944,
      "step": 10750
    },
    {
      "epoch": 1.458769500911731,
      "grad_norm": 1.2991937398910522,
      "learning_rate": 0.00010274213156828314,
      "loss": 3.4932,
      "step": 10800
    },
    {
      "epoch": 1.465523063415952,
      "grad_norm": 1.1214463710784912,
      "learning_rate": 0.00010229186365887704,
      "loss": 3.5189,
      "step": 10850
    },
    {
      "epoch": 1.472276625920173,
      "grad_norm": 1.2685474157333374,
      "learning_rate": 0.00010184159574947094,
      "loss": 3.477,
      "step": 10900
    },
    {
      "epoch": 1.479030188424394,
      "grad_norm": 1.1288641691207886,
      "learning_rate": 0.00010139132784006485,
      "loss": 3.4682,
      "step": 10950
    },
    {
      "epoch": 1.485783750928615,
      "grad_norm": 1.1589785814285278,
      "learning_rate": 0.00010094105993065874,
      "loss": 3.5294,
      "step": 11000
    },
    {
      "epoch": 1.4925373134328357,
      "grad_norm": 1.0787014961242676,
      "learning_rate": 0.00010049079202125264,
      "loss": 3.4287,
      "step": 11050
    },
    {
      "epoch": 1.499290875937057,
      "grad_norm": 1.2823989391326904,
      "learning_rate": 0.00010004052411184654,
      "loss": 3.4734,
      "step": 11100
    },
    {
      "epoch": 1.5060444384412777,
      "grad_norm": 1.2663609981536865,
      "learning_rate": 9.959025620244046e-05,
      "loss": 3.4753,
      "step": 11150
    },
    {
      "epoch": 1.5127980009454989,
      "grad_norm": 1.1892611980438232,
      "learning_rate": 9.913998829303435e-05,
      "loss": 3.494,
      "step": 11200
    },
    {
      "epoch": 1.5195515634497196,
      "grad_norm": 1.1747790575027466,
      "learning_rate": 9.868972038362826e-05,
      "loss": 3.4535,
      "step": 11250
    },
    {
      "epoch": 1.5263051259539409,
      "grad_norm": 1.2565991878509521,
      "learning_rate": 9.823945247422217e-05,
      "loss": 3.4566,
      "step": 11300
    },
    {
      "epoch": 1.5330586884581616,
      "grad_norm": 1.1128450632095337,
      "learning_rate": 9.778918456481608e-05,
      "loss": 3.4973,
      "step": 11350
    },
    {
      "epoch": 1.5398122509623826,
      "grad_norm": 1.1874217987060547,
      "learning_rate": 9.733891665540998e-05,
      "loss": 3.4632,
      "step": 11400
    },
    {
      "epoch": 1.5465658134666036,
      "grad_norm": 1.0899460315704346,
      "learning_rate": 9.688864874600387e-05,
      "loss": 3.4911,
      "step": 11450
    },
    {
      "epoch": 1.5533193759708246,
      "grad_norm": 1.0808004140853882,
      "learning_rate": 9.643838083659777e-05,
      "loss": 3.4371,
      "step": 11500
    },
    {
      "epoch": 1.5600729384750456,
      "grad_norm": 1.225278615951538,
      "learning_rate": 9.598811292719169e-05,
      "loss": 3.4977,
      "step": 11550
    },
    {
      "epoch": 1.5668265009792666,
      "grad_norm": 1.2434499263763428,
      "learning_rate": 9.553784501778559e-05,
      "loss": 3.4898,
      "step": 11600
    },
    {
      "epoch": 1.5735800634834876,
      "grad_norm": 1.1132221221923828,
      "learning_rate": 9.508757710837948e-05,
      "loss": 3.4515,
      "step": 11650
    },
    {
      "epoch": 1.5803336259877085,
      "grad_norm": 1.2330069541931152,
      "learning_rate": 9.463730919897339e-05,
      "loss": 3.4644,
      "step": 11700
    },
    {
      "epoch": 1.5870871884919295,
      "grad_norm": 1.317441701889038,
      "learning_rate": 9.41870412895673e-05,
      "loss": 3.4415,
      "step": 11750
    },
    {
      "epoch": 1.5938407509961505,
      "grad_norm": 1.1351028680801392,
      "learning_rate": 9.373677338016121e-05,
      "loss": 3.4685,
      "step": 11800
    },
    {
      "epoch": 1.6005943135003715,
      "grad_norm": 1.281675934791565,
      "learning_rate": 9.32865054707551e-05,
      "loss": 3.4693,
      "step": 11850
    },
    {
      "epoch": 1.6073478760045923,
      "grad_norm": 1.083606243133545,
      "learning_rate": 9.2836237561349e-05,
      "loss": 3.4803,
      "step": 11900
    },
    {
      "epoch": 1.6141014385088135,
      "grad_norm": 1.3203532695770264,
      "learning_rate": 9.23859696519429e-05,
      "loss": 3.4599,
      "step": 11950
    },
    {
      "epoch": 1.6208550010130343,
      "grad_norm": 1.1326409578323364,
      "learning_rate": 9.193570174253682e-05,
      "loss": 3.4501,
      "step": 12000
    },
    {
      "epoch": 1.6208550010130343,
      "eval_loss": 3.558605432510376,
      "eval_runtime": 159.6605,
      "eval_samples_per_second": 46.311,
      "eval_steps_per_second": 5.794,
      "step": 12000
    },
    {
      "epoch": 1.6276085635172555,
      "grad_norm": 1.161934733390808,
      "learning_rate": 9.148543383313072e-05,
      "loss": 3.4397,
      "step": 12050
    },
    {
      "epoch": 1.6343621260214762,
      "grad_norm": 1.0895283222198486,
      "learning_rate": 9.103516592372461e-05,
      "loss": 3.4715,
      "step": 12100
    },
    {
      "epoch": 1.6411156885256974,
      "grad_norm": 1.2023546695709229,
      "learning_rate": 9.058489801431852e-05,
      "loss": 3.4768,
      "step": 12150
    },
    {
      "epoch": 1.6478692510299182,
      "grad_norm": 1.1618061065673828,
      "learning_rate": 9.013463010491243e-05,
      "loss": 3.45,
      "step": 12200
    },
    {
      "epoch": 1.6546228135341394,
      "grad_norm": 1.2494512796401978,
      "learning_rate": 8.968436219550634e-05,
      "loss": 3.4432,
      "step": 12250
    },
    {
      "epoch": 1.6613763760383602,
      "grad_norm": 1.2284793853759766,
      "learning_rate": 8.923409428610023e-05,
      "loss": 3.4668,
      "step": 12300
    },
    {
      "epoch": 1.6681299385425812,
      "grad_norm": 1.0671699047088623,
      "learning_rate": 8.878382637669413e-05,
      "loss": 3.453,
      "step": 12350
    },
    {
      "epoch": 1.6748835010468022,
      "grad_norm": 1.117477297782898,
      "learning_rate": 8.833355846728804e-05,
      "loss": 3.457,
      "step": 12400
    },
    {
      "epoch": 1.6816370635510232,
      "grad_norm": 1.257798671722412,
      "learning_rate": 8.788329055788195e-05,
      "loss": 3.4873,
      "step": 12450
    },
    {
      "epoch": 1.6883906260552441,
      "grad_norm": 1.3511102199554443,
      "learning_rate": 8.743302264847584e-05,
      "loss": 3.4738,
      "step": 12500
    },
    {
      "epoch": 1.6951441885594651,
      "grad_norm": 1.2377961874008179,
      "learning_rate": 8.698275473906975e-05,
      "loss": 3.4292,
      "step": 12550
    },
    {
      "epoch": 1.7018977510636861,
      "grad_norm": 1.163217306137085,
      "learning_rate": 8.653248682966365e-05,
      "loss": 3.4807,
      "step": 12600
    },
    {
      "epoch": 1.7086513135679071,
      "grad_norm": 1.3705207109451294,
      "learning_rate": 8.608221892025757e-05,
      "loss": 3.4754,
      "step": 12650
    },
    {
      "epoch": 1.715404876072128,
      "grad_norm": 1.1755452156066895,
      "learning_rate": 8.563195101085147e-05,
      "loss": 3.4566,
      "step": 12700
    },
    {
      "epoch": 1.7221584385763489,
      "grad_norm": 1.098554253578186,
      "learning_rate": 8.518168310144536e-05,
      "loss": 3.4784,
      "step": 12750
    },
    {
      "epoch": 1.72891200108057,
      "grad_norm": 1.145667314529419,
      "learning_rate": 8.473141519203926e-05,
      "loss": 3.4455,
      "step": 12800
    },
    {
      "epoch": 1.7356655635847908,
      "grad_norm": 1.0707271099090576,
      "learning_rate": 8.428114728263317e-05,
      "loss": 3.4337,
      "step": 12850
    },
    {
      "epoch": 1.742419126089012,
      "grad_norm": 1.1810872554779053,
      "learning_rate": 8.383087937322708e-05,
      "loss": 3.4508,
      "step": 12900
    },
    {
      "epoch": 1.7491726885932328,
      "grad_norm": 1.1205930709838867,
      "learning_rate": 8.338061146382097e-05,
      "loss": 3.48,
      "step": 12950
    },
    {
      "epoch": 1.755926251097454,
      "grad_norm": 1.2379084825515747,
      "learning_rate": 8.293034355441488e-05,
      "loss": 3.4495,
      "step": 13000
    },
    {
      "epoch": 1.7626798136016748,
      "grad_norm": 1.1263418197631836,
      "learning_rate": 8.248007564500878e-05,
      "loss": 3.4425,
      "step": 13050
    },
    {
      "epoch": 1.769433376105896,
      "grad_norm": 1.1231364011764526,
      "learning_rate": 8.20298077356027e-05,
      "loss": 3.4715,
      "step": 13100
    },
    {
      "epoch": 1.7761869386101168,
      "grad_norm": 1.1382341384887695,
      "learning_rate": 8.157953982619659e-05,
      "loss": 3.4312,
      "step": 13150
    },
    {
      "epoch": 1.7829405011143378,
      "grad_norm": 1.1381231546401978,
      "learning_rate": 8.112927191679049e-05,
      "loss": 3.4874,
      "step": 13200
    },
    {
      "epoch": 1.7896940636185588,
      "grad_norm": 1.2679252624511719,
      "learning_rate": 8.06790040073844e-05,
      "loss": 3.4268,
      "step": 13250
    },
    {
      "epoch": 1.7964476261227798,
      "grad_norm": 1.1175379753112793,
      "learning_rate": 8.02287360979783e-05,
      "loss": 3.4424,
      "step": 13300
    },
    {
      "epoch": 1.8032011886270007,
      "grad_norm": 1.208178162574768,
      "learning_rate": 7.97784681885722e-05,
      "loss": 3.4643,
      "step": 13350
    },
    {
      "epoch": 1.8099547511312217,
      "grad_norm": 1.238441824913025,
      "learning_rate": 7.93282002791661e-05,
      "loss": 3.4505,
      "step": 13400
    },
    {
      "epoch": 1.8167083136354427,
      "grad_norm": 1.1077882051467896,
      "learning_rate": 7.887793236976001e-05,
      "loss": 3.4399,
      "step": 13450
    },
    {
      "epoch": 1.8234618761396637,
      "grad_norm": 1.509636640548706,
      "learning_rate": 7.842766446035391e-05,
      "loss": 3.4439,
      "step": 13500
    },
    {
      "epoch": 1.8302154386438847,
      "grad_norm": 1.158588171005249,
      "learning_rate": 7.797739655094782e-05,
      "loss": 3.5042,
      "step": 13550
    },
    {
      "epoch": 1.8369690011481055,
      "grad_norm": 1.080651879310608,
      "learning_rate": 7.752712864154172e-05,
      "loss": 3.4448,
      "step": 13600
    },
    {
      "epoch": 1.8437225636523267,
      "grad_norm": 1.142825722694397,
      "learning_rate": 7.707686073213562e-05,
      "loss": 3.4051,
      "step": 13650
    },
    {
      "epoch": 1.8504761261565474,
      "grad_norm": 1.1771997213363647,
      "learning_rate": 7.662659282272953e-05,
      "loss": 3.4661,
      "step": 13700
    },
    {
      "epoch": 1.8572296886607687,
      "grad_norm": 1.1967084407806396,
      "learning_rate": 7.617632491332343e-05,
      "loss": 3.4676,
      "step": 13750
    },
    {
      "epoch": 1.8639832511649894,
      "grad_norm": 1.0434329509735107,
      "learning_rate": 7.572605700391733e-05,
      "loss": 3.4458,
      "step": 13800
    },
    {
      "epoch": 1.8707368136692106,
      "grad_norm": 1.2448322772979736,
      "learning_rate": 7.527578909451124e-05,
      "loss": 3.4462,
      "step": 13850
    },
    {
      "epoch": 1.8774903761734314,
      "grad_norm": 1.2166403532028198,
      "learning_rate": 7.482552118510514e-05,
      "loss": 3.4618,
      "step": 13900
    },
    {
      "epoch": 1.8842439386776526,
      "grad_norm": 1.0430411100387573,
      "learning_rate": 7.437525327569904e-05,
      "loss": 3.4226,
      "step": 13950
    },
    {
      "epoch": 1.8909975011818734,
      "grad_norm": 1.0447677373886108,
      "learning_rate": 7.392498536629295e-05,
      "loss": 3.4455,
      "step": 14000
    },
    {
      "epoch": 1.8909975011818734,
      "eval_loss": 3.5370380878448486,
      "eval_runtime": 159.5124,
      "eval_samples_per_second": 46.354,
      "eval_steps_per_second": 5.799,
      "step": 14000
    }
  ],
  "logging_steps": 50,
  "max_steps": 22209,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 2000,
  "total_flos": 1.52605607067648e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
