{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.953020134228188,
  "eval_steps": 2000,
  "global_step": 22000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006711409395973154,
      "grad_norm": 9.059213638305664,
      "learning_rate": 0.00019955257270693513,
      "loss": 6.3573,
      "step": 50
    },
    {
      "epoch": 0.013422818791946308,
      "grad_norm": 5.988269329071045,
      "learning_rate": 0.00019910514541387027,
      "loss": 5.2105,
      "step": 100
    },
    {
      "epoch": 0.020134228187919462,
      "grad_norm": 4.801666736602783,
      "learning_rate": 0.0001986577181208054,
      "loss": 5.0965,
      "step": 150
    },
    {
      "epoch": 0.026845637583892617,
      "grad_norm": 3.5457496643066406,
      "learning_rate": 0.00019821029082774049,
      "loss": 4.7337,
      "step": 200
    },
    {
      "epoch": 0.03355704697986577,
      "grad_norm": 3.8858330249786377,
      "learning_rate": 0.00019776286353467563,
      "loss": 4.5573,
      "step": 250
    },
    {
      "epoch": 0.040268456375838924,
      "grad_norm": 3.429072856903076,
      "learning_rate": 0.00019731543624161075,
      "loss": 4.4417,
      "step": 300
    },
    {
      "epoch": 0.04697986577181208,
      "grad_norm": 2.875025987625122,
      "learning_rate": 0.00019686800894854587,
      "loss": 4.4013,
      "step": 350
    },
    {
      "epoch": 0.053691275167785234,
      "grad_norm": 2.6337287425994873,
      "learning_rate": 0.000196420581655481,
      "loss": 4.3415,
      "step": 400
    },
    {
      "epoch": 0.06040268456375839,
      "grad_norm": 2.992663860321045,
      "learning_rate": 0.00019597315436241613,
      "loss": 4.2696,
      "step": 450
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 2.728472948074341,
      "learning_rate": 0.00019552572706935123,
      "loss": 4.2827,
      "step": 500
    },
    {
      "epoch": 0.0738255033557047,
      "grad_norm": 3.014280080795288,
      "learning_rate": 0.00019507829977628635,
      "loss": 4.2057,
      "step": 550
    },
    {
      "epoch": 0.08053691275167785,
      "grad_norm": 4.001026630401611,
      "learning_rate": 0.0001946308724832215,
      "loss": 4.195,
      "step": 600
    },
    {
      "epoch": 0.087248322147651,
      "grad_norm": 2.9807379245758057,
      "learning_rate": 0.0001941834451901566,
      "loss": 4.2058,
      "step": 650
    },
    {
      "epoch": 0.09395973154362416,
      "grad_norm": 2.7131142616271973,
      "learning_rate": 0.00019373601789709173,
      "loss": 4.1686,
      "step": 700
    },
    {
      "epoch": 0.10067114093959731,
      "grad_norm": 2.475486993789673,
      "learning_rate": 0.00019328859060402688,
      "loss": 4.1377,
      "step": 750
    },
    {
      "epoch": 0.10738255033557047,
      "grad_norm": 2.450024127960205,
      "learning_rate": 0.00019284116331096197,
      "loss": 4.1222,
      "step": 800
    },
    {
      "epoch": 0.11409395973154363,
      "grad_norm": 2.2996630668640137,
      "learning_rate": 0.0001923937360178971,
      "loss": 4.1225,
      "step": 850
    },
    {
      "epoch": 0.12080536912751678,
      "grad_norm": 2.391512155532837,
      "learning_rate": 0.0001919463087248322,
      "loss": 4.0997,
      "step": 900
    },
    {
      "epoch": 0.12751677852348994,
      "grad_norm": 2.205012559890747,
      "learning_rate": 0.00019149888143176735,
      "loss": 4.0829,
      "step": 950
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 3.9155759811401367,
      "learning_rate": 0.00019105145413870247,
      "loss": 4.0697,
      "step": 1000
    },
    {
      "epoch": 0.14093959731543623,
      "grad_norm": 2.9188551902770996,
      "learning_rate": 0.0001906040268456376,
      "loss": 4.0385,
      "step": 1050
    },
    {
      "epoch": 0.1476510067114094,
      "grad_norm": 2.3148672580718994,
      "learning_rate": 0.0001901565995525727,
      "loss": 4.043,
      "step": 1100
    },
    {
      "epoch": 0.15436241610738255,
      "grad_norm": 2.816882848739624,
      "learning_rate": 0.00018970917225950783,
      "loss": 4.044,
      "step": 1150
    },
    {
      "epoch": 0.1610738255033557,
      "grad_norm": 2.275336980819702,
      "learning_rate": 0.00018926174496644295,
      "loss": 4.0351,
      "step": 1200
    },
    {
      "epoch": 0.16778523489932887,
      "grad_norm": 2.4266958236694336,
      "learning_rate": 0.0001888143176733781,
      "loss": 4.0295,
      "step": 1250
    },
    {
      "epoch": 0.174496644295302,
      "grad_norm": 2.4356465339660645,
      "learning_rate": 0.0001883668903803132,
      "loss": 4.0086,
      "step": 1300
    },
    {
      "epoch": 0.18120805369127516,
      "grad_norm": 2.0859360694885254,
      "learning_rate": 0.00018791946308724833,
      "loss": 4.0404,
      "step": 1350
    },
    {
      "epoch": 0.18791946308724833,
      "grad_norm": 2.1723875999450684,
      "learning_rate": 0.00018747203579418348,
      "loss": 3.9726,
      "step": 1400
    },
    {
      "epoch": 0.19463087248322147,
      "grad_norm": 2.2362122535705566,
      "learning_rate": 0.00018702460850111857,
      "loss": 4.0197,
      "step": 1450
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 2.1546080112457275,
      "learning_rate": 0.0001865771812080537,
      "loss": 3.9411,
      "step": 1500
    },
    {
      "epoch": 0.2080536912751678,
      "grad_norm": 2.5627191066741943,
      "learning_rate": 0.0001861297539149888,
      "loss": 3.9767,
      "step": 1550
    },
    {
      "epoch": 0.21476510067114093,
      "grad_norm": 2.0063345432281494,
      "learning_rate": 0.00018568232662192395,
      "loss": 3.9828,
      "step": 1600
    },
    {
      "epoch": 0.2214765100671141,
      "grad_norm": 3.065760850906372,
      "learning_rate": 0.00018523489932885907,
      "loss": 4.0297,
      "step": 1650
    },
    {
      "epoch": 0.22818791946308725,
      "grad_norm": 2.3317503929138184,
      "learning_rate": 0.0001847874720357942,
      "loss": 3.9709,
      "step": 1700
    },
    {
      "epoch": 0.2348993288590604,
      "grad_norm": 2.0260672569274902,
      "learning_rate": 0.0001843400447427293,
      "loss": 3.9435,
      "step": 1750
    },
    {
      "epoch": 0.24161073825503357,
      "grad_norm": 1.9929717779159546,
      "learning_rate": 0.00018389261744966443,
      "loss": 3.9675,
      "step": 1800
    },
    {
      "epoch": 0.2483221476510067,
      "grad_norm": 2.0602314472198486,
      "learning_rate": 0.00018344519015659955,
      "loss": 3.9668,
      "step": 1850
    },
    {
      "epoch": 0.2550335570469799,
      "grad_norm": 2.0592637062072754,
      "learning_rate": 0.0001829977628635347,
      "loss": 3.9219,
      "step": 1900
    },
    {
      "epoch": 0.26174496644295303,
      "grad_norm": 2.263200044631958,
      "learning_rate": 0.00018255033557046981,
      "loss": 3.9804,
      "step": 1950
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 2.108201265335083,
      "learning_rate": 0.00018210290827740493,
      "loss": 3.923,
      "step": 2000
    },
    {
      "epoch": 0.2684563758389262,
      "eval_loss": 3.9559221267700195,
      "eval_runtime": 131.6222,
      "eval_samples_per_second": 56.176,
      "eval_steps_per_second": 7.028,
      "step": 2000
    },
    {
      "epoch": 0.2751677852348993,
      "grad_norm": 2.4163026809692383,
      "learning_rate": 0.00018165548098434005,
      "loss": 3.9577,
      "step": 2050
    },
    {
      "epoch": 0.28187919463087246,
      "grad_norm": 2.202943801879883,
      "learning_rate": 0.00018120805369127517,
      "loss": 3.8842,
      "step": 2100
    },
    {
      "epoch": 0.28859060402684567,
      "grad_norm": 2.005596160888672,
      "learning_rate": 0.0001807606263982103,
      "loss": 3.9275,
      "step": 2150
    },
    {
      "epoch": 0.2953020134228188,
      "grad_norm": 2.168701171875,
      "learning_rate": 0.0001803131991051454,
      "loss": 3.8578,
      "step": 2200
    },
    {
      "epoch": 0.30201342281879195,
      "grad_norm": 1.9575178623199463,
      "learning_rate": 0.00017986577181208056,
      "loss": 3.9049,
      "step": 2250
    },
    {
      "epoch": 0.3087248322147651,
      "grad_norm": 1.9856013059616089,
      "learning_rate": 0.00017941834451901567,
      "loss": 3.9714,
      "step": 2300
    },
    {
      "epoch": 0.31543624161073824,
      "grad_norm": 2.1658060550689697,
      "learning_rate": 0.0001789709172259508,
      "loss": 3.9149,
      "step": 2350
    },
    {
      "epoch": 0.3221476510067114,
      "grad_norm": 1.959775686264038,
      "learning_rate": 0.0001785234899328859,
      "loss": 3.9269,
      "step": 2400
    },
    {
      "epoch": 0.3288590604026846,
      "grad_norm": 2.233516216278076,
      "learning_rate": 0.00017807606263982103,
      "loss": 3.9701,
      "step": 2450
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 2.2147064208984375,
      "learning_rate": 0.00017762863534675615,
      "loss": 3.8816,
      "step": 2500
    },
    {
      "epoch": 0.3422818791946309,
      "grad_norm": 2.325010299682617,
      "learning_rate": 0.0001771812080536913,
      "loss": 3.8948,
      "step": 2550
    },
    {
      "epoch": 0.348993288590604,
      "grad_norm": 2.2941882610321045,
      "learning_rate": 0.00017673378076062642,
      "loss": 3.8695,
      "step": 2600
    },
    {
      "epoch": 0.35570469798657717,
      "grad_norm": 2.3443572521209717,
      "learning_rate": 0.00017628635346756153,
      "loss": 3.9096,
      "step": 2650
    },
    {
      "epoch": 0.3624161073825503,
      "grad_norm": 2.377565860748291,
      "learning_rate": 0.00017583892617449665,
      "loss": 3.8816,
      "step": 2700
    },
    {
      "epoch": 0.3691275167785235,
      "grad_norm": 1.975734829902649,
      "learning_rate": 0.00017539149888143177,
      "loss": 3.9405,
      "step": 2750
    },
    {
      "epoch": 0.37583892617449666,
      "grad_norm": 2.141709804534912,
      "learning_rate": 0.0001749440715883669,
      "loss": 3.9025,
      "step": 2800
    },
    {
      "epoch": 0.3825503355704698,
      "grad_norm": 2.083601236343384,
      "learning_rate": 0.000174496644295302,
      "loss": 3.8183,
      "step": 2850
    },
    {
      "epoch": 0.38926174496644295,
      "grad_norm": 1.875345230102539,
      "learning_rate": 0.00017404921700223716,
      "loss": 3.919,
      "step": 2900
    },
    {
      "epoch": 0.3959731543624161,
      "grad_norm": 1.8471341133117676,
      "learning_rate": 0.00017360178970917228,
      "loss": 3.8631,
      "step": 2950
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 2.0982210636138916,
      "learning_rate": 0.0001731543624161074,
      "loss": 3.8133,
      "step": 3000
    },
    {
      "epoch": 0.40939597315436244,
      "grad_norm": 1.8660926818847656,
      "learning_rate": 0.00017270693512304251,
      "loss": 3.8577,
      "step": 3050
    },
    {
      "epoch": 0.4161073825503356,
      "grad_norm": 1.8190667629241943,
      "learning_rate": 0.00017225950782997763,
      "loss": 3.8909,
      "step": 3100
    },
    {
      "epoch": 0.4228187919463087,
      "grad_norm": 2.0440821647644043,
      "learning_rate": 0.00017181208053691275,
      "loss": 3.8875,
      "step": 3150
    },
    {
      "epoch": 0.42953020134228187,
      "grad_norm": 1.8206124305725098,
      "learning_rate": 0.00017136465324384787,
      "loss": 3.8581,
      "step": 3200
    },
    {
      "epoch": 0.436241610738255,
      "grad_norm": 1.974567174911499,
      "learning_rate": 0.00017091722595078302,
      "loss": 3.8942,
      "step": 3250
    },
    {
      "epoch": 0.4429530201342282,
      "grad_norm": 2.0264852046966553,
      "learning_rate": 0.00017046979865771814,
      "loss": 3.8586,
      "step": 3300
    },
    {
      "epoch": 0.44966442953020136,
      "grad_norm": 2.0460824966430664,
      "learning_rate": 0.00017002237136465325,
      "loss": 3.8812,
      "step": 3350
    },
    {
      "epoch": 0.4563758389261745,
      "grad_norm": 1.8607724905014038,
      "learning_rate": 0.00016957494407158837,
      "loss": 3.8711,
      "step": 3400
    },
    {
      "epoch": 0.46308724832214765,
      "grad_norm": 1.915757417678833,
      "learning_rate": 0.0001691275167785235,
      "loss": 3.8682,
      "step": 3450
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 2.7037572860717773,
      "learning_rate": 0.0001686800894854586,
      "loss": 3.8264,
      "step": 3500
    },
    {
      "epoch": 0.47651006711409394,
      "grad_norm": 2.0121653079986572,
      "learning_rate": 0.00016823266219239376,
      "loss": 3.8844,
      "step": 3550
    },
    {
      "epoch": 0.48322147651006714,
      "grad_norm": 1.955132007598877,
      "learning_rate": 0.00016778523489932888,
      "loss": 3.8489,
      "step": 3600
    },
    {
      "epoch": 0.4899328859060403,
      "grad_norm": 1.8197304010391235,
      "learning_rate": 0.000167337807606264,
      "loss": 3.7978,
      "step": 3650
    },
    {
      "epoch": 0.4966442953020134,
      "grad_norm": 2.17284893989563,
      "learning_rate": 0.00016689038031319912,
      "loss": 3.8393,
      "step": 3700
    },
    {
      "epoch": 0.5033557046979866,
      "grad_norm": 2.723198652267456,
      "learning_rate": 0.00016644295302013423,
      "loss": 3.8472,
      "step": 3750
    },
    {
      "epoch": 0.5100671140939598,
      "grad_norm": 2.091277599334717,
      "learning_rate": 0.00016599552572706935,
      "loss": 3.8138,
      "step": 3800
    },
    {
      "epoch": 0.5167785234899329,
      "grad_norm": 1.8208773136138916,
      "learning_rate": 0.00016554809843400447,
      "loss": 3.7573,
      "step": 3850
    },
    {
      "epoch": 0.5234899328859061,
      "grad_norm": 2.1696789264678955,
      "learning_rate": 0.00016510067114093962,
      "loss": 3.8242,
      "step": 3900
    },
    {
      "epoch": 0.5302013422818792,
      "grad_norm": 1.960264801979065,
      "learning_rate": 0.00016465324384787474,
      "loss": 3.7943,
      "step": 3950
    },
    {
      "epoch": 0.5369127516778524,
      "grad_norm": 2.069913625717163,
      "learning_rate": 0.00016420581655480986,
      "loss": 3.8278,
      "step": 4000
    },
    {
      "epoch": 0.5369127516778524,
      "eval_loss": 3.8491365909576416,
      "eval_runtime": 131.5126,
      "eval_samples_per_second": 56.223,
      "eval_steps_per_second": 7.034,
      "step": 4000
    },
    {
      "epoch": 0.5436241610738255,
      "grad_norm": 1.9448214769363403,
      "learning_rate": 0.00016375838926174498,
      "loss": 3.88,
      "step": 4050
    },
    {
      "epoch": 0.5503355704697986,
      "grad_norm": 2.1366593837738037,
      "learning_rate": 0.0001633109619686801,
      "loss": 3.8232,
      "step": 4100
    },
    {
      "epoch": 0.5570469798657718,
      "grad_norm": 1.7186349630355835,
      "learning_rate": 0.0001628635346756152,
      "loss": 3.7908,
      "step": 4150
    },
    {
      "epoch": 0.5637583892617449,
      "grad_norm": 1.9107493162155151,
      "learning_rate": 0.00016241610738255036,
      "loss": 3.9096,
      "step": 4200
    },
    {
      "epoch": 0.5704697986577181,
      "grad_norm": 2.179384231567383,
      "learning_rate": 0.00016196868008948548,
      "loss": 3.8066,
      "step": 4250
    },
    {
      "epoch": 0.5771812080536913,
      "grad_norm": 2.8723456859588623,
      "learning_rate": 0.0001615212527964206,
      "loss": 3.8417,
      "step": 4300
    },
    {
      "epoch": 0.5838926174496645,
      "grad_norm": 1.7784136533737183,
      "learning_rate": 0.0001610738255033557,
      "loss": 3.7713,
      "step": 4350
    },
    {
      "epoch": 0.5906040268456376,
      "grad_norm": 1.9460691213607788,
      "learning_rate": 0.00016062639821029084,
      "loss": 3.8213,
      "step": 4400
    },
    {
      "epoch": 0.5973154362416108,
      "grad_norm": 2.22724986076355,
      "learning_rate": 0.00016017897091722595,
      "loss": 3.8166,
      "step": 4450
    },
    {
      "epoch": 0.6040268456375839,
      "grad_norm": 2.1138367652893066,
      "learning_rate": 0.00015973154362416107,
      "loss": 3.7841,
      "step": 4500
    },
    {
      "epoch": 0.610738255033557,
      "grad_norm": 1.6636978387832642,
      "learning_rate": 0.00015928411633109622,
      "loss": 3.7845,
      "step": 4550
    },
    {
      "epoch": 0.6174496644295302,
      "grad_norm": 2.06239652633667,
      "learning_rate": 0.00015883668903803134,
      "loss": 3.812,
      "step": 4600
    },
    {
      "epoch": 0.6241610738255033,
      "grad_norm": 2.006086587905884,
      "learning_rate": 0.00015838926174496643,
      "loss": 3.8553,
      "step": 4650
    },
    {
      "epoch": 0.6308724832214765,
      "grad_norm": 1.9434709548950195,
      "learning_rate": 0.00015794183445190158,
      "loss": 3.8273,
      "step": 4700
    },
    {
      "epoch": 0.6375838926174496,
      "grad_norm": 1.658636450767517,
      "learning_rate": 0.0001574944071588367,
      "loss": 3.8109,
      "step": 4750
    },
    {
      "epoch": 0.6442953020134228,
      "grad_norm": 1.875777006149292,
      "learning_rate": 0.00015704697986577181,
      "loss": 3.8461,
      "step": 4800
    },
    {
      "epoch": 0.6510067114093959,
      "grad_norm": 2.221557378768921,
      "learning_rate": 0.00015659955257270696,
      "loss": 3.8064,
      "step": 4850
    },
    {
      "epoch": 0.6577181208053692,
      "grad_norm": 1.993389368057251,
      "learning_rate": 0.00015615212527964208,
      "loss": 3.8227,
      "step": 4900
    },
    {
      "epoch": 0.6644295302013423,
      "grad_norm": 1.9651765823364258,
      "learning_rate": 0.0001557046979865772,
      "loss": 3.7822,
      "step": 4950
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 1.7692502737045288,
      "learning_rate": 0.0001552572706935123,
      "loss": 3.7972,
      "step": 5000
    },
    {
      "epoch": 0.6778523489932886,
      "grad_norm": 1.8885042667388916,
      "learning_rate": 0.00015480984340044744,
      "loss": 3.8297,
      "step": 5050
    },
    {
      "epoch": 0.6845637583892618,
      "grad_norm": 2.0650875568389893,
      "learning_rate": 0.00015436241610738256,
      "loss": 3.8381,
      "step": 5100
    },
    {
      "epoch": 0.6912751677852349,
      "grad_norm": 1.8892370462417603,
      "learning_rate": 0.00015391498881431768,
      "loss": 3.824,
      "step": 5150
    },
    {
      "epoch": 0.697986577181208,
      "grad_norm": 1.8695491552352905,
      "learning_rate": 0.00015346756152125282,
      "loss": 3.8045,
      "step": 5200
    },
    {
      "epoch": 0.7046979865771812,
      "grad_norm": 1.9334746599197388,
      "learning_rate": 0.00015302013422818794,
      "loss": 3.7938,
      "step": 5250
    },
    {
      "epoch": 0.7114093959731543,
      "grad_norm": 1.798484206199646,
      "learning_rate": 0.00015257270693512303,
      "loss": 3.8632,
      "step": 5300
    },
    {
      "epoch": 0.7181208053691275,
      "grad_norm": 1.9079642295837402,
      "learning_rate": 0.00015212527964205818,
      "loss": 3.7512,
      "step": 5350
    },
    {
      "epoch": 0.7248322147651006,
      "grad_norm": 1.8121227025985718,
      "learning_rate": 0.0001516778523489933,
      "loss": 3.8543,
      "step": 5400
    },
    {
      "epoch": 0.7315436241610739,
      "grad_norm": 1.8004286289215088,
      "learning_rate": 0.00015123042505592842,
      "loss": 3.7631,
      "step": 5450
    },
    {
      "epoch": 0.738255033557047,
      "grad_norm": 1.8558800220489502,
      "learning_rate": 0.00015078299776286354,
      "loss": 3.8427,
      "step": 5500
    },
    {
      "epoch": 0.7449664429530202,
      "grad_norm": 1.6879888772964478,
      "learning_rate": 0.00015033557046979868,
      "loss": 3.8226,
      "step": 5550
    },
    {
      "epoch": 0.7516778523489933,
      "grad_norm": 1.8560349941253662,
      "learning_rate": 0.00014988814317673377,
      "loss": 3.7818,
      "step": 5600
    },
    {
      "epoch": 0.7583892617449665,
      "grad_norm": 1.845901370048523,
      "learning_rate": 0.0001494407158836689,
      "loss": 3.7933,
      "step": 5650
    },
    {
      "epoch": 0.7651006711409396,
      "grad_norm": 1.9891059398651123,
      "learning_rate": 0.00014899328859060404,
      "loss": 3.7808,
      "step": 5700
    },
    {
      "epoch": 0.7718120805369127,
      "grad_norm": 1.779826045036316,
      "learning_rate": 0.00014854586129753916,
      "loss": 3.7826,
      "step": 5750
    },
    {
      "epoch": 0.7785234899328859,
      "grad_norm": 1.7945411205291748,
      "learning_rate": 0.00014809843400447428,
      "loss": 3.7838,
      "step": 5800
    },
    {
      "epoch": 0.785234899328859,
      "grad_norm": 1.5990691184997559,
      "learning_rate": 0.00014765100671140942,
      "loss": 3.7485,
      "step": 5850
    },
    {
      "epoch": 0.7919463087248322,
      "grad_norm": 1.8975350856781006,
      "learning_rate": 0.00014720357941834454,
      "loss": 3.8023,
      "step": 5900
    },
    {
      "epoch": 0.7986577181208053,
      "grad_norm": 1.6243772506713867,
      "learning_rate": 0.00014675615212527963,
      "loss": 3.8065,
      "step": 5950
    },
    {
      "epoch": 0.8053691275167785,
      "grad_norm": 1.8668122291564941,
      "learning_rate": 0.00014630872483221478,
      "loss": 3.8065,
      "step": 6000
    },
    {
      "epoch": 0.8053691275167785,
      "eval_loss": 3.7925124168395996,
      "eval_runtime": 131.3679,
      "eval_samples_per_second": 56.285,
      "eval_steps_per_second": 7.041,
      "step": 6000
    },
    {
      "epoch": 0.8120805369127517,
      "grad_norm": 1.8303035497665405,
      "learning_rate": 0.0001458612975391499,
      "loss": 3.7321,
      "step": 6050
    },
    {
      "epoch": 0.8187919463087249,
      "grad_norm": 1.788169026374817,
      "learning_rate": 0.00014541387024608502,
      "loss": 3.7943,
      "step": 6100
    },
    {
      "epoch": 0.825503355704698,
      "grad_norm": 1.8505574464797974,
      "learning_rate": 0.00014496644295302014,
      "loss": 3.7577,
      "step": 6150
    },
    {
      "epoch": 0.8322147651006712,
      "grad_norm": 1.6973223686218262,
      "learning_rate": 0.00014451901565995528,
      "loss": 3.7289,
      "step": 6200
    },
    {
      "epoch": 0.8389261744966443,
      "grad_norm": 1.7279934883117676,
      "learning_rate": 0.00014407158836689037,
      "loss": 3.7662,
      "step": 6250
    },
    {
      "epoch": 0.8456375838926175,
      "grad_norm": 1.8448787927627563,
      "learning_rate": 0.0001436241610738255,
      "loss": 3.765,
      "step": 6300
    },
    {
      "epoch": 0.8523489932885906,
      "grad_norm": 1.8236418962478638,
      "learning_rate": 0.00014317673378076064,
      "loss": 3.8006,
      "step": 6350
    },
    {
      "epoch": 0.8590604026845637,
      "grad_norm": 1.8819167613983154,
      "learning_rate": 0.00014272930648769576,
      "loss": 3.7665,
      "step": 6400
    },
    {
      "epoch": 0.8657718120805369,
      "grad_norm": 1.8498637676239014,
      "learning_rate": 0.00014228187919463088,
      "loss": 3.7119,
      "step": 6450
    },
    {
      "epoch": 0.87248322147651,
      "grad_norm": 1.6423312425613403,
      "learning_rate": 0.00014183445190156602,
      "loss": 3.7812,
      "step": 6500
    },
    {
      "epoch": 0.8791946308724832,
      "grad_norm": 1.7031521797180176,
      "learning_rate": 0.00014138702460850112,
      "loss": 3.8027,
      "step": 6550
    },
    {
      "epoch": 0.8859060402684564,
      "grad_norm": 1.8456487655639648,
      "learning_rate": 0.00014093959731543624,
      "loss": 3.7036,
      "step": 6600
    },
    {
      "epoch": 0.8926174496644296,
      "grad_norm": 1.6596404314041138,
      "learning_rate": 0.00014049217002237135,
      "loss": 3.7313,
      "step": 6650
    },
    {
      "epoch": 0.8993288590604027,
      "grad_norm": 1.7161309719085693,
      "learning_rate": 0.0001400447427293065,
      "loss": 3.7492,
      "step": 6700
    },
    {
      "epoch": 0.9060402684563759,
      "grad_norm": 1.931617259979248,
      "learning_rate": 0.00013959731543624162,
      "loss": 3.7319,
      "step": 6750
    },
    {
      "epoch": 0.912751677852349,
      "grad_norm": 1.8721851110458374,
      "learning_rate": 0.00013914988814317674,
      "loss": 3.7728,
      "step": 6800
    },
    {
      "epoch": 0.9194630872483222,
      "grad_norm": 1.9592097997665405,
      "learning_rate": 0.00013870246085011186,
      "loss": 3.7685,
      "step": 6850
    },
    {
      "epoch": 0.9261744966442953,
      "grad_norm": 1.6482694149017334,
      "learning_rate": 0.00013825503355704698,
      "loss": 3.7506,
      "step": 6900
    },
    {
      "epoch": 0.9328859060402684,
      "grad_norm": 1.8815938234329224,
      "learning_rate": 0.0001378076062639821,
      "loss": 3.7504,
      "step": 6950
    },
    {
      "epoch": 0.9395973154362416,
      "grad_norm": 1.7126567363739014,
      "learning_rate": 0.00013736017897091724,
      "loss": 3.7905,
      "step": 7000
    },
    {
      "epoch": 0.9463087248322147,
      "grad_norm": 1.8583459854125977,
      "learning_rate": 0.00013691275167785236,
      "loss": 3.7004,
      "step": 7050
    },
    {
      "epoch": 0.9530201342281879,
      "grad_norm": 1.670233130455017,
      "learning_rate": 0.00013646532438478748,
      "loss": 3.7442,
      "step": 7100
    },
    {
      "epoch": 0.959731543624161,
      "grad_norm": 1.9234365224838257,
      "learning_rate": 0.00013601789709172263,
      "loss": 3.7151,
      "step": 7150
    },
    {
      "epoch": 0.9664429530201343,
      "grad_norm": 1.597793459892273,
      "learning_rate": 0.00013557046979865772,
      "loss": 3.7546,
      "step": 7200
    },
    {
      "epoch": 0.9731543624161074,
      "grad_norm": 2.2746214866638184,
      "learning_rate": 0.00013512304250559284,
      "loss": 3.7753,
      "step": 7250
    },
    {
      "epoch": 0.9798657718120806,
      "grad_norm": 1.7023111581802368,
      "learning_rate": 0.00013467561521252796,
      "loss": 3.6945,
      "step": 7300
    },
    {
      "epoch": 0.9865771812080537,
      "grad_norm": 1.8443833589553833,
      "learning_rate": 0.0001342281879194631,
      "loss": 3.7294,
      "step": 7350
    },
    {
      "epoch": 0.9932885906040269,
      "grad_norm": 1.8114638328552246,
      "learning_rate": 0.00013378076062639822,
      "loss": 3.7431,
      "step": 7400
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.9123870134353638,
      "learning_rate": 0.00013333333333333334,
      "loss": 3.7477,
      "step": 7450
    },
    {
      "epoch": 1.0067114093959733,
      "grad_norm": 1.6625853776931763,
      "learning_rate": 0.00013288590604026846,
      "loss": 3.6101,
      "step": 7500
    },
    {
      "epoch": 1.0134228187919463,
      "grad_norm": 1.583512783050537,
      "learning_rate": 0.00013243847874720358,
      "loss": 3.6349,
      "step": 7550
    },
    {
      "epoch": 1.0201342281879195,
      "grad_norm": 1.602927803993225,
      "learning_rate": 0.0001319910514541387,
      "loss": 3.6302,
      "step": 7600
    },
    {
      "epoch": 1.0268456375838926,
      "grad_norm": 1.9316679239273071,
      "learning_rate": 0.00013154362416107384,
      "loss": 3.6441,
      "step": 7650
    },
    {
      "epoch": 1.0335570469798658,
      "grad_norm": 1.7358527183532715,
      "learning_rate": 0.00013109619686800896,
      "loss": 3.6081,
      "step": 7700
    },
    {
      "epoch": 1.0402684563758389,
      "grad_norm": 1.8509011268615723,
      "learning_rate": 0.00013064876957494408,
      "loss": 3.5952,
      "step": 7750
    },
    {
      "epoch": 1.0469798657718121,
      "grad_norm": 1.591277003288269,
      "learning_rate": 0.0001302013422818792,
      "loss": 3.6575,
      "step": 7800
    },
    {
      "epoch": 1.0536912751677852,
      "grad_norm": 1.7361763715744019,
      "learning_rate": 0.00012975391498881432,
      "loss": 3.6655,
      "step": 7850
    },
    {
      "epoch": 1.0604026845637584,
      "grad_norm": 1.6611849069595337,
      "learning_rate": 0.00012930648769574944,
      "loss": 3.658,
      "step": 7900
    },
    {
      "epoch": 1.0671140939597314,
      "grad_norm": 1.6635255813598633,
      "learning_rate": 0.00012885906040268456,
      "loss": 3.6465,
      "step": 7950
    },
    {
      "epoch": 1.0738255033557047,
      "grad_norm": 1.6869796514511108,
      "learning_rate": 0.0001284116331096197,
      "loss": 3.6512,
      "step": 8000
    },
    {
      "epoch": 1.0738255033557047,
      "eval_loss": 3.762192964553833,
      "eval_runtime": 131.4256,
      "eval_samples_per_second": 56.26,
      "eval_steps_per_second": 7.038,
      "step": 8000
    },
    {
      "epoch": 1.0805369127516777,
      "grad_norm": 1.6121760606765747,
      "learning_rate": 0.00012796420581655482,
      "loss": 3.6396,
      "step": 8050
    },
    {
      "epoch": 1.087248322147651,
      "grad_norm": 1.6784602403640747,
      "learning_rate": 0.00012751677852348994,
      "loss": 3.6121,
      "step": 8100
    },
    {
      "epoch": 1.0939597315436242,
      "grad_norm": 1.6049096584320068,
      "learning_rate": 0.00012706935123042506,
      "loss": 3.6255,
      "step": 8150
    },
    {
      "epoch": 1.1006711409395973,
      "grad_norm": 1.6055667400360107,
      "learning_rate": 0.00012662192393736018,
      "loss": 3.6027,
      "step": 8200
    },
    {
      "epoch": 1.1073825503355705,
      "grad_norm": 1.5239349603652954,
      "learning_rate": 0.0001261744966442953,
      "loss": 3.6234,
      "step": 8250
    },
    {
      "epoch": 1.1140939597315436,
      "grad_norm": 1.8718994855880737,
      "learning_rate": 0.00012572706935123044,
      "loss": 3.6929,
      "step": 8300
    },
    {
      "epoch": 1.1208053691275168,
      "grad_norm": 1.5848318338394165,
      "learning_rate": 0.00012527964205816556,
      "loss": 3.6618,
      "step": 8350
    },
    {
      "epoch": 1.1275167785234899,
      "grad_norm": 1.8713595867156982,
      "learning_rate": 0.00012483221476510068,
      "loss": 3.6194,
      "step": 8400
    },
    {
      "epoch": 1.1342281879194631,
      "grad_norm": 1.4972821474075317,
      "learning_rate": 0.0001243847874720358,
      "loss": 3.6262,
      "step": 8450
    },
    {
      "epoch": 1.1409395973154361,
      "grad_norm": 1.6696515083312988,
      "learning_rate": 0.00012393736017897092,
      "loss": 3.6965,
      "step": 8500
    },
    {
      "epoch": 1.1476510067114094,
      "grad_norm": 1.589736819267273,
      "learning_rate": 0.00012348993288590604,
      "loss": 3.6349,
      "step": 8550
    },
    {
      "epoch": 1.1543624161073827,
      "grad_norm": 1.7360472679138184,
      "learning_rate": 0.00012304250559284116,
      "loss": 3.626,
      "step": 8600
    },
    {
      "epoch": 1.1610738255033557,
      "grad_norm": 1.700801134109497,
      "learning_rate": 0.0001225950782997763,
      "loss": 3.6309,
      "step": 8650
    },
    {
      "epoch": 1.167785234899329,
      "grad_norm": 1.7908769845962524,
      "learning_rate": 0.00012214765100671142,
      "loss": 3.602,
      "step": 8700
    },
    {
      "epoch": 1.174496644295302,
      "grad_norm": 1.6651477813720703,
      "learning_rate": 0.00012170022371364653,
      "loss": 3.5969,
      "step": 8750
    },
    {
      "epoch": 1.1812080536912752,
      "grad_norm": 1.8064934015274048,
      "learning_rate": 0.00012125279642058168,
      "loss": 3.6119,
      "step": 8800
    },
    {
      "epoch": 1.1879194630872483,
      "grad_norm": 1.635682225227356,
      "learning_rate": 0.0001208053691275168,
      "loss": 3.6074,
      "step": 8850
    },
    {
      "epoch": 1.1946308724832215,
      "grad_norm": 1.5716607570648193,
      "learning_rate": 0.0001203579418344519,
      "loss": 3.5816,
      "step": 8900
    },
    {
      "epoch": 1.2013422818791946,
      "grad_norm": 1.6771187782287598,
      "learning_rate": 0.00011991051454138702,
      "loss": 3.6467,
      "step": 8950
    },
    {
      "epoch": 1.2080536912751678,
      "grad_norm": 1.6293506622314453,
      "learning_rate": 0.00011946308724832216,
      "loss": 3.6332,
      "step": 9000
    },
    {
      "epoch": 1.2147651006711409,
      "grad_norm": 1.7682856321334839,
      "learning_rate": 0.00011901565995525727,
      "loss": 3.603,
      "step": 9050
    },
    {
      "epoch": 1.221476510067114,
      "grad_norm": 1.8327367305755615,
      "learning_rate": 0.00011856823266219239,
      "loss": 3.617,
      "step": 9100
    },
    {
      "epoch": 1.2281879194630871,
      "grad_norm": 1.6715229749679565,
      "learning_rate": 0.00011812080536912754,
      "loss": 3.6311,
      "step": 9150
    },
    {
      "epoch": 1.2348993288590604,
      "grad_norm": 1.6335283517837524,
      "learning_rate": 0.00011767337807606264,
      "loss": 3.5934,
      "step": 9200
    },
    {
      "epoch": 1.2416107382550337,
      "grad_norm": 1.5648472309112549,
      "learning_rate": 0.00011722595078299776,
      "loss": 3.6058,
      "step": 9250
    },
    {
      "epoch": 1.2483221476510067,
      "grad_norm": 1.7293617725372314,
      "learning_rate": 0.0001167785234899329,
      "loss": 3.6418,
      "step": 9300
    },
    {
      "epoch": 1.25503355704698,
      "grad_norm": 1.7410277128219604,
      "learning_rate": 0.00011633109619686801,
      "loss": 3.6426,
      "step": 9350
    },
    {
      "epoch": 1.261744966442953,
      "grad_norm": 1.7763489484786987,
      "learning_rate": 0.00011588366890380313,
      "loss": 3.646,
      "step": 9400
    },
    {
      "epoch": 1.2684563758389262,
      "grad_norm": 1.8597512245178223,
      "learning_rate": 0.00011543624161073828,
      "loss": 3.6183,
      "step": 9450
    },
    {
      "epoch": 1.2751677852348993,
      "grad_norm": 1.824418544769287,
      "learning_rate": 0.00011498881431767338,
      "loss": 3.6337,
      "step": 9500
    },
    {
      "epoch": 1.2818791946308725,
      "grad_norm": 1.7031885385513306,
      "learning_rate": 0.0001145413870246085,
      "loss": 3.6074,
      "step": 9550
    },
    {
      "epoch": 1.2885906040268456,
      "grad_norm": 1.52085280418396,
      "learning_rate": 0.00011409395973154362,
      "loss": 3.611,
      "step": 9600
    },
    {
      "epoch": 1.2953020134228188,
      "grad_norm": 1.6925112009048462,
      "learning_rate": 0.00011364653243847875,
      "loss": 3.6207,
      "step": 9650
    },
    {
      "epoch": 1.302013422818792,
      "grad_norm": 1.6998307704925537,
      "learning_rate": 0.00011319910514541387,
      "loss": 3.5777,
      "step": 9700
    },
    {
      "epoch": 1.308724832214765,
      "grad_norm": 1.6910320520401,
      "learning_rate": 0.00011275167785234899,
      "loss": 3.6558,
      "step": 9750
    },
    {
      "epoch": 1.3154362416107381,
      "grad_norm": 1.7850961685180664,
      "learning_rate": 0.00011230425055928412,
      "loss": 3.5661,
      "step": 9800
    },
    {
      "epoch": 1.3221476510067114,
      "grad_norm": 1.6768145561218262,
      "learning_rate": 0.00011185682326621924,
      "loss": 3.6425,
      "step": 9850
    },
    {
      "epoch": 1.3288590604026846,
      "grad_norm": 1.6904698610305786,
      "learning_rate": 0.00011140939597315436,
      "loss": 3.6056,
      "step": 9900
    },
    {
      "epoch": 1.3355704697986577,
      "grad_norm": 1.8642719984054565,
      "learning_rate": 0.00011096196868008951,
      "loss": 3.5971,
      "step": 9950
    },
    {
      "epoch": 1.342281879194631,
      "grad_norm": 1.519580364227295,
      "learning_rate": 0.00011051454138702461,
      "loss": 3.5904,
      "step": 10000
    },
    {
      "epoch": 1.342281879194631,
      "eval_loss": 3.7398016452789307,
      "eval_runtime": 131.4267,
      "eval_samples_per_second": 56.26,
      "eval_steps_per_second": 7.038,
      "step": 10000
    },
    {
      "epoch": 1.348993288590604,
      "grad_norm": 1.7577157020568848,
      "learning_rate": 0.00011006711409395973,
      "loss": 3.6008,
      "step": 10050
    },
    {
      "epoch": 1.3557046979865772,
      "grad_norm": 1.6900218725204468,
      "learning_rate": 0.00010961968680089485,
      "loss": 3.6178,
      "step": 10100
    },
    {
      "epoch": 1.3624161073825503,
      "grad_norm": 1.7236902713775635,
      "learning_rate": 0.00010917225950782998,
      "loss": 3.6097,
      "step": 10150
    },
    {
      "epoch": 1.3691275167785235,
      "grad_norm": 1.5731052160263062,
      "learning_rate": 0.0001087248322147651,
      "loss": 3.5963,
      "step": 10200
    },
    {
      "epoch": 1.3758389261744965,
      "grad_norm": 1.5737526416778564,
      "learning_rate": 0.00010827740492170022,
      "loss": 3.6873,
      "step": 10250
    },
    {
      "epoch": 1.3825503355704698,
      "grad_norm": 1.8640624284744263,
      "learning_rate": 0.00010782997762863535,
      "loss": 3.6043,
      "step": 10300
    },
    {
      "epoch": 1.389261744966443,
      "grad_norm": 1.6538336277008057,
      "learning_rate": 0.00010738255033557047,
      "loss": 3.6589,
      "step": 10350
    },
    {
      "epoch": 1.395973154362416,
      "grad_norm": 1.719210147857666,
      "learning_rate": 0.00010693512304250559,
      "loss": 3.594,
      "step": 10400
    },
    {
      "epoch": 1.4026845637583891,
      "grad_norm": 1.6727123260498047,
      "learning_rate": 0.00010648769574944072,
      "loss": 3.5784,
      "step": 10450
    },
    {
      "epoch": 1.4093959731543624,
      "grad_norm": 1.699833631515503,
      "learning_rate": 0.00010604026845637584,
      "loss": 3.6332,
      "step": 10500
    },
    {
      "epoch": 1.4161073825503356,
      "grad_norm": 1.7181702852249146,
      "learning_rate": 0.00010559284116331096,
      "loss": 3.5988,
      "step": 10550
    },
    {
      "epoch": 1.4228187919463087,
      "grad_norm": 1.986975908279419,
      "learning_rate": 0.0001051454138702461,
      "loss": 3.5954,
      "step": 10600
    },
    {
      "epoch": 1.429530201342282,
      "grad_norm": 1.5294240713119507,
      "learning_rate": 0.00010469798657718121,
      "loss": 3.6023,
      "step": 10650
    },
    {
      "epoch": 1.436241610738255,
      "grad_norm": 1.703437089920044,
      "learning_rate": 0.00010425055928411633,
      "loss": 3.5715,
      "step": 10700
    },
    {
      "epoch": 1.4429530201342282,
      "grad_norm": 1.4820797443389893,
      "learning_rate": 0.00010380313199105145,
      "loss": 3.5311,
      "step": 10750
    },
    {
      "epoch": 1.4496644295302015,
      "grad_norm": 1.598300576210022,
      "learning_rate": 0.00010335570469798659,
      "loss": 3.5792,
      "step": 10800
    },
    {
      "epoch": 1.4563758389261745,
      "grad_norm": 1.6508630514144897,
      "learning_rate": 0.0001029082774049217,
      "loss": 3.6476,
      "step": 10850
    },
    {
      "epoch": 1.4630872483221475,
      "grad_norm": 1.753381609916687,
      "learning_rate": 0.00010246085011185682,
      "loss": 3.6668,
      "step": 10900
    },
    {
      "epoch": 1.4697986577181208,
      "grad_norm": 1.5645761489868164,
      "learning_rate": 0.00010201342281879196,
      "loss": 3.6468,
      "step": 10950
    },
    {
      "epoch": 1.476510067114094,
      "grad_norm": 1.6267133951187134,
      "learning_rate": 0.00010156599552572707,
      "loss": 3.5771,
      "step": 11000
    },
    {
      "epoch": 1.483221476510067,
      "grad_norm": 1.6823115348815918,
      "learning_rate": 0.0001011185682326622,
      "loss": 3.656,
      "step": 11050
    },
    {
      "epoch": 1.4899328859060403,
      "grad_norm": 1.648402452468872,
      "learning_rate": 0.00010067114093959733,
      "loss": 3.6198,
      "step": 11100
    },
    {
      "epoch": 1.4966442953020134,
      "grad_norm": 2.052081823348999,
      "learning_rate": 0.00010022371364653245,
      "loss": 3.6213,
      "step": 11150
    },
    {
      "epoch": 1.5033557046979866,
      "grad_norm": 1.899064302444458,
      "learning_rate": 9.977628635346756e-05,
      "loss": 3.6593,
      "step": 11200
    },
    {
      "epoch": 1.5100671140939599,
      "grad_norm": 1.4065371751785278,
      "learning_rate": 9.93288590604027e-05,
      "loss": 3.5954,
      "step": 11250
    },
    {
      "epoch": 1.516778523489933,
      "grad_norm": 1.7522718906402588,
      "learning_rate": 9.888143176733782e-05,
      "loss": 3.5868,
      "step": 11300
    },
    {
      "epoch": 1.523489932885906,
      "grad_norm": 1.6208070516586304,
      "learning_rate": 9.843400447427293e-05,
      "loss": 3.6009,
      "step": 11350
    },
    {
      "epoch": 1.5302013422818792,
      "grad_norm": 1.659182071685791,
      "learning_rate": 9.798657718120807e-05,
      "loss": 3.6035,
      "step": 11400
    },
    {
      "epoch": 1.5369127516778525,
      "grad_norm": 1.637748122215271,
      "learning_rate": 9.753914988814317e-05,
      "loss": 3.6355,
      "step": 11450
    },
    {
      "epoch": 1.5436241610738255,
      "grad_norm": 1.5991532802581787,
      "learning_rate": 9.70917225950783e-05,
      "loss": 3.6597,
      "step": 11500
    },
    {
      "epoch": 1.5503355704697985,
      "grad_norm": 1.5644680261611938,
      "learning_rate": 9.664429530201344e-05,
      "loss": 3.6478,
      "step": 11550
    },
    {
      "epoch": 1.5570469798657718,
      "grad_norm": 1.8149958848953247,
      "learning_rate": 9.619686800894854e-05,
      "loss": 3.5722,
      "step": 11600
    },
    {
      "epoch": 1.563758389261745,
      "grad_norm": 1.5871100425720215,
      "learning_rate": 9.574944071588368e-05,
      "loss": 3.5887,
      "step": 11650
    },
    {
      "epoch": 1.570469798657718,
      "grad_norm": 1.4967173337936401,
      "learning_rate": 9.53020134228188e-05,
      "loss": 3.6135,
      "step": 11700
    },
    {
      "epoch": 1.5771812080536913,
      "grad_norm": 1.5811350345611572,
      "learning_rate": 9.485458612975391e-05,
      "loss": 3.6271,
      "step": 11750
    },
    {
      "epoch": 1.5838926174496644,
      "grad_norm": 1.6718335151672363,
      "learning_rate": 9.440715883668905e-05,
      "loss": 3.6401,
      "step": 11800
    },
    {
      "epoch": 1.5906040268456376,
      "grad_norm": 1.6872515678405762,
      "learning_rate": 9.395973154362417e-05,
      "loss": 3.609,
      "step": 11850
    },
    {
      "epoch": 1.5973154362416109,
      "grad_norm": 1.6751142740249634,
      "learning_rate": 9.351230425055928e-05,
      "loss": 3.6152,
      "step": 11900
    },
    {
      "epoch": 1.604026845637584,
      "grad_norm": 1.854961633682251,
      "learning_rate": 9.30648769574944e-05,
      "loss": 3.5739,
      "step": 11950
    },
    {
      "epoch": 1.610738255033557,
      "grad_norm": 1.9909965991973877,
      "learning_rate": 9.261744966442954e-05,
      "loss": 3.5365,
      "step": 12000
    },
    {
      "epoch": 1.610738255033557,
      "eval_loss": 3.7152395248413086,
      "eval_runtime": 132.191,
      "eval_samples_per_second": 55.934,
      "eval_steps_per_second": 6.997,
      "step": 12000
    },
    {
      "epoch": 1.6174496644295302,
      "grad_norm": 1.5459636449813843,
      "learning_rate": 9.217002237136466e-05,
      "loss": 3.643,
      "step": 12050
    },
    {
      "epoch": 1.6241610738255035,
      "grad_norm": 1.4881346225738525,
      "learning_rate": 9.172259507829977e-05,
      "loss": 3.5728,
      "step": 12100
    },
    {
      "epoch": 1.6308724832214765,
      "grad_norm": 1.8114911317825317,
      "learning_rate": 9.127516778523491e-05,
      "loss": 3.5938,
      "step": 12150
    },
    {
      "epoch": 1.6375838926174495,
      "grad_norm": 1.5445913076400757,
      "learning_rate": 9.082774049217003e-05,
      "loss": 3.6346,
      "step": 12200
    },
    {
      "epoch": 1.6442953020134228,
      "grad_norm": 1.6415749788284302,
      "learning_rate": 9.038031319910515e-05,
      "loss": 3.5721,
      "step": 12250
    },
    {
      "epoch": 1.651006711409396,
      "grad_norm": 1.5994230508804321,
      "learning_rate": 8.993288590604028e-05,
      "loss": 3.6013,
      "step": 12300
    },
    {
      "epoch": 1.6577181208053693,
      "grad_norm": 1.5009089708328247,
      "learning_rate": 8.94854586129754e-05,
      "loss": 3.6383,
      "step": 12350
    },
    {
      "epoch": 1.6644295302013423,
      "grad_norm": 1.5820980072021484,
      "learning_rate": 8.903803131991052e-05,
      "loss": 3.6376,
      "step": 12400
    },
    {
      "epoch": 1.6711409395973154,
      "grad_norm": 1.5899134874343872,
      "learning_rate": 8.859060402684565e-05,
      "loss": 3.6187,
      "step": 12450
    },
    {
      "epoch": 1.6778523489932886,
      "grad_norm": 1.6859196424484253,
      "learning_rate": 8.814317673378077e-05,
      "loss": 3.6717,
      "step": 12500
    },
    {
      "epoch": 1.6845637583892619,
      "grad_norm": 1.563368797302246,
      "learning_rate": 8.769574944071589e-05,
      "loss": 3.5823,
      "step": 12550
    },
    {
      "epoch": 1.691275167785235,
      "grad_norm": 1.715071439743042,
      "learning_rate": 8.7248322147651e-05,
      "loss": 3.5865,
      "step": 12600
    },
    {
      "epoch": 1.697986577181208,
      "grad_norm": 1.770233392715454,
      "learning_rate": 8.680089485458614e-05,
      "loss": 3.5897,
      "step": 12650
    },
    {
      "epoch": 1.7046979865771812,
      "grad_norm": 1.5617735385894775,
      "learning_rate": 8.635346756152126e-05,
      "loss": 3.6189,
      "step": 12700
    },
    {
      "epoch": 1.7114093959731544,
      "grad_norm": 1.7810873985290527,
      "learning_rate": 8.590604026845638e-05,
      "loss": 3.6113,
      "step": 12750
    },
    {
      "epoch": 1.7181208053691275,
      "grad_norm": 1.505215048789978,
      "learning_rate": 8.545861297539151e-05,
      "loss": 3.56,
      "step": 12800
    },
    {
      "epoch": 1.7248322147651005,
      "grad_norm": 1.5554203987121582,
      "learning_rate": 8.501118568232663e-05,
      "loss": 3.5901,
      "step": 12850
    },
    {
      "epoch": 1.7315436241610738,
      "grad_norm": 1.7322720289230347,
      "learning_rate": 8.456375838926175e-05,
      "loss": 3.6356,
      "step": 12900
    },
    {
      "epoch": 1.738255033557047,
      "grad_norm": 1.8408923149108887,
      "learning_rate": 8.411633109619688e-05,
      "loss": 3.5913,
      "step": 12950
    },
    {
      "epoch": 1.7449664429530203,
      "grad_norm": 1.8254508972167969,
      "learning_rate": 8.3668903803132e-05,
      "loss": 3.6384,
      "step": 13000
    },
    {
      "epoch": 1.7516778523489933,
      "grad_norm": 1.6986461877822876,
      "learning_rate": 8.322147651006712e-05,
      "loss": 3.5849,
      "step": 13050
    },
    {
      "epoch": 1.7583892617449663,
      "grad_norm": 1.5796324014663696,
      "learning_rate": 8.277404921700224e-05,
      "loss": 3.5708,
      "step": 13100
    },
    {
      "epoch": 1.7651006711409396,
      "grad_norm": 1.6621372699737549,
      "learning_rate": 8.232662192393737e-05,
      "loss": 3.5802,
      "step": 13150
    },
    {
      "epoch": 1.7718120805369129,
      "grad_norm": 1.8478411436080933,
      "learning_rate": 8.187919463087249e-05,
      "loss": 3.5491,
      "step": 13200
    },
    {
      "epoch": 1.778523489932886,
      "grad_norm": 1.6501010656356812,
      "learning_rate": 8.14317673378076e-05,
      "loss": 3.5939,
      "step": 13250
    },
    {
      "epoch": 1.785234899328859,
      "grad_norm": 1.5774372816085815,
      "learning_rate": 8.098434004474274e-05,
      "loss": 3.626,
      "step": 13300
    },
    {
      "epoch": 1.7919463087248322,
      "grad_norm": 1.591950535774231,
      "learning_rate": 8.053691275167784e-05,
      "loss": 3.6489,
      "step": 13350
    },
    {
      "epoch": 1.7986577181208054,
      "grad_norm": 1.5690762996673584,
      "learning_rate": 8.008948545861298e-05,
      "loss": 3.6071,
      "step": 13400
    },
    {
      "epoch": 1.8053691275167785,
      "grad_norm": 1.6270502805709839,
      "learning_rate": 7.964205816554811e-05,
      "loss": 3.5921,
      "step": 13450
    },
    {
      "epoch": 1.8120805369127517,
      "grad_norm": 1.382842779159546,
      "learning_rate": 7.919463087248322e-05,
      "loss": 3.5857,
      "step": 13500
    },
    {
      "epoch": 1.8187919463087248,
      "grad_norm": 1.5864313840866089,
      "learning_rate": 7.874720357941835e-05,
      "loss": 3.6018,
      "step": 13550
    },
    {
      "epoch": 1.825503355704698,
      "grad_norm": 1.8909226655960083,
      "learning_rate": 7.829977628635348e-05,
      "loss": 3.5765,
      "step": 13600
    },
    {
      "epoch": 1.8322147651006713,
      "grad_norm": 1.5435892343521118,
      "learning_rate": 7.78523489932886e-05,
      "loss": 3.5903,
      "step": 13650
    },
    {
      "epoch": 1.8389261744966443,
      "grad_norm": 1.630387306213379,
      "learning_rate": 7.740492170022372e-05,
      "loss": 3.6136,
      "step": 13700
    },
    {
      "epoch": 1.8456375838926173,
      "grad_norm": 1.7216609716415405,
      "learning_rate": 7.695749440715884e-05,
      "loss": 3.6451,
      "step": 13750
    },
    {
      "epoch": 1.8523489932885906,
      "grad_norm": 1.634642481803894,
      "learning_rate": 7.651006711409397e-05,
      "loss": 3.5737,
      "step": 13800
    },
    {
      "epoch": 1.8590604026845639,
      "grad_norm": 1.6248191595077515,
      "learning_rate": 7.606263982102909e-05,
      "loss": 3.5816,
      "step": 13850
    },
    {
      "epoch": 1.8657718120805369,
      "grad_norm": 1.5351625680923462,
      "learning_rate": 7.561521252796421e-05,
      "loss": 3.5598,
      "step": 13900
    },
    {
      "epoch": 1.87248322147651,
      "grad_norm": 1.5752105712890625,
      "learning_rate": 7.516778523489934e-05,
      "loss": 3.5781,
      "step": 13950
    },
    {
      "epoch": 1.8791946308724832,
      "grad_norm": 1.5955381393432617,
      "learning_rate": 7.472035794183445e-05,
      "loss": 3.6,
      "step": 14000
    },
    {
      "epoch": 1.8791946308724832,
      "eval_loss": 3.693551778793335,
      "eval_runtime": 131.2614,
      "eval_samples_per_second": 56.33,
      "eval_steps_per_second": 7.047,
      "step": 14000
    },
    {
      "epoch": 1.8859060402684564,
      "grad_norm": 1.5690748691558838,
      "learning_rate": 7.427293064876958e-05,
      "loss": 3.5532,
      "step": 14050
    },
    {
      "epoch": 1.8926174496644297,
      "grad_norm": 1.6235268115997314,
      "learning_rate": 7.382550335570471e-05,
      "loss": 3.5895,
      "step": 14100
    },
    {
      "epoch": 1.8993288590604027,
      "grad_norm": 1.6818549633026123,
      "learning_rate": 7.337807606263982e-05,
      "loss": 3.5612,
      "step": 14150
    },
    {
      "epoch": 1.9060402684563758,
      "grad_norm": 1.663934350013733,
      "learning_rate": 7.293064876957495e-05,
      "loss": 3.5959,
      "step": 14200
    },
    {
      "epoch": 1.912751677852349,
      "grad_norm": 1.6539013385772705,
      "learning_rate": 7.248322147651007e-05,
      "loss": 3.5968,
      "step": 14250
    },
    {
      "epoch": 1.9194630872483223,
      "grad_norm": 1.6349482536315918,
      "learning_rate": 7.203579418344519e-05,
      "loss": 3.561,
      "step": 14300
    },
    {
      "epoch": 1.9261744966442953,
      "grad_norm": 1.5512967109680176,
      "learning_rate": 7.158836689038032e-05,
      "loss": 3.6122,
      "step": 14350
    },
    {
      "epoch": 1.9328859060402683,
      "grad_norm": 1.5733485221862793,
      "learning_rate": 7.114093959731544e-05,
      "loss": 3.5733,
      "step": 14400
    },
    {
      "epoch": 1.9395973154362416,
      "grad_norm": 1.6950063705444336,
      "learning_rate": 7.069351230425056e-05,
      "loss": 3.5835,
      "step": 14450
    },
    {
      "epoch": 1.9463087248322148,
      "grad_norm": 1.6364177465438843,
      "learning_rate": 7.024608501118568e-05,
      "loss": 3.5415,
      "step": 14500
    },
    {
      "epoch": 1.9530201342281879,
      "grad_norm": 1.5891371965408325,
      "learning_rate": 6.979865771812081e-05,
      "loss": 3.5413,
      "step": 14550
    },
    {
      "epoch": 1.959731543624161,
      "grad_norm": 1.542587161064148,
      "learning_rate": 6.935123042505593e-05,
      "loss": 3.5687,
      "step": 14600
    },
    {
      "epoch": 1.9664429530201342,
      "grad_norm": 1.6132466793060303,
      "learning_rate": 6.890380313199105e-05,
      "loss": 3.5451,
      "step": 14650
    },
    {
      "epoch": 1.9731543624161074,
      "grad_norm": 1.5847713947296143,
      "learning_rate": 6.845637583892618e-05,
      "loss": 3.6226,
      "step": 14700
    },
    {
      "epoch": 1.9798657718120807,
      "grad_norm": 1.6603374481201172,
      "learning_rate": 6.800894854586131e-05,
      "loss": 3.6195,
      "step": 14750
    },
    {
      "epoch": 1.9865771812080537,
      "grad_norm": 1.6746350526809692,
      "learning_rate": 6.756152125279642e-05,
      "loss": 3.6039,
      "step": 14800
    },
    {
      "epoch": 1.9932885906040267,
      "grad_norm": 1.6161046028137207,
      "learning_rate": 6.711409395973155e-05,
      "loss": 3.6258,
      "step": 14850
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.983564853668213,
      "learning_rate": 6.666666666666667e-05,
      "loss": 3.5626,
      "step": 14900
    },
    {
      "epoch": 2.0067114093959733,
      "grad_norm": 1.700010061264038,
      "learning_rate": 6.621923937360179e-05,
      "loss": 3.4687,
      "step": 14950
    },
    {
      "epoch": 2.0134228187919465,
      "grad_norm": 1.5118120908737183,
      "learning_rate": 6.577181208053692e-05,
      "loss": 3.4522,
      "step": 15000
    },
    {
      "epoch": 2.0201342281879193,
      "grad_norm": 1.6272305250167847,
      "learning_rate": 6.532438478747204e-05,
      "loss": 3.4345,
      "step": 15050
    },
    {
      "epoch": 2.0268456375838926,
      "grad_norm": 1.636059284210205,
      "learning_rate": 6.487695749440716e-05,
      "loss": 3.4457,
      "step": 15100
    },
    {
      "epoch": 2.033557046979866,
      "grad_norm": 1.5918855667114258,
      "learning_rate": 6.442953020134228e-05,
      "loss": 3.5283,
      "step": 15150
    },
    {
      "epoch": 2.040268456375839,
      "grad_norm": 3.1089508533477783,
      "learning_rate": 6.398210290827741e-05,
      "loss": 3.4382,
      "step": 15200
    },
    {
      "epoch": 2.046979865771812,
      "grad_norm": 1.5301421880722046,
      "learning_rate": 6.353467561521253e-05,
      "loss": 3.4701,
      "step": 15250
    },
    {
      "epoch": 2.053691275167785,
      "grad_norm": 1.5706636905670166,
      "learning_rate": 6.308724832214765e-05,
      "loss": 3.43,
      "step": 15300
    },
    {
      "epoch": 2.0604026845637584,
      "grad_norm": 1.5943714380264282,
      "learning_rate": 6.263982102908278e-05,
      "loss": 3.4217,
      "step": 15350
    },
    {
      "epoch": 2.0671140939597317,
      "grad_norm": 1.5279321670532227,
      "learning_rate": 6.21923937360179e-05,
      "loss": 3.494,
      "step": 15400
    },
    {
      "epoch": 2.073825503355705,
      "grad_norm": 1.5368901491165161,
      "learning_rate": 6.174496644295302e-05,
      "loss": 3.4754,
      "step": 15450
    },
    {
      "epoch": 2.0805369127516777,
      "grad_norm": 1.6210906505584717,
      "learning_rate": 6.129753914988815e-05,
      "loss": 3.5078,
      "step": 15500
    },
    {
      "epoch": 2.087248322147651,
      "grad_norm": 1.598082423210144,
      "learning_rate": 6.0850111856823265e-05,
      "loss": 3.5207,
      "step": 15550
    },
    {
      "epoch": 2.0939597315436242,
      "grad_norm": 1.5259453058242798,
      "learning_rate": 6.04026845637584e-05,
      "loss": 3.447,
      "step": 15600
    },
    {
      "epoch": 2.1006711409395975,
      "grad_norm": 1.5364136695861816,
      "learning_rate": 5.995525727069351e-05,
      "loss": 3.4719,
      "step": 15650
    },
    {
      "epoch": 2.1073825503355703,
      "grad_norm": 1.6183377504348755,
      "learning_rate": 5.9507829977628635e-05,
      "loss": 3.4324,
      "step": 15700
    },
    {
      "epoch": 2.1140939597315436,
      "grad_norm": 1.7548177242279053,
      "learning_rate": 5.906040268456377e-05,
      "loss": 3.4919,
      "step": 15750
    },
    {
      "epoch": 2.120805369127517,
      "grad_norm": 1.5408416986465454,
      "learning_rate": 5.861297539149888e-05,
      "loss": 3.5021,
      "step": 15800
    },
    {
      "epoch": 2.12751677852349,
      "grad_norm": 1.5358806848526,
      "learning_rate": 5.8165548098434006e-05,
      "loss": 3.4898,
      "step": 15850
    },
    {
      "epoch": 2.134228187919463,
      "grad_norm": 1.6727560758590698,
      "learning_rate": 5.771812080536914e-05,
      "loss": 3.4655,
      "step": 15900
    },
    {
      "epoch": 2.140939597315436,
      "grad_norm": 1.7171465158462524,
      "learning_rate": 5.727069351230425e-05,
      "loss": 3.4855,
      "step": 15950
    },
    {
      "epoch": 2.1476510067114094,
      "grad_norm": 1.5527547597885132,
      "learning_rate": 5.6823266219239376e-05,
      "loss": 3.4853,
      "step": 16000
    },
    {
      "epoch": 2.1476510067114094,
      "eval_loss": 3.688236951828003,
      "eval_runtime": 131.6847,
      "eval_samples_per_second": 56.149,
      "eval_steps_per_second": 7.024,
      "step": 16000
    },
    {
      "epoch": 2.1543624161073827,
      "grad_norm": 1.4944343566894531,
      "learning_rate": 5.6375838926174495e-05,
      "loss": 3.4452,
      "step": 16050
    },
    {
      "epoch": 2.1610738255033555,
      "grad_norm": 1.732407808303833,
      "learning_rate": 5.592841163310962e-05,
      "loss": 3.5025,
      "step": 16100
    },
    {
      "epoch": 2.1677852348993287,
      "grad_norm": 1.7929542064666748,
      "learning_rate": 5.5480984340044754e-05,
      "loss": 3.4796,
      "step": 16150
    },
    {
      "epoch": 2.174496644295302,
      "grad_norm": 1.8108255863189697,
      "learning_rate": 5.5033557046979866e-05,
      "loss": 3.5048,
      "step": 16200
    },
    {
      "epoch": 2.1812080536912752,
      "grad_norm": 1.5022813081741333,
      "learning_rate": 5.458612975391499e-05,
      "loss": 3.5085,
      "step": 16250
    },
    {
      "epoch": 2.1879194630872485,
      "grad_norm": 1.5279277563095093,
      "learning_rate": 5.413870246085011e-05,
      "loss": 3.456,
      "step": 16300
    },
    {
      "epoch": 2.1946308724832213,
      "grad_norm": 1.5237174034118652,
      "learning_rate": 5.3691275167785237e-05,
      "loss": 3.471,
      "step": 16350
    },
    {
      "epoch": 2.2013422818791946,
      "grad_norm": 1.479443073272705,
      "learning_rate": 5.324384787472036e-05,
      "loss": 3.4487,
      "step": 16400
    },
    {
      "epoch": 2.208053691275168,
      "grad_norm": 1.5857013463974,
      "learning_rate": 5.279642058165548e-05,
      "loss": 3.492,
      "step": 16450
    },
    {
      "epoch": 2.214765100671141,
      "grad_norm": 1.6844232082366943,
      "learning_rate": 5.234899328859061e-05,
      "loss": 3.4242,
      "step": 16500
    },
    {
      "epoch": 2.221476510067114,
      "grad_norm": 1.5908645391464233,
      "learning_rate": 5.1901565995525726e-05,
      "loss": 3.4762,
      "step": 16550
    },
    {
      "epoch": 2.228187919463087,
      "grad_norm": 1.6423405408859253,
      "learning_rate": 5.145413870246085e-05,
      "loss": 3.5081,
      "step": 16600
    },
    {
      "epoch": 2.2348993288590604,
      "grad_norm": 1.659384846687317,
      "learning_rate": 5.100671140939598e-05,
      "loss": 3.4553,
      "step": 16650
    },
    {
      "epoch": 2.2416107382550337,
      "grad_norm": 1.687003493309021,
      "learning_rate": 5.05592841163311e-05,
      "loss": 3.5097,
      "step": 16700
    },
    {
      "epoch": 2.248322147651007,
      "grad_norm": 1.5032856464385986,
      "learning_rate": 5.011185682326622e-05,
      "loss": 3.5335,
      "step": 16750
    },
    {
      "epoch": 2.2550335570469797,
      "grad_norm": 1.5321348905563354,
      "learning_rate": 4.966442953020135e-05,
      "loss": 3.5075,
      "step": 16800
    },
    {
      "epoch": 2.261744966442953,
      "grad_norm": 1.482308030128479,
      "learning_rate": 4.921700223713647e-05,
      "loss": 3.4468,
      "step": 16850
    },
    {
      "epoch": 2.2684563758389262,
      "grad_norm": 1.5854287147521973,
      "learning_rate": 4.8769574944071586e-05,
      "loss": 3.4775,
      "step": 16900
    },
    {
      "epoch": 2.2751677852348995,
      "grad_norm": 1.5161702632904053,
      "learning_rate": 4.832214765100672e-05,
      "loss": 3.4527,
      "step": 16950
    },
    {
      "epoch": 2.2818791946308723,
      "grad_norm": 1.4749826192855835,
      "learning_rate": 4.787472035794184e-05,
      "loss": 3.4733,
      "step": 17000
    },
    {
      "epoch": 2.2885906040268456,
      "grad_norm": 1.5748398303985596,
      "learning_rate": 4.742729306487696e-05,
      "loss": 3.4581,
      "step": 17050
    },
    {
      "epoch": 2.295302013422819,
      "grad_norm": 1.7699251174926758,
      "learning_rate": 4.697986577181208e-05,
      "loss": 3.4646,
      "step": 17100
    },
    {
      "epoch": 2.302013422818792,
      "grad_norm": 1.4705076217651367,
      "learning_rate": 4.65324384787472e-05,
      "loss": 3.4499,
      "step": 17150
    },
    {
      "epoch": 2.3087248322147653,
      "grad_norm": 1.5515395402908325,
      "learning_rate": 4.608501118568233e-05,
      "loss": 3.5401,
      "step": 17200
    },
    {
      "epoch": 2.315436241610738,
      "grad_norm": 1.5180931091308594,
      "learning_rate": 4.5637583892617453e-05,
      "loss": 3.4621,
      "step": 17250
    },
    {
      "epoch": 2.3221476510067114,
      "grad_norm": 1.5818499326705933,
      "learning_rate": 4.519015659955257e-05,
      "loss": 3.4854,
      "step": 17300
    },
    {
      "epoch": 2.3288590604026846,
      "grad_norm": 1.6693962812423706,
      "learning_rate": 4.47427293064877e-05,
      "loss": 3.4992,
      "step": 17350
    },
    {
      "epoch": 2.335570469798658,
      "grad_norm": 1.4875576496124268,
      "learning_rate": 4.4295302013422824e-05,
      "loss": 3.5164,
      "step": 17400
    },
    {
      "epoch": 2.3422818791946307,
      "grad_norm": 1.4905768632888794,
      "learning_rate": 4.384787472035794e-05,
      "loss": 3.4516,
      "step": 17450
    },
    {
      "epoch": 2.348993288590604,
      "grad_norm": 1.6626787185668945,
      "learning_rate": 4.340044742729307e-05,
      "loss": 3.4491,
      "step": 17500
    },
    {
      "epoch": 2.3557046979865772,
      "grad_norm": 1.486130714416504,
      "learning_rate": 4.295302013422819e-05,
      "loss": 3.4647,
      "step": 17550
    },
    {
      "epoch": 2.3624161073825505,
      "grad_norm": 1.578402042388916,
      "learning_rate": 4.2505592841163314e-05,
      "loss": 3.5658,
      "step": 17600
    },
    {
      "epoch": 2.3691275167785237,
      "grad_norm": 1.5254724025726318,
      "learning_rate": 4.205816554809844e-05,
      "loss": 3.4941,
      "step": 17650
    },
    {
      "epoch": 2.3758389261744965,
      "grad_norm": 1.575823426246643,
      "learning_rate": 4.161073825503356e-05,
      "loss": 3.4406,
      "step": 17700
    },
    {
      "epoch": 2.38255033557047,
      "grad_norm": 1.553980827331543,
      "learning_rate": 4.1163310961968684e-05,
      "loss": 3.4815,
      "step": 17750
    },
    {
      "epoch": 2.389261744966443,
      "grad_norm": 1.4604966640472412,
      "learning_rate": 4.07158836689038e-05,
      "loss": 3.4394,
      "step": 17800
    },
    {
      "epoch": 2.395973154362416,
      "grad_norm": 1.5054965019226074,
      "learning_rate": 4.026845637583892e-05,
      "loss": 3.4629,
      "step": 17850
    },
    {
      "epoch": 2.402684563758389,
      "grad_norm": 1.69291353225708,
      "learning_rate": 3.9821029082774055e-05,
      "loss": 3.4733,
      "step": 17900
    },
    {
      "epoch": 2.4093959731543624,
      "grad_norm": 1.5518121719360352,
      "learning_rate": 3.9373601789709174e-05,
      "loss": 3.4354,
      "step": 17950
    },
    {
      "epoch": 2.4161073825503356,
      "grad_norm": 1.5091224908828735,
      "learning_rate": 3.89261744966443e-05,
      "loss": 3.4679,
      "step": 18000
    },
    {
      "epoch": 2.4161073825503356,
      "eval_loss": 3.677961826324463,
      "eval_runtime": 131.9056,
      "eval_samples_per_second": 56.055,
      "eval_steps_per_second": 7.013,
      "step": 18000
    },
    {
      "epoch": 2.422818791946309,
      "grad_norm": 1.6512433290481567,
      "learning_rate": 3.847874720357942e-05,
      "loss": 3.4954,
      "step": 18050
    },
    {
      "epoch": 2.4295302013422817,
      "grad_norm": 1.561806321144104,
      "learning_rate": 3.8031319910514545e-05,
      "loss": 3.4185,
      "step": 18100
    },
    {
      "epoch": 2.436241610738255,
      "grad_norm": 1.4441784620285034,
      "learning_rate": 3.758389261744967e-05,
      "loss": 3.4286,
      "step": 18150
    },
    {
      "epoch": 2.442953020134228,
      "grad_norm": 1.5762578248977661,
      "learning_rate": 3.713646532438479e-05,
      "loss": 3.4717,
      "step": 18200
    },
    {
      "epoch": 2.4496644295302015,
      "grad_norm": 1.621727705001831,
      "learning_rate": 3.668903803131991e-05,
      "loss": 3.4612,
      "step": 18250
    },
    {
      "epoch": 2.4563758389261743,
      "grad_norm": 1.577537178993225,
      "learning_rate": 3.6241610738255034e-05,
      "loss": 3.4991,
      "step": 18300
    },
    {
      "epoch": 2.4630872483221475,
      "grad_norm": 1.6579889059066772,
      "learning_rate": 3.579418344519016e-05,
      "loss": 3.4584,
      "step": 18350
    },
    {
      "epoch": 2.469798657718121,
      "grad_norm": 1.6065407991409302,
      "learning_rate": 3.534675615212528e-05,
      "loss": 3.4871,
      "step": 18400
    },
    {
      "epoch": 2.476510067114094,
      "grad_norm": 1.4890015125274658,
      "learning_rate": 3.4899328859060405e-05,
      "loss": 3.4647,
      "step": 18450
    },
    {
      "epoch": 2.4832214765100673,
      "grad_norm": 1.4778727293014526,
      "learning_rate": 3.4451901565995524e-05,
      "loss": 3.5011,
      "step": 18500
    },
    {
      "epoch": 2.48993288590604,
      "grad_norm": 1.571297287940979,
      "learning_rate": 3.4004474272930656e-05,
      "loss": 3.4967,
      "step": 18550
    },
    {
      "epoch": 2.4966442953020134,
      "grad_norm": 1.4486345052719116,
      "learning_rate": 3.3557046979865775e-05,
      "loss": 3.4682,
      "step": 18600
    },
    {
      "epoch": 2.5033557046979866,
      "grad_norm": 1.4328080415725708,
      "learning_rate": 3.3109619686800894e-05,
      "loss": 3.4826,
      "step": 18650
    },
    {
      "epoch": 2.51006711409396,
      "grad_norm": 1.4827697277069092,
      "learning_rate": 3.266219239373602e-05,
      "loss": 3.4883,
      "step": 18700
    },
    {
      "epoch": 2.5167785234899327,
      "grad_norm": 1.4568219184875488,
      "learning_rate": 3.221476510067114e-05,
      "loss": 3.4875,
      "step": 18750
    },
    {
      "epoch": 2.523489932885906,
      "grad_norm": 1.5134809017181396,
      "learning_rate": 3.1767337807606265e-05,
      "loss": 3.4789,
      "step": 18800
    },
    {
      "epoch": 2.530201342281879,
      "grad_norm": 1.571007251739502,
      "learning_rate": 3.131991051454139e-05,
      "loss": 3.4473,
      "step": 18850
    },
    {
      "epoch": 2.5369127516778525,
      "grad_norm": 1.6070690155029297,
      "learning_rate": 3.087248322147651e-05,
      "loss": 3.4812,
      "step": 18900
    },
    {
      "epoch": 2.5436241610738257,
      "grad_norm": 1.5830878019332886,
      "learning_rate": 3.0425055928411632e-05,
      "loss": 3.446,
      "step": 18950
    },
    {
      "epoch": 2.5503355704697985,
      "grad_norm": 1.4645942449569702,
      "learning_rate": 2.9977628635346755e-05,
      "loss": 3.4903,
      "step": 19000
    },
    {
      "epoch": 2.557046979865772,
      "grad_norm": 1.4938678741455078,
      "learning_rate": 2.9530201342281884e-05,
      "loss": 3.4283,
      "step": 19050
    },
    {
      "epoch": 2.563758389261745,
      "grad_norm": 1.4417558908462524,
      "learning_rate": 2.9082774049217003e-05,
      "loss": 3.4218,
      "step": 19100
    },
    {
      "epoch": 2.570469798657718,
      "grad_norm": 1.5813027620315552,
      "learning_rate": 2.8635346756152125e-05,
      "loss": 3.5108,
      "step": 19150
    },
    {
      "epoch": 2.577181208053691,
      "grad_norm": 1.4702732563018799,
      "learning_rate": 2.8187919463087248e-05,
      "loss": 3.4734,
      "step": 19200
    },
    {
      "epoch": 2.5838926174496644,
      "grad_norm": 1.5251154899597168,
      "learning_rate": 2.7740492170022377e-05,
      "loss": 3.483,
      "step": 19250
    },
    {
      "epoch": 2.5906040268456376,
      "grad_norm": 1.44096040725708,
      "learning_rate": 2.7293064876957496e-05,
      "loss": 3.4877,
      "step": 19300
    },
    {
      "epoch": 2.597315436241611,
      "grad_norm": 1.5353776216506958,
      "learning_rate": 2.6845637583892618e-05,
      "loss": 3.4496,
      "step": 19350
    },
    {
      "epoch": 2.604026845637584,
      "grad_norm": 1.5403281450271606,
      "learning_rate": 2.639821029082774e-05,
      "loss": 3.5021,
      "step": 19400
    },
    {
      "epoch": 2.610738255033557,
      "grad_norm": 1.624643325805664,
      "learning_rate": 2.5950782997762863e-05,
      "loss": 3.4512,
      "step": 19450
    },
    {
      "epoch": 2.61744966442953,
      "grad_norm": 1.4706156253814697,
      "learning_rate": 2.550335570469799e-05,
      "loss": 3.474,
      "step": 19500
    },
    {
      "epoch": 2.6241610738255035,
      "grad_norm": 1.4738643169403076,
      "learning_rate": 2.505592841163311e-05,
      "loss": 3.4325,
      "step": 19550
    },
    {
      "epoch": 2.6308724832214763,
      "grad_norm": 1.597188949584961,
      "learning_rate": 2.4608501118568234e-05,
      "loss": 3.466,
      "step": 19600
    },
    {
      "epoch": 2.6375838926174495,
      "grad_norm": 1.70299232006073,
      "learning_rate": 2.416107382550336e-05,
      "loss": 3.5028,
      "step": 19650
    },
    {
      "epoch": 2.6442953020134228,
      "grad_norm": 1.4929473400115967,
      "learning_rate": 2.371364653243848e-05,
      "loss": 3.4732,
      "step": 19700
    },
    {
      "epoch": 2.651006711409396,
      "grad_norm": 1.4166103601455688,
      "learning_rate": 2.32662192393736e-05,
      "loss": 3.4879,
      "step": 19750
    },
    {
      "epoch": 2.6577181208053693,
      "grad_norm": 1.5941276550292969,
      "learning_rate": 2.2818791946308727e-05,
      "loss": 3.4742,
      "step": 19800
    },
    {
      "epoch": 2.6644295302013425,
      "grad_norm": 1.63690984249115,
      "learning_rate": 2.237136465324385e-05,
      "loss": 3.4759,
      "step": 19850
    },
    {
      "epoch": 2.6711409395973154,
      "grad_norm": 1.5306415557861328,
      "learning_rate": 2.192393736017897e-05,
      "loss": 3.5182,
      "step": 19900
    },
    {
      "epoch": 2.6778523489932886,
      "grad_norm": 1.5418696403503418,
      "learning_rate": 2.1476510067114094e-05,
      "loss": 3.4871,
      "step": 19950
    },
    {
      "epoch": 2.684563758389262,
      "grad_norm": 1.5317692756652832,
      "learning_rate": 2.102908277404922e-05,
      "loss": 3.4767,
      "step": 20000
    },
    {
      "epoch": 2.684563758389262,
      "eval_loss": 3.6654725074768066,
      "eval_runtime": 131.6533,
      "eval_samples_per_second": 56.163,
      "eval_steps_per_second": 7.026,
      "step": 20000
    },
    {
      "epoch": 2.6912751677852347,
      "grad_norm": 1.5448510646820068,
      "learning_rate": 2.0581655480984342e-05,
      "loss": 3.4785,
      "step": 20050
    },
    {
      "epoch": 2.697986577181208,
      "grad_norm": 1.6618150472640991,
      "learning_rate": 2.013422818791946e-05,
      "loss": 3.4565,
      "step": 20100
    },
    {
      "epoch": 2.704697986577181,
      "grad_norm": 1.6845602989196777,
      "learning_rate": 1.9686800894854587e-05,
      "loss": 3.4378,
      "step": 20150
    },
    {
      "epoch": 2.7114093959731544,
      "grad_norm": 1.5278732776641846,
      "learning_rate": 1.923937360178971e-05,
      "loss": 3.4872,
      "step": 20200
    },
    {
      "epoch": 2.7181208053691277,
      "grad_norm": 1.5627647638320923,
      "learning_rate": 1.8791946308724835e-05,
      "loss": 3.5151,
      "step": 20250
    },
    {
      "epoch": 2.7248322147651005,
      "grad_norm": 1.5048692226409912,
      "learning_rate": 1.8344519015659954e-05,
      "loss": 3.4351,
      "step": 20300
    },
    {
      "epoch": 2.7315436241610738,
      "grad_norm": 1.5671119689941406,
      "learning_rate": 1.789709172259508e-05,
      "loss": 3.4363,
      "step": 20350
    },
    {
      "epoch": 2.738255033557047,
      "grad_norm": 1.5389512777328491,
      "learning_rate": 1.7449664429530202e-05,
      "loss": 3.4204,
      "step": 20400
    },
    {
      "epoch": 2.7449664429530203,
      "grad_norm": 1.4271159172058105,
      "learning_rate": 1.7002237136465328e-05,
      "loss": 3.4789,
      "step": 20450
    },
    {
      "epoch": 2.751677852348993,
      "grad_norm": 1.5658557415008545,
      "learning_rate": 1.6554809843400447e-05,
      "loss": 3.4631,
      "step": 20500
    },
    {
      "epoch": 2.7583892617449663,
      "grad_norm": 1.6067215204238892,
      "learning_rate": 1.610738255033557e-05,
      "loss": 3.4614,
      "step": 20550
    },
    {
      "epoch": 2.7651006711409396,
      "grad_norm": 1.5980968475341797,
      "learning_rate": 1.5659955257270695e-05,
      "loss": 3.4884,
      "step": 20600
    },
    {
      "epoch": 2.771812080536913,
      "grad_norm": 1.462877631187439,
      "learning_rate": 1.5212527964205816e-05,
      "loss": 3.4587,
      "step": 20650
    },
    {
      "epoch": 2.778523489932886,
      "grad_norm": 1.4764991998672485,
      "learning_rate": 1.4765100671140942e-05,
      "loss": 3.4259,
      "step": 20700
    },
    {
      "epoch": 2.785234899328859,
      "grad_norm": 1.4098968505859375,
      "learning_rate": 1.4317673378076063e-05,
      "loss": 3.4494,
      "step": 20750
    },
    {
      "epoch": 2.791946308724832,
      "grad_norm": 1.5662325620651245,
      "learning_rate": 1.3870246085011188e-05,
      "loss": 3.4403,
      "step": 20800
    },
    {
      "epoch": 2.7986577181208054,
      "grad_norm": 1.580735683441162,
      "learning_rate": 1.3422818791946309e-05,
      "loss": 3.4331,
      "step": 20850
    },
    {
      "epoch": 2.8053691275167782,
      "grad_norm": 1.5595062971115112,
      "learning_rate": 1.2975391498881432e-05,
      "loss": 3.4493,
      "step": 20900
    },
    {
      "epoch": 2.8120805369127515,
      "grad_norm": 1.5045111179351807,
      "learning_rate": 1.2527964205816556e-05,
      "loss": 3.4349,
      "step": 20950
    },
    {
      "epoch": 2.8187919463087248,
      "grad_norm": 1.480884075164795,
      "learning_rate": 1.208053691275168e-05,
      "loss": 3.4724,
      "step": 21000
    },
    {
      "epoch": 2.825503355704698,
      "grad_norm": 1.541366457939148,
      "learning_rate": 1.16331096196868e-05,
      "loss": 3.504,
      "step": 21050
    },
    {
      "epoch": 2.8322147651006713,
      "grad_norm": 1.4172203540802002,
      "learning_rate": 1.1185682326621925e-05,
      "loss": 3.419,
      "step": 21100
    },
    {
      "epoch": 2.8389261744966445,
      "grad_norm": 1.5004678964614868,
      "learning_rate": 1.0738255033557047e-05,
      "loss": 3.4637,
      "step": 21150
    },
    {
      "epoch": 2.8456375838926173,
      "grad_norm": 1.9785261154174805,
      "learning_rate": 1.0290827740492171e-05,
      "loss": 3.4537,
      "step": 21200
    },
    {
      "epoch": 2.8523489932885906,
      "grad_norm": 1.4204965829849243,
      "learning_rate": 9.843400447427293e-06,
      "loss": 3.5079,
      "step": 21250
    },
    {
      "epoch": 2.859060402684564,
      "grad_norm": 1.521323800086975,
      "learning_rate": 9.395973154362418e-06,
      "loss": 3.4777,
      "step": 21300
    },
    {
      "epoch": 2.8657718120805367,
      "grad_norm": 1.6412720680236816,
      "learning_rate": 8.94854586129754e-06,
      "loss": 3.5129,
      "step": 21350
    },
    {
      "epoch": 2.87248322147651,
      "grad_norm": 1.5451759099960327,
      "learning_rate": 8.501118568232664e-06,
      "loss": 3.5096,
      "step": 21400
    },
    {
      "epoch": 2.879194630872483,
      "grad_norm": 1.5279150009155273,
      "learning_rate": 8.053691275167785e-06,
      "loss": 3.4441,
      "step": 21450
    },
    {
      "epoch": 2.8859060402684564,
      "grad_norm": 1.5803308486938477,
      "learning_rate": 7.606263982102908e-06,
      "loss": 3.4142,
      "step": 21500
    },
    {
      "epoch": 2.8926174496644297,
      "grad_norm": 1.5679939985275269,
      "learning_rate": 7.158836689038031e-06,
      "loss": 3.4968,
      "step": 21550
    },
    {
      "epoch": 2.899328859060403,
      "grad_norm": 1.413280725479126,
      "learning_rate": 6.7114093959731546e-06,
      "loss": 3.4516,
      "step": 21600
    },
    {
      "epoch": 2.9060402684563758,
      "grad_norm": 1.5826222896575928,
      "learning_rate": 6.263982102908278e-06,
      "loss": 3.5182,
      "step": 21650
    },
    {
      "epoch": 2.912751677852349,
      "grad_norm": 1.537450909614563,
      "learning_rate": 5.8165548098434e-06,
      "loss": 3.4371,
      "step": 21700
    },
    {
      "epoch": 2.9194630872483223,
      "grad_norm": 1.4899959564208984,
      "learning_rate": 5.3691275167785235e-06,
      "loss": 3.4387,
      "step": 21750
    },
    {
      "epoch": 2.926174496644295,
      "grad_norm": 1.641611933708191,
      "learning_rate": 4.921700223713647e-06,
      "loss": 3.4509,
      "step": 21800
    },
    {
      "epoch": 2.9328859060402683,
      "grad_norm": 1.4639891386032104,
      "learning_rate": 4.47427293064877e-06,
      "loss": 3.4558,
      "step": 21850
    },
    {
      "epoch": 2.9395973154362416,
      "grad_norm": 1.5091283321380615,
      "learning_rate": 4.026845637583892e-06,
      "loss": 3.4392,
      "step": 21900
    },
    {
      "epoch": 2.946308724832215,
      "grad_norm": 1.504909634590149,
      "learning_rate": 3.5794183445190157e-06,
      "loss": 3.3914,
      "step": 21950
    },
    {
      "epoch": 2.953020134228188,
      "grad_norm": 1.4761972427368164,
      "learning_rate": 3.131991051454139e-06,
      "loss": 3.5307,
      "step": 22000
    },
    {
      "epoch": 2.953020134228188,
      "eval_loss": 3.6573634147644043,
      "eval_runtime": 131.5405,
      "eval_samples_per_second": 56.211,
      "eval_steps_per_second": 7.032,
      "step": 22000
    }
  ],
  "logging_steps": 50,
  "max_steps": 22350,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 2000,
  "total_flos": 9.950574697198387e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
