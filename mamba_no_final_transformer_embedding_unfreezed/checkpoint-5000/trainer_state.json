{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.48267207259387973,
  "eval_steps": 20000,
  "global_step": 5000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004826720725938797,
      "grad_norm": 5.157319068908691,
      "learning_rate": 0.00019903465585481225,
      "loss": 9.8016,
      "step": 50
    },
    {
      "epoch": 0.009653441451877595,
      "grad_norm": 3.169473648071289,
      "learning_rate": 0.0001980693117096245,
      "loss": 5.9327,
      "step": 100
    },
    {
      "epoch": 0.014480162177816391,
      "grad_norm": 3.5320143699645996,
      "learning_rate": 0.00019710396756443673,
      "loss": 5.4071,
      "step": 150
    },
    {
      "epoch": 0.01930688290375519,
      "grad_norm": 3.2244515419006348,
      "learning_rate": 0.00019613862341924897,
      "loss": 5.153,
      "step": 200
    },
    {
      "epoch": 0.024133603629693984,
      "grad_norm": 2.9150404930114746,
      "learning_rate": 0.00019517327927406122,
      "loss": 4.9738,
      "step": 250
    },
    {
      "epoch": 0.028960324355632783,
      "grad_norm": 2.6836743354797363,
      "learning_rate": 0.00019420793512887346,
      "loss": 4.8478,
      "step": 300
    },
    {
      "epoch": 0.03378704508157158,
      "grad_norm": 2.6480507850646973,
      "learning_rate": 0.0001932425909836857,
      "loss": 4.7768,
      "step": 350
    },
    {
      "epoch": 0.03861376580751038,
      "grad_norm": 2.5476138591766357,
      "learning_rate": 0.0001922772468384979,
      "loss": 4.7125,
      "step": 400
    },
    {
      "epoch": 0.043440486533449174,
      "grad_norm": 2.5622105598449707,
      "learning_rate": 0.00019131190269331015,
      "loss": 4.6803,
      "step": 450
    },
    {
      "epoch": 0.04826720725938797,
      "grad_norm": 2.569155693054199,
      "learning_rate": 0.00019034655854812242,
      "loss": 4.6178,
      "step": 500
    },
    {
      "epoch": 0.05309392798532677,
      "grad_norm": 2.3240466117858887,
      "learning_rate": 0.00018938121440293466,
      "loss": 4.5676,
      "step": 550
    },
    {
      "epoch": 0.057920648711265565,
      "grad_norm": 2.1708295345306396,
      "learning_rate": 0.0001884158702577469,
      "loss": 4.5617,
      "step": 600
    },
    {
      "epoch": 0.06274736943720437,
      "grad_norm": 2.2527666091918945,
      "learning_rate": 0.00018745052611255912,
      "loss": 4.5318,
      "step": 650
    },
    {
      "epoch": 0.06757409016314316,
      "grad_norm": 2.3612465858459473,
      "learning_rate": 0.00018648518196737136,
      "loss": 4.5075,
      "step": 700
    },
    {
      "epoch": 0.07240081088908196,
      "grad_norm": 2.138241767883301,
      "learning_rate": 0.0001855198378221836,
      "loss": 4.4627,
      "step": 750
    },
    {
      "epoch": 0.07722753161502076,
      "grad_norm": 2.5902340412139893,
      "learning_rate": 0.00018455449367699587,
      "loss": 4.4386,
      "step": 800
    },
    {
      "epoch": 0.08205425234095955,
      "grad_norm": 2.0994956493377686,
      "learning_rate": 0.0001835891495318081,
      "loss": 4.4268,
      "step": 850
    },
    {
      "epoch": 0.08688097306689835,
      "grad_norm": 2.0907363891601562,
      "learning_rate": 0.00018262380538662033,
      "loss": 4.4135,
      "step": 900
    },
    {
      "epoch": 0.09170769379283715,
      "grad_norm": 2.0017874240875244,
      "learning_rate": 0.00018165846124143257,
      "loss": 4.3989,
      "step": 950
    },
    {
      "epoch": 0.09653441451877594,
      "grad_norm": 1.9576056003570557,
      "learning_rate": 0.0001806931170962448,
      "loss": 4.415,
      "step": 1000
    },
    {
      "epoch": 0.10136113524471474,
      "grad_norm": 2.359292507171631,
      "learning_rate": 0.00017972777295105705,
      "loss": 4.3676,
      "step": 1050
    },
    {
      "epoch": 0.10618785597065354,
      "grad_norm": 2.072448253631592,
      "learning_rate": 0.00017876242880586932,
      "loss": 4.351,
      "step": 1100
    },
    {
      "epoch": 0.11101457669659233,
      "grad_norm": 2.0725276470184326,
      "learning_rate": 0.00017779708466068153,
      "loss": 4.3553,
      "step": 1150
    },
    {
      "epoch": 0.11584129742253113,
      "grad_norm": 2.080881118774414,
      "learning_rate": 0.00017683174051549377,
      "loss": 4.3437,
      "step": 1200
    },
    {
      "epoch": 0.12066801814846993,
      "grad_norm": 2.266587257385254,
      "learning_rate": 0.00017586639637030602,
      "loss": 4.306,
      "step": 1250
    },
    {
      "epoch": 0.12549473887440873,
      "grad_norm": 1.9064018726348877,
      "learning_rate": 0.00017490105222511826,
      "loss": 4.3272,
      "step": 1300
    },
    {
      "epoch": 0.13032145960034752,
      "grad_norm": 2.0975658893585205,
      "learning_rate": 0.0001739357080799305,
      "loss": 4.3122,
      "step": 1350
    },
    {
      "epoch": 0.1351481803262863,
      "grad_norm": 2.4044857025146484,
      "learning_rate": 0.00017297036393474274,
      "loss": 4.2878,
      "step": 1400
    },
    {
      "epoch": 0.13997490105222513,
      "grad_norm": 2.0520241260528564,
      "learning_rate": 0.00017200501978955498,
      "loss": 4.2679,
      "step": 1450
    },
    {
      "epoch": 0.1448016217781639,
      "grad_norm": 2.1134302616119385,
      "learning_rate": 0.00017103967564436722,
      "loss": 4.2902,
      "step": 1500
    },
    {
      "epoch": 0.1496283425041027,
      "grad_norm": 2.04510498046875,
      "learning_rate": 0.00017007433149917946,
      "loss": 4.2766,
      "step": 1550
    },
    {
      "epoch": 0.15445506323004152,
      "grad_norm": 1.966300129890442,
      "learning_rate": 0.0001691089873539917,
      "loss": 4.2709,
      "step": 1600
    },
    {
      "epoch": 0.1592817839559803,
      "grad_norm": 2.2024474143981934,
      "learning_rate": 0.00016814364320880395,
      "loss": 4.2549,
      "step": 1650
    },
    {
      "epoch": 0.1641085046819191,
      "grad_norm": 1.8837565183639526,
      "learning_rate": 0.0001671782990636162,
      "loss": 4.2438,
      "step": 1700
    },
    {
      "epoch": 0.1689352254078579,
      "grad_norm": 2.01192045211792,
      "learning_rate": 0.00016621295491842843,
      "loss": 4.2491,
      "step": 1750
    },
    {
      "epoch": 0.1737619461337967,
      "grad_norm": 1.86737859249115,
      "learning_rate": 0.00016524761077324067,
      "loss": 4.2364,
      "step": 1800
    },
    {
      "epoch": 0.17858866685973548,
      "grad_norm": 2.028008222579956,
      "learning_rate": 0.0001642822666280529,
      "loss": 4.2264,
      "step": 1850
    },
    {
      "epoch": 0.1834153875856743,
      "grad_norm": 2.127014636993408,
      "learning_rate": 0.00016331692248286515,
      "loss": 4.2342,
      "step": 1900
    },
    {
      "epoch": 0.1882421083116131,
      "grad_norm": 1.992933988571167,
      "learning_rate": 0.00016235157833767737,
      "loss": 4.2332,
      "step": 1950
    },
    {
      "epoch": 0.19306882903755188,
      "grad_norm": 1.8150848150253296,
      "learning_rate": 0.00016138623419248964,
      "loss": 4.2222,
      "step": 2000
    },
    {
      "epoch": 0.1978955497634907,
      "grad_norm": 1.8652533292770386,
      "learning_rate": 0.00016042089004730188,
      "loss": 4.2253,
      "step": 2050
    },
    {
      "epoch": 0.20272227048942948,
      "grad_norm": 1.9236096143722534,
      "learning_rate": 0.00015945554590211412,
      "loss": 4.198,
      "step": 2100
    },
    {
      "epoch": 0.20754899121536827,
      "grad_norm": 1.9021263122558594,
      "learning_rate": 0.00015849020175692636,
      "loss": 4.2,
      "step": 2150
    },
    {
      "epoch": 0.21237571194130708,
      "grad_norm": 2.1451685428619385,
      "learning_rate": 0.00015752485761173857,
      "loss": 4.1837,
      "step": 2200
    },
    {
      "epoch": 0.21720243266724587,
      "grad_norm": 1.9215197563171387,
      "learning_rate": 0.00015655951346655082,
      "loss": 4.1958,
      "step": 2250
    },
    {
      "epoch": 0.22202915339318466,
      "grad_norm": 1.9578837156295776,
      "learning_rate": 0.00015559416932136308,
      "loss": 4.2012,
      "step": 2300
    },
    {
      "epoch": 0.22685587411912347,
      "grad_norm": 2.0307207107543945,
      "learning_rate": 0.00015462882517617533,
      "loss": 4.1654,
      "step": 2350
    },
    {
      "epoch": 0.23168259484506226,
      "grad_norm": 1.8201836347579956,
      "learning_rate": 0.00015366348103098757,
      "loss": 4.203,
      "step": 2400
    },
    {
      "epoch": 0.23650931557100105,
      "grad_norm": 2.041116714477539,
      "learning_rate": 0.00015269813688579978,
      "loss": 4.1832,
      "step": 2450
    },
    {
      "epoch": 0.24133603629693987,
      "grad_norm": 2.3927600383758545,
      "learning_rate": 0.00015173279274061202,
      "loss": 4.1791,
      "step": 2500
    },
    {
      "epoch": 0.24616275702287865,
      "grad_norm": 1.8787637948989868,
      "learning_rate": 0.00015076744859542426,
      "loss": 4.1408,
      "step": 2550
    },
    {
      "epoch": 0.25098947774881747,
      "grad_norm": 1.8831043243408203,
      "learning_rate": 0.00014980210445023653,
      "loss": 4.1625,
      "step": 2600
    },
    {
      "epoch": 0.25581619847475623,
      "grad_norm": 1.7635570764541626,
      "learning_rate": 0.00014883676030504877,
      "loss": 4.1561,
      "step": 2650
    },
    {
      "epoch": 0.26064291920069504,
      "grad_norm": 1.9111034870147705,
      "learning_rate": 0.000147871416159861,
      "loss": 4.1454,
      "step": 2700
    },
    {
      "epoch": 0.26546963992663386,
      "grad_norm": 1.9849793910980225,
      "learning_rate": 0.00014690607201467323,
      "loss": 4.1615,
      "step": 2750
    },
    {
      "epoch": 0.2702963606525726,
      "grad_norm": 1.822638750076294,
      "learning_rate": 0.00014594072786948547,
      "loss": 4.1316,
      "step": 2800
    },
    {
      "epoch": 0.27512308137851144,
      "grad_norm": 2.196438789367676,
      "learning_rate": 0.0001449753837242977,
      "loss": 4.1447,
      "step": 2850
    },
    {
      "epoch": 0.27994980210445025,
      "grad_norm": 1.9603737592697144,
      "learning_rate": 0.00014401003957910998,
      "loss": 4.1246,
      "step": 2900
    },
    {
      "epoch": 0.284776522830389,
      "grad_norm": 2.112482786178589,
      "learning_rate": 0.0001430446954339222,
      "loss": 4.1244,
      "step": 2950
    },
    {
      "epoch": 0.2896032435563278,
      "grad_norm": 1.9923651218414307,
      "learning_rate": 0.00014207935128873444,
      "loss": 4.1465,
      "step": 3000
    },
    {
      "epoch": 0.29442996428226664,
      "grad_norm": 2.0097315311431885,
      "learning_rate": 0.00014111400714354668,
      "loss": 4.1061,
      "step": 3050
    },
    {
      "epoch": 0.2992566850082054,
      "grad_norm": 1.821945071220398,
      "learning_rate": 0.00014014866299835892,
      "loss": 4.1098,
      "step": 3100
    },
    {
      "epoch": 0.3040834057341442,
      "grad_norm": 2.0364973545074463,
      "learning_rate": 0.00013918331885317116,
      "loss": 4.1137,
      "step": 3150
    },
    {
      "epoch": 0.30891012646008303,
      "grad_norm": 1.8813040256500244,
      "learning_rate": 0.0001382179747079834,
      "loss": 4.1161,
      "step": 3200
    },
    {
      "epoch": 0.3137368471860218,
      "grad_norm": 2.0794146060943604,
      "learning_rate": 0.00013725263056279564,
      "loss": 4.115,
      "step": 3250
    },
    {
      "epoch": 0.3185635679119606,
      "grad_norm": 2.096228837966919,
      "learning_rate": 0.00013628728641760788,
      "loss": 4.1193,
      "step": 3300
    },
    {
      "epoch": 0.3233902886378994,
      "grad_norm": 1.6751331090927124,
      "learning_rate": 0.00013532194227242013,
      "loss": 4.084,
      "step": 3350
    },
    {
      "epoch": 0.3282170093638382,
      "grad_norm": 1.7695767879486084,
      "learning_rate": 0.00013435659812723237,
      "loss": 4.0888,
      "step": 3400
    },
    {
      "epoch": 0.333043730089777,
      "grad_norm": 1.9630340337753296,
      "learning_rate": 0.0001333912539820446,
      "loss": 4.1016,
      "step": 3450
    },
    {
      "epoch": 0.3378704508157158,
      "grad_norm": 1.8818252086639404,
      "learning_rate": 0.00013242590983685685,
      "loss": 4.0894,
      "step": 3500
    },
    {
      "epoch": 0.3426971715416546,
      "grad_norm": 1.8672274351119995,
      "learning_rate": 0.0001314605656916691,
      "loss": 4.0655,
      "step": 3550
    },
    {
      "epoch": 0.3475238922675934,
      "grad_norm": 1.67190420627594,
      "learning_rate": 0.00013049522154648133,
      "loss": 4.0704,
      "step": 3600
    },
    {
      "epoch": 0.3523506129935322,
      "grad_norm": 2.0213065147399902,
      "learning_rate": 0.00012952987740129357,
      "loss": 4.0962,
      "step": 3650
    },
    {
      "epoch": 0.35717733371947097,
      "grad_norm": 1.9907252788543701,
      "learning_rate": 0.00012856453325610581,
      "loss": 4.085,
      "step": 3700
    },
    {
      "epoch": 0.3620040544454098,
      "grad_norm": 1.713038444519043,
      "learning_rate": 0.00012759918911091803,
      "loss": 4.0891,
      "step": 3750
    },
    {
      "epoch": 0.3668307751713486,
      "grad_norm": 1.7325425148010254,
      "learning_rate": 0.0001266338449657303,
      "loss": 4.0883,
      "step": 3800
    },
    {
      "epoch": 0.37165749589728736,
      "grad_norm": 1.8436989784240723,
      "learning_rate": 0.00012566850082054254,
      "loss": 4.0775,
      "step": 3850
    },
    {
      "epoch": 0.3764842166232262,
      "grad_norm": 1.9168317317962646,
      "learning_rate": 0.00012470315667535478,
      "loss": 4.0693,
      "step": 3900
    },
    {
      "epoch": 0.381310937349165,
      "grad_norm": 1.8634140491485596,
      "learning_rate": 0.000123737812530167,
      "loss": 4.0943,
      "step": 3950
    },
    {
      "epoch": 0.38613765807510375,
      "grad_norm": 1.7801287174224854,
      "learning_rate": 0.00012277246838497924,
      "loss": 4.0781,
      "step": 4000
    },
    {
      "epoch": 0.39096437880104257,
      "grad_norm": 1.6942285299301147,
      "learning_rate": 0.00012180712423979148,
      "loss": 4.0727,
      "step": 4050
    },
    {
      "epoch": 0.3957910995269814,
      "grad_norm": 1.6034724712371826,
      "learning_rate": 0.00012084178009460375,
      "loss": 4.057,
      "step": 4100
    },
    {
      "epoch": 0.40061782025292014,
      "grad_norm": 1.8103234767913818,
      "learning_rate": 0.00011987643594941597,
      "loss": 4.0549,
      "step": 4150
    },
    {
      "epoch": 0.40544454097885896,
      "grad_norm": 1.7076698541641235,
      "learning_rate": 0.00011891109180422821,
      "loss": 4.0585,
      "step": 4200
    },
    {
      "epoch": 0.4102712617047978,
      "grad_norm": 1.5828120708465576,
      "learning_rate": 0.00011794574765904046,
      "loss": 4.0697,
      "step": 4250
    },
    {
      "epoch": 0.41509798243073653,
      "grad_norm": 1.8018310070037842,
      "learning_rate": 0.00011698040351385268,
      "loss": 4.0744,
      "step": 4300
    },
    {
      "epoch": 0.41992470315667535,
      "grad_norm": 1.8084375858306885,
      "learning_rate": 0.00011601505936866492,
      "loss": 4.078,
      "step": 4350
    },
    {
      "epoch": 0.42475142388261417,
      "grad_norm": 1.6827994585037231,
      "learning_rate": 0.00011504971522347718,
      "loss": 4.0498,
      "step": 4400
    },
    {
      "epoch": 0.4295781446085529,
      "grad_norm": 1.8500152826309204,
      "learning_rate": 0.00011408437107828942,
      "loss": 4.0539,
      "step": 4450
    },
    {
      "epoch": 0.43440486533449174,
      "grad_norm": 1.7086713314056396,
      "learning_rate": 0.00011311902693310166,
      "loss": 4.0427,
      "step": 4500
    },
    {
      "epoch": 0.43923158606043056,
      "grad_norm": 1.663208246231079,
      "learning_rate": 0.00011215368278791389,
      "loss": 4.0592,
      "step": 4550
    },
    {
      "epoch": 0.4440583067863693,
      "grad_norm": 1.7406188249588013,
      "learning_rate": 0.00011118833864272613,
      "loss": 4.0608,
      "step": 4600
    },
    {
      "epoch": 0.44888502751230813,
      "grad_norm": 1.5982779264450073,
      "learning_rate": 0.00011022299449753837,
      "loss": 4.0442,
      "step": 4650
    },
    {
      "epoch": 0.45371174823824695,
      "grad_norm": 1.703682780265808,
      "learning_rate": 0.00010925765035235063,
      "loss": 4.032,
      "step": 4700
    },
    {
      "epoch": 0.4585384689641857,
      "grad_norm": 1.7503421306610107,
      "learning_rate": 0.00010829230620716287,
      "loss": 4.0271,
      "step": 4750
    },
    {
      "epoch": 0.4633651896901245,
      "grad_norm": 1.4982459545135498,
      "learning_rate": 0.0001073269620619751,
      "loss": 4.0231,
      "step": 4800
    },
    {
      "epoch": 0.46819191041606334,
      "grad_norm": 1.5342957973480225,
      "learning_rate": 0.00010636161791678734,
      "loss": 4.0294,
      "step": 4850
    },
    {
      "epoch": 0.4730186311420021,
      "grad_norm": 1.770047903060913,
      "learning_rate": 0.00010539627377159958,
      "loss": 4.0401,
      "step": 4900
    },
    {
      "epoch": 0.4778453518679409,
      "grad_norm": 1.8603571653366089,
      "learning_rate": 0.00010443092962641181,
      "loss": 4.0348,
      "step": 4950
    },
    {
      "epoch": 0.48267207259387973,
      "grad_norm": 1.7546390295028687,
      "learning_rate": 0.00010346558548122408,
      "loss": 4.0203,
      "step": 5000
    }
  ],
  "logging_steps": 50,
  "max_steps": 10359,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 5000,
  "total_flos": 3.809517502464e+16,
  "train_batch_size": 48,
  "trial_name": null,
  "trial_params": null
}
