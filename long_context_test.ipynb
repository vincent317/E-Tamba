{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def load_json_gz(filename):\n",
    "    with gzip.open(filename, 'r') as f:\n",
    "        i = 0\n",
    "        ret = []\n",
    "        for json_line in f:\n",
    "            if i == 10000:\n",
    "                return ret\n",
    "            data = json.loads(json_line)\n",
    "            text = data['text']\n",
    "            if len(text) > 2000:\n",
    "                ret.append(text)\n",
    "                i += 1\n",
    "\n",
    "# Load 10000 strings from C4 dataset: https://huggingface.co/datasets/allenai/c4/tree/main/en\n",
    "strings = load_json_gz('graphing/c4-validation.00000-of-00008.json.gz')\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_task(batch_size=64, batches=10, model=None, tokenizer=None, token_max_len=25, shuffle=False):\n",
    "    string_idx = 0\n",
    "    success_copies = 0\n",
    "    for _ in tqdm(range(batches)):\n",
    "        cur_batch = []\n",
    "        for count in range(batch_size):\n",
    "            cur_batch.append(strings[count + string_idx])\n",
    "        outputs = tokenizer(cur_batch, return_tensors=\"pt\", truncation=True, max_length=token_max_len).to(device)\n",
    "        input_ids = outputs['input_ids']\n",
    "        attn_masks = outputs['attention_mask']\n",
    "        if shuffle:\n",
    "            col_perm = torch.randperm(input_ids.size(1))\n",
    "            input_ids = input_ids[:, col_perm]\n",
    "        input_ids = torch.cat([input_ids, input_ids], dim=1)\n",
    "        input_ids = torch.cat([input_ids, input_ids[:, 0:1]], dim=1)\n",
    "        output_ids = model.generate(input_ids, max_new_tokens = token_max_len-1)\n",
    "        for count in range(batch_size):\n",
    "            gold_token_len = (input_ids.shape[1]-1) // 2\n",
    "            if torch.equal(input_ids[count][:gold_token_len], output_ids[count][gold_token_len*2:]):\n",
    "                success_copies += 1\n",
    "        string_idx += batch_size\n",
    "    return success_copies / (batch_size * batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM\n",
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "# pythia_160m_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "#   \"EleutherAI/pythia-160m\"\n",
    "# )\n",
    "# pythia_160m_model.to(device)\n",
    "# pythia_160m_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#   \"EleutherAI/pythia-160m\"\n",
    "# )\n",
    "# pythia_160m_tokenizer.pad_token_id = pythia_160m_tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaTransformerForLM(\n",
       "  (model): MambaTransformer(\n",
       "    (embed_in): Embedding(50304, 768)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (first_transformer_layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mamba_layers): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (mixer): Mamba(\n",
       "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
       "          (act): SiLU()\n",
       "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (final_transformer_layer): GPTNeoXLayer(\n",
       "      (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (attention): GPTNeoXAttention(\n",
       "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "        (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp): GPTNeoXMLP(\n",
       "        (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling_mamba_transformer import MambaTransformerForLM, MambaTransformerConfig\n",
    "\n",
    "checkpoint_point_path = 'seq_len_2048_6_transformer_layers/checkpoint-14000/model.safetensors'\n",
    "model = MambaTransformerForLM(MambaTransformerConfig(), checkpoint_point_path).to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.nn as nn\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-160m', padding_side='left')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "batches = 10\n",
    "batch_size = 64\n",
    "shuffle = False\n",
    "token_max_len = 150\n",
    "\n",
    "softmax = nn.Softmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [13:28<00:00, 80.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "string_idx = 0\n",
    "success_copies = 0\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(batches)):\n",
    "        cur_batch = []\n",
    "        for count in range(batch_size):\n",
    "            cur_batch.append(strings[count + string_idx])\n",
    "        outputs = tokenizer(cur_batch, return_tensors=\"pt\", truncation=True, max_length=token_max_len).to(device)\n",
    "        input_ids = outputs['input_ids']\n",
    "        attn_masks = outputs['attention_mask']\n",
    "        if shuffle:\n",
    "            col_perm = torch.randperm(input_ids.size(1))\n",
    "            input_ids = input_ids[:, col_perm]\n",
    "        input_ids = torch.cat([input_ids, input_ids], dim=1)\n",
    "        input_ids = torch.cat([input_ids, input_ids[:, 0:1]], dim=1)\n",
    "        output_ids = input_ids\n",
    "        for i in range(token_max_len-1):\n",
    "            bs, seq_len = output_ids.size()\n",
    "            mask = torch.ones(bs, seq_len).to('cuda')\n",
    "            logits = model(input_ids=output_ids, attention_mask=mask, labels=None)['logits'] # bs, seq_len, vocab_size\n",
    "            next_token = torch.unsqueeze(torch.argmax(softmax(logits), dim=-1)[:, -1], 1)\n",
    "            output_ids = torch.cat((output_ids, next_token), dim=-1) # bs, seq_len, 1\n",
    "            \n",
    "        for count in range(batch_size):\n",
    "            gold_token_len = (input_ids.shape[1]-1) // 2\n",
    "            if torch.equal(input_ids[count][:gold_token_len], output_ids[count][gold_token_len*2:]):\n",
    "                success_copies += 1\n",
    "        string_idx += batch_size\n",
    "print(success_copies / (batch_size * batches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(copy_task(model=pythia_160m_model, tokenizer=pythia_160m_tokenizer, token_max_len=25, shuffle=False))\n",
    "print(copy_task(model=pythia_160m_model, tokenizer=pythia_160m_tokenizer, token_max_len=50, shuffle=False))\n",
    "# print(copy_task(model=pythia_160m_model, tokenizer=pythia_160m_tokenizer, token_max_len=100, shuffle=False))\n",
    "# print(copy_task(model=pythia_160m_model, tokenizer=pythia_160m_tokenizer, token_max_len=150, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(50280, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x MambaBlock(\n",
       "        (norm): MambaRMSNorm()\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mamba_130m_tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "mamba_130m_tokenizer.pad_token_id = mamba_130m_tokenizer.eos_token_id\n",
    "mamba_130m_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "mamba_130m_model.eval()\n",
    "mamba_130m_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.840625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(copy_task(model=mamba_130m_model, tokenizer=mamba_130m_tokenizer, token_max_len=25, shuffle=False))\n",
    "# print(copy_task(model=mamba_130m_model, tokenizer=mamba_130m_tokenizer, token_max_len=50))\n",
    "# print(copy_task(model=mamba_130m_model, tokenizer=mamba_130m_tokenizer, token_max_len=100))\n",
    "# print(copy_task(model=mamba_130m_model, tokenizer=mamba_130m_tokenizer, token_max_len=150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_out",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
