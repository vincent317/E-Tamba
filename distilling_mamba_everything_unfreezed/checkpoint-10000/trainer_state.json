{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8044405116241654,
  "eval_steps": 20000,
  "global_step": 10000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004022202558120827,
      "grad_norm": 2.4890615940093994,
      "learning_rate": 0.00015935644759070068,
      "loss": 6.9329,
      "step": 50
    },
    {
      "epoch": 0.008044405116241654,
      "grad_norm": 2.8732964992523193,
      "learning_rate": 0.00015871289518140134,
      "loss": 3.2687,
      "step": 100
    },
    {
      "epoch": 0.01206660767436248,
      "grad_norm": 2.124500274658203,
      "learning_rate": 0.000158069342772102,
      "loss": 2.7392,
      "step": 150
    },
    {
      "epoch": 0.01608881023248331,
      "grad_norm": 2.136768341064453,
      "learning_rate": 0.00015742579036280268,
      "loss": 2.4771,
      "step": 200
    },
    {
      "epoch": 0.020111012790604135,
      "grad_norm": 2.1400184631347656,
      "learning_rate": 0.00015678223795350334,
      "loss": 2.2869,
      "step": 250
    },
    {
      "epoch": 0.02413321534872496,
      "grad_norm": 1.3168089389801025,
      "learning_rate": 0.00015613868554420403,
      "loss": 2.1297,
      "step": 300
    },
    {
      "epoch": 0.028155417906845788,
      "grad_norm": 2.7445034980773926,
      "learning_rate": 0.00015549513313490467,
      "loss": 2.0299,
      "step": 350
    },
    {
      "epoch": 0.03217762046496662,
      "grad_norm": 1.44679856300354,
      "learning_rate": 0.00015485158072560534,
      "loss": 1.9399,
      "step": 400
    },
    {
      "epoch": 0.03619982302308744,
      "grad_norm": 1.615033507347107,
      "learning_rate": 0.00015420802831630603,
      "loss": 1.8639,
      "step": 450
    },
    {
      "epoch": 0.04022202558120827,
      "grad_norm": 1.0225090980529785,
      "learning_rate": 0.0001535644759070067,
      "loss": 1.8068,
      "step": 500
    },
    {
      "epoch": 0.04424422813932909,
      "grad_norm": 2.7151172161102295,
      "learning_rate": 0.00015292092349770736,
      "loss": 1.7557,
      "step": 550
    },
    {
      "epoch": 0.04826643069744992,
      "grad_norm": 1.1550672054290771,
      "learning_rate": 0.00015227737108840803,
      "loss": 1.7106,
      "step": 600
    },
    {
      "epoch": 0.05228863325557075,
      "grad_norm": 1.035819411277771,
      "learning_rate": 0.0001516338186791087,
      "loss": 1.6951,
      "step": 650
    },
    {
      "epoch": 0.056310835813691576,
      "grad_norm": 1.290684700012207,
      "learning_rate": 0.00015099026626980936,
      "loss": 1.6582,
      "step": 700
    },
    {
      "epoch": 0.060333038371812406,
      "grad_norm": 1.0458109378814697,
      "learning_rate": 0.00015034671386051002,
      "loss": 1.6362,
      "step": 750
    },
    {
      "epoch": 0.06435524092993324,
      "grad_norm": 0.9664485454559326,
      "learning_rate": 0.0001497031614512107,
      "loss": 1.6147,
      "step": 800
    },
    {
      "epoch": 0.06837744348805405,
      "grad_norm": 1.583582878112793,
      "learning_rate": 0.00014905960904191136,
      "loss": 1.5848,
      "step": 850
    },
    {
      "epoch": 0.07239964604617488,
      "grad_norm": 0.921249508857727,
      "learning_rate": 0.00014841605663261202,
      "loss": 1.5548,
      "step": 900
    },
    {
      "epoch": 0.07642184860429571,
      "grad_norm": 1.0722730159759521,
      "learning_rate": 0.00014777250422331271,
      "loss": 1.5418,
      "step": 950
    },
    {
      "epoch": 0.08044405116241654,
      "grad_norm": 0.9145339727401733,
      "learning_rate": 0.00014712895181401338,
      "loss": 1.5463,
      "step": 1000
    },
    {
      "epoch": 0.08446625372053737,
      "grad_norm": 0.8664803504943848,
      "learning_rate": 0.00014648539940471402,
      "loss": 1.5334,
      "step": 1050
    },
    {
      "epoch": 0.08848845627865819,
      "grad_norm": 0.9050459861755371,
      "learning_rate": 0.0001458418469954147,
      "loss": 1.5237,
      "step": 1100
    },
    {
      "epoch": 0.09251065883677902,
      "grad_norm": 1.1388388872146606,
      "learning_rate": 0.00014519829458611538,
      "loss": 1.5066,
      "step": 1150
    },
    {
      "epoch": 0.09653286139489985,
      "grad_norm": 0.7738500833511353,
      "learning_rate": 0.00014455474217681604,
      "loss": 1.5004,
      "step": 1200
    },
    {
      "epoch": 0.10055506395302068,
      "grad_norm": 0.8031710982322693,
      "learning_rate": 0.0001439111897675167,
      "loss": 1.4921,
      "step": 1250
    },
    {
      "epoch": 0.1045772665111415,
      "grad_norm": 0.7259467840194702,
      "learning_rate": 0.00014326763735821737,
      "loss": 1.4736,
      "step": 1300
    },
    {
      "epoch": 0.10859946906926232,
      "grad_norm": 0.747370719909668,
      "learning_rate": 0.00014262408494891804,
      "loss": 1.4769,
      "step": 1350
    },
    {
      "epoch": 0.11262167162738315,
      "grad_norm": 0.8219165802001953,
      "learning_rate": 0.0001419805325396187,
      "loss": 1.4626,
      "step": 1400
    },
    {
      "epoch": 0.11664387418550398,
      "grad_norm": 0.7680050134658813,
      "learning_rate": 0.00014133698013031937,
      "loss": 1.4498,
      "step": 1450
    },
    {
      "epoch": 0.12066607674362481,
      "grad_norm": 0.8124447464942932,
      "learning_rate": 0.00014069342772102004,
      "loss": 1.4429,
      "step": 1500
    },
    {
      "epoch": 0.12468827930174564,
      "grad_norm": 0.736314594745636,
      "learning_rate": 0.0001400498753117207,
      "loss": 1.4347,
      "step": 1550
    },
    {
      "epoch": 0.12871048185986647,
      "grad_norm": 0.8213746547698975,
      "learning_rate": 0.00013940632290242137,
      "loss": 1.4343,
      "step": 1600
    },
    {
      "epoch": 0.1327326844179873,
      "grad_norm": 0.856047511100769,
      "learning_rate": 0.00013876277049312206,
      "loss": 1.4301,
      "step": 1650
    },
    {
      "epoch": 0.1367548869761081,
      "grad_norm": 0.7476931810379028,
      "learning_rate": 0.0001381192180838227,
      "loss": 1.4213,
      "step": 1700
    },
    {
      "epoch": 0.14077708953422893,
      "grad_norm": 0.7100381851196289,
      "learning_rate": 0.00013747566567452336,
      "loss": 1.4174,
      "step": 1750
    },
    {
      "epoch": 0.14479929209234976,
      "grad_norm": 0.7607312798500061,
      "learning_rate": 0.00013683211326522406,
      "loss": 1.4132,
      "step": 1800
    },
    {
      "epoch": 0.1488214946504706,
      "grad_norm": 0.7051347494125366,
      "learning_rate": 0.00013618856085592472,
      "loss": 1.4156,
      "step": 1850
    },
    {
      "epoch": 0.15284369720859142,
      "grad_norm": 0.7388345003128052,
      "learning_rate": 0.00013554500844662536,
      "loss": 1.4031,
      "step": 1900
    },
    {
      "epoch": 0.15686589976671225,
      "grad_norm": 0.6584649085998535,
      "learning_rate": 0.00013490145603732605,
      "loss": 1.4054,
      "step": 1950
    },
    {
      "epoch": 0.16088810232483308,
      "grad_norm": 1.212066888809204,
      "learning_rate": 0.00013425790362802672,
      "loss": 1.3947,
      "step": 2000
    },
    {
      "epoch": 0.1649103048829539,
      "grad_norm": 0.7469912171363831,
      "learning_rate": 0.00013361435121872739,
      "loss": 1.3887,
      "step": 2050
    },
    {
      "epoch": 0.16893250744107474,
      "grad_norm": 0.6744495630264282,
      "learning_rate": 0.00013297079880942805,
      "loss": 1.39,
      "step": 2100
    },
    {
      "epoch": 0.17295470999919557,
      "grad_norm": 0.6849004030227661,
      "learning_rate": 0.00013232724640012872,
      "loss": 1.3826,
      "step": 2150
    },
    {
      "epoch": 0.17697691255731637,
      "grad_norm": 0.7336851954460144,
      "learning_rate": 0.00013168369399082938,
      "loss": 1.3725,
      "step": 2200
    },
    {
      "epoch": 0.1809991151154372,
      "grad_norm": 0.8152992129325867,
      "learning_rate": 0.00013104014158153005,
      "loss": 1.3845,
      "step": 2250
    },
    {
      "epoch": 0.18502131767355803,
      "grad_norm": 0.6474448442459106,
      "learning_rate": 0.00013039658917223074,
      "loss": 1.3754,
      "step": 2300
    },
    {
      "epoch": 0.18904352023167886,
      "grad_norm": 4.631404399871826,
      "learning_rate": 0.00012975303676293138,
      "loss": 1.3811,
      "step": 2350
    },
    {
      "epoch": 0.1930657227897997,
      "grad_norm": 0.7269085645675659,
      "learning_rate": 0.00012910948435363204,
      "loss": 1.3654,
      "step": 2400
    },
    {
      "epoch": 0.19708792534792052,
      "grad_norm": 0.792264461517334,
      "learning_rate": 0.00012846593194433274,
      "loss": 1.3653,
      "step": 2450
    },
    {
      "epoch": 0.20111012790604135,
      "grad_norm": 0.6725736260414124,
      "learning_rate": 0.0001278223795350334,
      "loss": 1.3623,
      "step": 2500
    },
    {
      "epoch": 0.20513233046416218,
      "grad_norm": 0.716214120388031,
      "learning_rate": 0.00012717882712573407,
      "loss": 1.3599,
      "step": 2550
    },
    {
      "epoch": 0.209154533022283,
      "grad_norm": 1.1258844137191772,
      "learning_rate": 0.00012653527471643473,
      "loss": 1.3507,
      "step": 2600
    },
    {
      "epoch": 0.21317673558040384,
      "grad_norm": 0.8135349750518799,
      "learning_rate": 0.0001258917223071354,
      "loss": 1.3661,
      "step": 2650
    },
    {
      "epoch": 0.21719893813852464,
      "grad_norm": 0.5992360711097717,
      "learning_rate": 0.00012524816989783607,
      "loss": 1.352,
      "step": 2700
    },
    {
      "epoch": 0.22122114069664547,
      "grad_norm": 0.9940087199211121,
      "learning_rate": 0.00012460461748853673,
      "loss": 1.3475,
      "step": 2750
    },
    {
      "epoch": 0.2252433432547663,
      "grad_norm": 0.6403551697731018,
      "learning_rate": 0.0001239610650792374,
      "loss": 1.3435,
      "step": 2800
    },
    {
      "epoch": 0.22926554581288713,
      "grad_norm": 0.7335209846496582,
      "learning_rate": 0.00012331751266993806,
      "loss": 1.3432,
      "step": 2850
    },
    {
      "epoch": 0.23328774837100796,
      "grad_norm": 0.8032348155975342,
      "learning_rate": 0.00012267396026063873,
      "loss": 1.3484,
      "step": 2900
    },
    {
      "epoch": 0.2373099509291288,
      "grad_norm": 0.6810804605484009,
      "learning_rate": 0.00012203040785133941,
      "loss": 1.3401,
      "step": 2950
    },
    {
      "epoch": 0.24133215348724962,
      "grad_norm": 0.7474021315574646,
      "learning_rate": 0.00012138685544204007,
      "loss": 1.3394,
      "step": 3000
    },
    {
      "epoch": 0.24535435604537045,
      "grad_norm": 0.6659166812896729,
      "learning_rate": 0.00012074330303274074,
      "loss": 1.324,
      "step": 3050
    },
    {
      "epoch": 0.24937655860349128,
      "grad_norm": 0.6468557715415955,
      "learning_rate": 0.0001200997506234414,
      "loss": 1.3273,
      "step": 3100
    },
    {
      "epoch": 0.2533987611616121,
      "grad_norm": 0.6166044473648071,
      "learning_rate": 0.00011945619821414207,
      "loss": 1.3271,
      "step": 3150
    },
    {
      "epoch": 0.25742096371973294,
      "grad_norm": 0.6390374898910522,
      "learning_rate": 0.00011881264580484275,
      "loss": 1.3324,
      "step": 3200
    },
    {
      "epoch": 0.26144316627785374,
      "grad_norm": 0.6143361330032349,
      "learning_rate": 0.0001181690933955434,
      "loss": 1.3278,
      "step": 3250
    },
    {
      "epoch": 0.2654653688359746,
      "grad_norm": 0.5937938690185547,
      "learning_rate": 0.00011752554098624407,
      "loss": 1.3351,
      "step": 3300
    },
    {
      "epoch": 0.2694875713940954,
      "grad_norm": 0.6383444666862488,
      "learning_rate": 0.00011688198857694475,
      "loss": 1.3197,
      "step": 3350
    },
    {
      "epoch": 0.2735097739522162,
      "grad_norm": 0.5931671857833862,
      "learning_rate": 0.00011623843616764541,
      "loss": 1.3241,
      "step": 3400
    },
    {
      "epoch": 0.27753197651033706,
      "grad_norm": 0.65858393907547,
      "learning_rate": 0.00011559488375834609,
      "loss": 1.3144,
      "step": 3450
    },
    {
      "epoch": 0.28155417906845787,
      "grad_norm": 0.6355201601982117,
      "learning_rate": 0.00011495133134904674,
      "loss": 1.3135,
      "step": 3500
    },
    {
      "epoch": 0.2855763816265787,
      "grad_norm": 0.5866701006889343,
      "learning_rate": 0.00011430777893974741,
      "loss": 1.3125,
      "step": 3550
    },
    {
      "epoch": 0.2895985841846995,
      "grad_norm": 0.6678761839866638,
      "learning_rate": 0.00011366422653044809,
      "loss": 1.3186,
      "step": 3600
    },
    {
      "epoch": 0.2936207867428204,
      "grad_norm": 0.669615626335144,
      "learning_rate": 0.00011302067412114875,
      "loss": 1.3035,
      "step": 3650
    },
    {
      "epoch": 0.2976429893009412,
      "grad_norm": 0.6734874248504639,
      "learning_rate": 0.0001123771217118494,
      "loss": 1.3057,
      "step": 3700
    },
    {
      "epoch": 0.30166519185906204,
      "grad_norm": 0.5842022895812988,
      "learning_rate": 0.00011173356930255008,
      "loss": 1.308,
      "step": 3750
    },
    {
      "epoch": 0.30568739441718284,
      "grad_norm": 0.6986467838287354,
      "learning_rate": 0.00011109001689325075,
      "loss": 1.3,
      "step": 3800
    },
    {
      "epoch": 0.3097095969753037,
      "grad_norm": 0.6808900833129883,
      "learning_rate": 0.00011044646448395143,
      "loss": 1.3036,
      "step": 3850
    },
    {
      "epoch": 0.3137317995334245,
      "grad_norm": 0.592846691608429,
      "learning_rate": 0.0001098029120746521,
      "loss": 1.3072,
      "step": 3900
    },
    {
      "epoch": 0.3177540020915453,
      "grad_norm": 0.6984266638755798,
      "learning_rate": 0.00010915935966535275,
      "loss": 1.302,
      "step": 3950
    },
    {
      "epoch": 0.32177620464966616,
      "grad_norm": 0.6236177086830139,
      "learning_rate": 0.00010851580725605343,
      "loss": 1.2953,
      "step": 4000
    },
    {
      "epoch": 0.32579840720778697,
      "grad_norm": 0.6055262088775635,
      "learning_rate": 0.00010787225484675409,
      "loss": 1.2926,
      "step": 4050
    },
    {
      "epoch": 0.3298206097659078,
      "grad_norm": 0.5453222393989563,
      "learning_rate": 0.00010722870243745477,
      "loss": 1.2971,
      "step": 4100
    },
    {
      "epoch": 0.3338428123240286,
      "grad_norm": 0.5496383309364319,
      "learning_rate": 0.00010658515002815542,
      "loss": 1.2937,
      "step": 4150
    },
    {
      "epoch": 0.3378650148821495,
      "grad_norm": 0.588947594165802,
      "learning_rate": 0.00010594159761885609,
      "loss": 1.2902,
      "step": 4200
    },
    {
      "epoch": 0.3418872174402703,
      "grad_norm": 0.5392375588417053,
      "learning_rate": 0.00010529804520955677,
      "loss": 1.2885,
      "step": 4250
    },
    {
      "epoch": 0.34590941999839114,
      "grad_norm": 0.7034677863121033,
      "learning_rate": 0.00010465449280025743,
      "loss": 1.2826,
      "step": 4300
    },
    {
      "epoch": 0.34993162255651195,
      "grad_norm": 0.5785167217254639,
      "learning_rate": 0.00010401094039095809,
      "loss": 1.2877,
      "step": 4350
    },
    {
      "epoch": 0.35395382511463275,
      "grad_norm": 0.6118981838226318,
      "learning_rate": 0.00010336738798165876,
      "loss": 1.2934,
      "step": 4400
    },
    {
      "epoch": 0.3579760276727536,
      "grad_norm": 0.7435745000839233,
      "learning_rate": 0.00010272383557235943,
      "loss": 1.2818,
      "step": 4450
    },
    {
      "epoch": 0.3619982302308744,
      "grad_norm": 0.6845859289169312,
      "learning_rate": 0.00010208028316306011,
      "loss": 1.2827,
      "step": 4500
    },
    {
      "epoch": 0.36602043278899526,
      "grad_norm": 0.5946281552314758,
      "learning_rate": 0.00010143673075376078,
      "loss": 1.2861,
      "step": 4550
    },
    {
      "epoch": 0.37004263534711607,
      "grad_norm": 0.5468514561653137,
      "learning_rate": 0.00010079317834446143,
      "loss": 1.2796,
      "step": 4600
    },
    {
      "epoch": 0.3740648379052369,
      "grad_norm": 0.7021697759628296,
      "learning_rate": 0.0001001496259351621,
      "loss": 1.2804,
      "step": 4650
    },
    {
      "epoch": 0.3780870404633577,
      "grad_norm": 0.703152060508728,
      "learning_rate": 9.950607352586277e-05,
      "loss": 1.2793,
      "step": 4700
    },
    {
      "epoch": 0.3821092430214786,
      "grad_norm": 0.5662014484405518,
      "learning_rate": 9.886252111656344e-05,
      "loss": 1.2781,
      "step": 4750
    },
    {
      "epoch": 0.3861314455795994,
      "grad_norm": 0.6324970126152039,
      "learning_rate": 9.82189687072641e-05,
      "loss": 1.2791,
      "step": 4800
    },
    {
      "epoch": 0.3901536481377202,
      "grad_norm": 0.5823405385017395,
      "learning_rate": 9.757541629796477e-05,
      "loss": 1.275,
      "step": 4850
    },
    {
      "epoch": 0.39417585069584105,
      "grad_norm": 0.5228702425956726,
      "learning_rate": 9.693186388866543e-05,
      "loss": 1.2736,
      "step": 4900
    },
    {
      "epoch": 0.39819805325396185,
      "grad_norm": 0.7172161936759949,
      "learning_rate": 9.628831147936611e-05,
      "loss": 1.279,
      "step": 4950
    },
    {
      "epoch": 0.4022202558120827,
      "grad_norm": 0.5634454488754272,
      "learning_rate": 9.564475907006678e-05,
      "loss": 1.2655,
      "step": 5000
    },
    {
      "epoch": 0.4062424583702035,
      "grad_norm": 0.5482621192932129,
      "learning_rate": 9.500120666076743e-05,
      "loss": 1.267,
      "step": 5050
    },
    {
      "epoch": 0.41026466092832437,
      "grad_norm": 0.5723125338554382,
      "learning_rate": 9.435765425146811e-05,
      "loss": 1.273,
      "step": 5100
    },
    {
      "epoch": 0.41428686348644517,
      "grad_norm": 0.5198033452033997,
      "learning_rate": 9.371410184216878e-05,
      "loss": 1.2709,
      "step": 5150
    },
    {
      "epoch": 0.418309066044566,
      "grad_norm": 0.7251081466674805,
      "learning_rate": 9.307054943286946e-05,
      "loss": 1.2725,
      "step": 5200
    },
    {
      "epoch": 0.4223312686026868,
      "grad_norm": 0.5081143379211426,
      "learning_rate": 9.242699702357011e-05,
      "loss": 1.262,
      "step": 5250
    },
    {
      "epoch": 0.4263534711608077,
      "grad_norm": 0.5063687562942505,
      "learning_rate": 9.178344461427077e-05,
      "loss": 1.2641,
      "step": 5300
    },
    {
      "epoch": 0.4303756737189285,
      "grad_norm": 0.5551260113716125,
      "learning_rate": 9.113989220497145e-05,
      "loss": 1.2699,
      "step": 5350
    },
    {
      "epoch": 0.4343978762770493,
      "grad_norm": 0.6342585682868958,
      "learning_rate": 9.049633979567212e-05,
      "loss": 1.2651,
      "step": 5400
    },
    {
      "epoch": 0.43842007883517015,
      "grad_norm": 0.5168182849884033,
      "learning_rate": 8.98527873863728e-05,
      "loss": 1.2658,
      "step": 5450
    },
    {
      "epoch": 0.44244228139329095,
      "grad_norm": 0.5318729281425476,
      "learning_rate": 8.920923497707345e-05,
      "loss": 1.2712,
      "step": 5500
    },
    {
      "epoch": 0.4464644839514118,
      "grad_norm": 0.49420368671417236,
      "learning_rate": 8.856568256777412e-05,
      "loss": 1.2604,
      "step": 5550
    },
    {
      "epoch": 0.4504866865095326,
      "grad_norm": 0.7057091593742371,
      "learning_rate": 8.79221301584748e-05,
      "loss": 1.2597,
      "step": 5600
    },
    {
      "epoch": 0.45450888906765347,
      "grad_norm": 0.5921667218208313,
      "learning_rate": 8.727857774917546e-05,
      "loss": 1.2653,
      "step": 5650
    },
    {
      "epoch": 0.45853109162577427,
      "grad_norm": 0.5272806286811829,
      "learning_rate": 8.663502533987611e-05,
      "loss": 1.2575,
      "step": 5700
    },
    {
      "epoch": 0.4625532941838951,
      "grad_norm": 0.563353419303894,
      "learning_rate": 8.599147293057679e-05,
      "loss": 1.2583,
      "step": 5750
    },
    {
      "epoch": 0.4665754967420159,
      "grad_norm": 0.6450011134147644,
      "learning_rate": 8.534792052127746e-05,
      "loss": 1.2591,
      "step": 5800
    },
    {
      "epoch": 0.47059769930013673,
      "grad_norm": 0.5384729504585266,
      "learning_rate": 8.470436811197814e-05,
      "loss": 1.2561,
      "step": 5850
    },
    {
      "epoch": 0.4746199018582576,
      "grad_norm": 0.5148622989654541,
      "learning_rate": 8.40608157026788e-05,
      "loss": 1.2617,
      "step": 5900
    },
    {
      "epoch": 0.4786421044163784,
      "grad_norm": 0.546597957611084,
      "learning_rate": 8.341726329337945e-05,
      "loss": 1.2522,
      "step": 5950
    },
    {
      "epoch": 0.48266430697449925,
      "grad_norm": 0.5441663265228271,
      "learning_rate": 8.277371088408013e-05,
      "loss": 1.2605,
      "step": 6000
    },
    {
      "epoch": 0.48668650953262005,
      "grad_norm": 0.518447995185852,
      "learning_rate": 8.21301584747808e-05,
      "loss": 1.2541,
      "step": 6050
    },
    {
      "epoch": 0.4907087120907409,
      "grad_norm": 0.5107919573783875,
      "learning_rate": 8.148660606548146e-05,
      "loss": 1.252,
      "step": 6100
    },
    {
      "epoch": 0.4947309146488617,
      "grad_norm": 0.6154139637947083,
      "learning_rate": 8.084305365618213e-05,
      "loss": 1.2584,
      "step": 6150
    },
    {
      "epoch": 0.49875311720698257,
      "grad_norm": 0.6334192752838135,
      "learning_rate": 8.01995012468828e-05,
      "loss": 1.2501,
      "step": 6200
    },
    {
      "epoch": 0.5027753197651034,
      "grad_norm": 0.5377209782600403,
      "learning_rate": 7.955594883758346e-05,
      "loss": 1.2525,
      "step": 6250
    },
    {
      "epoch": 0.5067975223232242,
      "grad_norm": 0.5803179740905762,
      "learning_rate": 7.891239642828414e-05,
      "loss": 1.2556,
      "step": 6300
    },
    {
      "epoch": 0.510819724881345,
      "grad_norm": 0.513047993183136,
      "learning_rate": 7.82688440189848e-05,
      "loss": 1.2551,
      "step": 6350
    },
    {
      "epoch": 0.5148419274394659,
      "grad_norm": 0.5425321459770203,
      "learning_rate": 7.762529160968547e-05,
      "loss": 1.2554,
      "step": 6400
    },
    {
      "epoch": 0.5188641299975867,
      "grad_norm": 0.5897896885871887,
      "learning_rate": 7.698173920038614e-05,
      "loss": 1.2497,
      "step": 6450
    },
    {
      "epoch": 0.5228863325557075,
      "grad_norm": 0.4902781546115875,
      "learning_rate": 7.63381867910868e-05,
      "loss": 1.251,
      "step": 6500
    },
    {
      "epoch": 0.5269085351138283,
      "grad_norm": 0.5771247744560242,
      "learning_rate": 7.569463438178747e-05,
      "loss": 1.2537,
      "step": 6550
    },
    {
      "epoch": 0.5309307376719492,
      "grad_norm": 0.4801037609577179,
      "learning_rate": 7.505108197248815e-05,
      "loss": 1.2389,
      "step": 6600
    },
    {
      "epoch": 0.53495294023007,
      "grad_norm": 0.5114966630935669,
      "learning_rate": 7.44075295631888e-05,
      "loss": 1.2469,
      "step": 6650
    },
    {
      "epoch": 0.5389751427881908,
      "grad_norm": 0.5111217498779297,
      "learning_rate": 7.376397715388948e-05,
      "loss": 1.2429,
      "step": 6700
    },
    {
      "epoch": 0.5429973453463116,
      "grad_norm": 0.5359189510345459,
      "learning_rate": 7.312042474459014e-05,
      "loss": 1.2452,
      "step": 6750
    },
    {
      "epoch": 0.5470195479044324,
      "grad_norm": 0.5202511548995972,
      "learning_rate": 7.247687233529081e-05,
      "loss": 1.2417,
      "step": 6800
    },
    {
      "epoch": 0.5510417504625533,
      "grad_norm": 0.5288210511207581,
      "learning_rate": 7.183331992599149e-05,
      "loss": 1.2435,
      "step": 6850
    },
    {
      "epoch": 0.5550639530206741,
      "grad_norm": 0.5215522646903992,
      "learning_rate": 7.118976751669214e-05,
      "loss": 1.2467,
      "step": 6900
    },
    {
      "epoch": 0.5590861555787949,
      "grad_norm": 0.5165191292762756,
      "learning_rate": 7.054621510739282e-05,
      "loss": 1.2388,
      "step": 6950
    },
    {
      "epoch": 0.5631083581369157,
      "grad_norm": 0.4970744848251343,
      "learning_rate": 6.990266269809349e-05,
      "loss": 1.2406,
      "step": 7000
    },
    {
      "epoch": 0.5671305606950366,
      "grad_norm": 0.5136338472366333,
      "learning_rate": 6.925911028879415e-05,
      "loss": 1.2395,
      "step": 7050
    },
    {
      "epoch": 0.5711527632531574,
      "grad_norm": 0.4894406199455261,
      "learning_rate": 6.861555787949482e-05,
      "loss": 1.2397,
      "step": 7100
    },
    {
      "epoch": 0.5751749658112782,
      "grad_norm": 0.5584831833839417,
      "learning_rate": 6.797200547019548e-05,
      "loss": 1.2378,
      "step": 7150
    },
    {
      "epoch": 0.579197168369399,
      "grad_norm": 0.6358110308647156,
      "learning_rate": 6.732845306089615e-05,
      "loss": 1.2381,
      "step": 7200
    },
    {
      "epoch": 0.58321937092752,
      "grad_norm": 0.5877314805984497,
      "learning_rate": 6.668490065159681e-05,
      "loss": 1.2398,
      "step": 7250
    },
    {
      "epoch": 0.5872415734856408,
      "grad_norm": 0.5011232495307922,
      "learning_rate": 6.60413482422975e-05,
      "loss": 1.2322,
      "step": 7300
    },
    {
      "epoch": 0.5912637760437616,
      "grad_norm": 0.4616398513317108,
      "learning_rate": 6.539779583299816e-05,
      "loss": 1.2374,
      "step": 7350
    },
    {
      "epoch": 0.5952859786018824,
      "grad_norm": 0.5188731551170349,
      "learning_rate": 6.475424342369882e-05,
      "loss": 1.2397,
      "step": 7400
    },
    {
      "epoch": 0.5993081811600032,
      "grad_norm": 0.5429155826568604,
      "learning_rate": 6.411069101439949e-05,
      "loss": 1.2389,
      "step": 7450
    },
    {
      "epoch": 0.6033303837181241,
      "grad_norm": 0.5140334963798523,
      "learning_rate": 6.346713860510016e-05,
      "loss": 1.2332,
      "step": 7500
    },
    {
      "epoch": 0.6073525862762449,
      "grad_norm": 0.49409759044647217,
      "learning_rate": 6.282358619580082e-05,
      "loss": 1.2338,
      "step": 7550
    },
    {
      "epoch": 0.6113747888343657,
      "grad_norm": 0.5355201363563538,
      "learning_rate": 6.21800337865015e-05,
      "loss": 1.2354,
      "step": 7600
    },
    {
      "epoch": 0.6153969913924865,
      "grad_norm": 0.5299375653266907,
      "learning_rate": 6.153648137720215e-05,
      "loss": 1.2321,
      "step": 7650
    },
    {
      "epoch": 0.6194191939506074,
      "grad_norm": 0.5406738519668579,
      "learning_rate": 6.089292896790283e-05,
      "loss": 1.2358,
      "step": 7700
    },
    {
      "epoch": 0.6234413965087282,
      "grad_norm": 0.5010151863098145,
      "learning_rate": 6.024937655860349e-05,
      "loss": 1.2373,
      "step": 7750
    },
    {
      "epoch": 0.627463599066849,
      "grad_norm": 0.5196851491928101,
      "learning_rate": 5.960582414930416e-05,
      "loss": 1.2222,
      "step": 7800
    },
    {
      "epoch": 0.6314858016249698,
      "grad_norm": 0.5198109149932861,
      "learning_rate": 5.8962271740004836e-05,
      "loss": 1.2304,
      "step": 7850
    },
    {
      "epoch": 0.6355080041830906,
      "grad_norm": 0.52145916223526,
      "learning_rate": 5.8318719330705495e-05,
      "loss": 1.2297,
      "step": 7900
    },
    {
      "epoch": 0.6395302067412115,
      "grad_norm": 0.5913663506507874,
      "learning_rate": 5.767516692140617e-05,
      "loss": 1.2284,
      "step": 7950
    },
    {
      "epoch": 0.6435524092993323,
      "grad_norm": 0.5714961290359497,
      "learning_rate": 5.703161451210683e-05,
      "loss": 1.2295,
      "step": 8000
    },
    {
      "epoch": 0.6475746118574531,
      "grad_norm": 0.4882732927799225,
      "learning_rate": 5.6388062102807505e-05,
      "loss": 1.2327,
      "step": 8050
    },
    {
      "epoch": 0.6515968144155739,
      "grad_norm": 0.5473686456680298,
      "learning_rate": 5.5744509693508164e-05,
      "loss": 1.2317,
      "step": 8100
    },
    {
      "epoch": 0.6556190169736948,
      "grad_norm": 0.5099277496337891,
      "learning_rate": 5.5100957284208836e-05,
      "loss": 1.2253,
      "step": 8150
    },
    {
      "epoch": 0.6596412195318156,
      "grad_norm": 0.47601258754730225,
      "learning_rate": 5.44574048749095e-05,
      "loss": 1.2312,
      "step": 8200
    },
    {
      "epoch": 0.6636634220899365,
      "grad_norm": 0.5656133890151978,
      "learning_rate": 5.3813852465610174e-05,
      "loss": 1.2233,
      "step": 8250
    },
    {
      "epoch": 0.6676856246480573,
      "grad_norm": 0.5101476311683655,
      "learning_rate": 5.317030005631085e-05,
      "loss": 1.2279,
      "step": 8300
    },
    {
      "epoch": 0.671707827206178,
      "grad_norm": 0.48101550340652466,
      "learning_rate": 5.2526747647011506e-05,
      "loss": 1.2284,
      "step": 8350
    },
    {
      "epoch": 0.675730029764299,
      "grad_norm": 0.4687703251838684,
      "learning_rate": 5.188319523771218e-05,
      "loss": 1.2278,
      "step": 8400
    },
    {
      "epoch": 0.6797522323224198,
      "grad_norm": 0.4390990138053894,
      "learning_rate": 5.1239642828412844e-05,
      "loss": 1.229,
      "step": 8450
    },
    {
      "epoch": 0.6837744348805406,
      "grad_norm": 0.4652140736579895,
      "learning_rate": 5.0596090419113516e-05,
      "loss": 1.2249,
      "step": 8500
    },
    {
      "epoch": 0.6877966374386614,
      "grad_norm": 0.485411137342453,
      "learning_rate": 4.9952538009814175e-05,
      "loss": 1.224,
      "step": 8550
    },
    {
      "epoch": 0.6918188399967823,
      "grad_norm": 0.5163487792015076,
      "learning_rate": 4.930898560051485e-05,
      "loss": 1.2178,
      "step": 8600
    },
    {
      "epoch": 0.6958410425549031,
      "grad_norm": 0.48162707686424255,
      "learning_rate": 4.866543319121551e-05,
      "loss": 1.218,
      "step": 8650
    },
    {
      "epoch": 0.6998632451130239,
      "grad_norm": 0.473122775554657,
      "learning_rate": 4.802188078191618e-05,
      "loss": 1.2228,
      "step": 8700
    },
    {
      "epoch": 0.7038854476711447,
      "grad_norm": 1.080450415611267,
      "learning_rate": 4.737832837261685e-05,
      "loss": 1.2176,
      "step": 8750
    },
    {
      "epoch": 0.7079076502292655,
      "grad_norm": 0.47382307052612305,
      "learning_rate": 4.6734775963317517e-05,
      "loss": 1.2212,
      "step": 8800
    },
    {
      "epoch": 0.7119298527873864,
      "grad_norm": 0.5250477194786072,
      "learning_rate": 4.609122355401819e-05,
      "loss": 1.2192,
      "step": 8850
    },
    {
      "epoch": 0.7159520553455072,
      "grad_norm": 0.476870596408844,
      "learning_rate": 4.544767114471885e-05,
      "loss": 1.219,
      "step": 8900
    },
    {
      "epoch": 0.719974257903628,
      "grad_norm": 0.48875513672828674,
      "learning_rate": 4.480411873541952e-05,
      "loss": 1.2201,
      "step": 8950
    },
    {
      "epoch": 0.7239964604617488,
      "grad_norm": 0.5518898963928223,
      "learning_rate": 4.4160566326120186e-05,
      "loss": 1.2171,
      "step": 9000
    },
    {
      "epoch": 0.7280186630198697,
      "grad_norm": 0.6373410224914551,
      "learning_rate": 4.351701391682086e-05,
      "loss": 1.2193,
      "step": 9050
    },
    {
      "epoch": 0.7320408655779905,
      "grad_norm": 0.5379607081413269,
      "learning_rate": 4.287346150752152e-05,
      "loss": 1.2221,
      "step": 9100
    },
    {
      "epoch": 0.7360630681361113,
      "grad_norm": 0.46999892592430115,
      "learning_rate": 4.222990909822219e-05,
      "loss": 1.221,
      "step": 9150
    },
    {
      "epoch": 0.7400852706942321,
      "grad_norm": 0.5113339424133301,
      "learning_rate": 4.1586356688922855e-05,
      "loss": 1.2183,
      "step": 9200
    },
    {
      "epoch": 0.7441074732523529,
      "grad_norm": 0.49148696660995483,
      "learning_rate": 4.094280427962353e-05,
      "loss": 1.2191,
      "step": 9250
    },
    {
      "epoch": 0.7481296758104738,
      "grad_norm": 0.5605328679084778,
      "learning_rate": 4.029925187032419e-05,
      "loss": 1.2213,
      "step": 9300
    },
    {
      "epoch": 0.7521518783685947,
      "grad_norm": 0.5597246289253235,
      "learning_rate": 3.965569946102486e-05,
      "loss": 1.2181,
      "step": 9350
    },
    {
      "epoch": 0.7561740809267155,
      "grad_norm": 0.49881938099861145,
      "learning_rate": 3.9012147051725524e-05,
      "loss": 1.2133,
      "step": 9400
    },
    {
      "epoch": 0.7601962834848363,
      "grad_norm": 0.47615835070610046,
      "learning_rate": 3.83685946424262e-05,
      "loss": 1.214,
      "step": 9450
    },
    {
      "epoch": 0.7642184860429572,
      "grad_norm": 0.4723599851131439,
      "learning_rate": 3.772504223312686e-05,
      "loss": 1.22,
      "step": 9500
    },
    {
      "epoch": 0.768240688601078,
      "grad_norm": 0.5380647778511047,
      "learning_rate": 3.7081489823827535e-05,
      "loss": 1.2219,
      "step": 9550
    },
    {
      "epoch": 0.7722628911591988,
      "grad_norm": 0.5490273833274841,
      "learning_rate": 3.64379374145282e-05,
      "loss": 1.2106,
      "step": 9600
    },
    {
      "epoch": 0.7762850937173196,
      "grad_norm": 0.48823055624961853,
      "learning_rate": 3.5794385005228866e-05,
      "loss": 1.219,
      "step": 9650
    },
    {
      "epoch": 0.7803072962754404,
      "grad_norm": 0.4809708297252655,
      "learning_rate": 3.515083259592953e-05,
      "loss": 1.21,
      "step": 9700
    },
    {
      "epoch": 0.7843294988335613,
      "grad_norm": 0.4300834834575653,
      "learning_rate": 3.4507280186630204e-05,
      "loss": 1.215,
      "step": 9750
    },
    {
      "epoch": 0.7883517013916821,
      "grad_norm": 0.476905882358551,
      "learning_rate": 3.386372777733087e-05,
      "loss": 1.2213,
      "step": 9800
    },
    {
      "epoch": 0.7923739039498029,
      "grad_norm": 0.47577306628227234,
      "learning_rate": 3.3220175368031535e-05,
      "loss": 1.2155,
      "step": 9850
    },
    {
      "epoch": 0.7963961065079237,
      "grad_norm": 0.4941977858543396,
      "learning_rate": 3.25766229587322e-05,
      "loss": 1.2144,
      "step": 9900
    },
    {
      "epoch": 0.8004183090660446,
      "grad_norm": 0.4474700093269348,
      "learning_rate": 3.1933070549432874e-05,
      "loss": 1.2132,
      "step": 9950
    },
    {
      "epoch": 0.8044405116241654,
      "grad_norm": 0.46527743339538574,
      "learning_rate": 3.128951814013354e-05,
      "loss": 1.2191,
      "step": 10000
    }
  ],
  "logging_steps": 50,
  "max_steps": 12431,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 5000,
  "total_flos": 1.43841558528e+17,
  "train_batch_size": 20,
  "trial_name": null,
  "trial_params": null
}
