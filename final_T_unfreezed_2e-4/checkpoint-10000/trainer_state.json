{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9653441451877595,
  "eval_steps": 20000,
  "global_step": 10000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004826720725938797,
      "grad_norm": 1.5158735513687134,
      "learning_rate": 0.00019903465585481225,
      "loss": 10.0793,
      "step": 50
    },
    {
      "epoch": 0.009653441451877595,
      "grad_norm": 2.4133636951446533,
      "learning_rate": 0.0001980693117096245,
      "loss": 5.3058,
      "step": 100
    },
    {
      "epoch": 0.014480162177816391,
      "grad_norm": 0.8892596960067749,
      "learning_rate": 0.00019710396756443673,
      "loss": 4.9863,
      "step": 150
    },
    {
      "epoch": 0.01930688290375519,
      "grad_norm": 1.0456712245941162,
      "learning_rate": 0.00019613862341924897,
      "loss": 4.725,
      "step": 200
    },
    {
      "epoch": 0.024133603629693984,
      "grad_norm": 0.909625232219696,
      "learning_rate": 0.00019517327927406122,
      "loss": 4.5896,
      "step": 250
    },
    {
      "epoch": 0.028960324355632783,
      "grad_norm": 0.8399307131767273,
      "learning_rate": 0.00019420793512887346,
      "loss": 4.4916,
      "step": 300
    },
    {
      "epoch": 0.03378704508157158,
      "grad_norm": 0.8643233180046082,
      "learning_rate": 0.0001932425909836857,
      "loss": 4.4332,
      "step": 350
    },
    {
      "epoch": 0.03861376580751038,
      "grad_norm": 1.1824326515197754,
      "learning_rate": 0.0001922772468384979,
      "loss": 4.369,
      "step": 400
    },
    {
      "epoch": 0.043440486533449174,
      "grad_norm": 1.003928780555725,
      "learning_rate": 0.00019131190269331015,
      "loss": 4.352,
      "step": 450
    },
    {
      "epoch": 0.04826720725938797,
      "grad_norm": 0.8359873294830322,
      "learning_rate": 0.00019034655854812242,
      "loss": 4.3102,
      "step": 500
    },
    {
      "epoch": 0.05309392798532677,
      "grad_norm": 0.8316652178764343,
      "learning_rate": 0.00018938121440293466,
      "loss": 4.2827,
      "step": 550
    },
    {
      "epoch": 0.057920648711265565,
      "grad_norm": 0.9698308110237122,
      "learning_rate": 0.0001884158702577469,
      "loss": 4.2891,
      "step": 600
    },
    {
      "epoch": 0.06274736943720437,
      "grad_norm": 0.8544343709945679,
      "learning_rate": 0.00018745052611255912,
      "loss": 4.2473,
      "step": 650
    },
    {
      "epoch": 0.06757409016314316,
      "grad_norm": 0.7757757902145386,
      "learning_rate": 0.00018648518196737136,
      "loss": 4.2197,
      "step": 700
    },
    {
      "epoch": 0.07240081088908196,
      "grad_norm": 0.7996542453765869,
      "learning_rate": 0.0001855198378221836,
      "loss": 4.1944,
      "step": 750
    },
    {
      "epoch": 0.07722753161502076,
      "grad_norm": 0.8920103907585144,
      "learning_rate": 0.00018455449367699587,
      "loss": 4.1747,
      "step": 800
    },
    {
      "epoch": 0.08205425234095955,
      "grad_norm": 0.7815591096878052,
      "learning_rate": 0.0001835891495318081,
      "loss": 4.1728,
      "step": 850
    },
    {
      "epoch": 0.08688097306689835,
      "grad_norm": 0.8062090277671814,
      "learning_rate": 0.00018262380538662033,
      "loss": 4.1551,
      "step": 900
    },
    {
      "epoch": 0.09170769379283715,
      "grad_norm": 0.8033649921417236,
      "learning_rate": 0.00018165846124143257,
      "loss": 4.1512,
      "step": 950
    },
    {
      "epoch": 0.09653441451877594,
      "grad_norm": 0.8324350714683533,
      "learning_rate": 0.0001806931170962448,
      "loss": 4.1656,
      "step": 1000
    },
    {
      "epoch": 0.10136113524471474,
      "grad_norm": 0.8897930979728699,
      "learning_rate": 0.00017972777295105705,
      "loss": 4.124,
      "step": 1050
    },
    {
      "epoch": 0.10618785597065354,
      "grad_norm": 0.8129699230194092,
      "learning_rate": 0.00017876242880586932,
      "loss": 4.1188,
      "step": 1100
    },
    {
      "epoch": 0.11101457669659233,
      "grad_norm": 0.7739059329032898,
      "learning_rate": 0.00017779708466068153,
      "loss": 4.1178,
      "step": 1150
    },
    {
      "epoch": 0.11584129742253113,
      "grad_norm": 0.8025767803192139,
      "learning_rate": 0.00017683174051549377,
      "loss": 4.1035,
      "step": 1200
    },
    {
      "epoch": 0.12066801814846993,
      "grad_norm": 0.7473998665809631,
      "learning_rate": 0.00017586639637030602,
      "loss": 4.0747,
      "step": 1250
    },
    {
      "epoch": 0.12549473887440873,
      "grad_norm": 0.8450676798820496,
      "learning_rate": 0.00017490105222511826,
      "loss": 4.1007,
      "step": 1300
    },
    {
      "epoch": 0.13032145960034752,
      "grad_norm": 0.754374086856842,
      "learning_rate": 0.0001739357080799305,
      "loss": 4.0865,
      "step": 1350
    },
    {
      "epoch": 0.1351481803262863,
      "grad_norm": 0.7713772654533386,
      "learning_rate": 0.00017297036393474274,
      "loss": 4.0619,
      "step": 1400
    },
    {
      "epoch": 0.13997490105222513,
      "grad_norm": 0.8080806136131287,
      "learning_rate": 0.00017200501978955498,
      "loss": 4.0491,
      "step": 1450
    },
    {
      "epoch": 0.1448016217781639,
      "grad_norm": 0.8060615062713623,
      "learning_rate": 0.00017103967564436722,
      "loss": 4.0651,
      "step": 1500
    },
    {
      "epoch": 0.1496283425041027,
      "grad_norm": 0.7606507539749146,
      "learning_rate": 0.00017007433149917946,
      "loss": 4.0607,
      "step": 1550
    },
    {
      "epoch": 0.15445506323004152,
      "grad_norm": 0.7602542042732239,
      "learning_rate": 0.0001691089873539917,
      "loss": 4.0649,
      "step": 1600
    },
    {
      "epoch": 0.1592817839559803,
      "grad_norm": 0.8148393630981445,
      "learning_rate": 0.00016814364320880395,
      "loss": 4.0531,
      "step": 1650
    },
    {
      "epoch": 0.1641085046819191,
      "grad_norm": 0.767332911491394,
      "learning_rate": 0.0001671782990636162,
      "loss": 4.0313,
      "step": 1700
    },
    {
      "epoch": 0.1689352254078579,
      "grad_norm": 0.7823126912117004,
      "learning_rate": 0.00016621295491842843,
      "loss": 4.0424,
      "step": 1750
    },
    {
      "epoch": 0.1737619461337967,
      "grad_norm": 0.7575879693031311,
      "learning_rate": 0.00016524761077324067,
      "loss": 4.0265,
      "step": 1800
    },
    {
      "epoch": 0.17858866685973548,
      "grad_norm": 0.7901566624641418,
      "learning_rate": 0.0001642822666280529,
      "loss": 4.0247,
      "step": 1850
    },
    {
      "epoch": 0.1834153875856743,
      "grad_norm": 0.7711377739906311,
      "learning_rate": 0.00016331692248286515,
      "loss": 4.0367,
      "step": 1900
    },
    {
      "epoch": 0.1882421083116131,
      "grad_norm": 0.7391073107719421,
      "learning_rate": 0.00016235157833767737,
      "loss": 4.0335,
      "step": 1950
    },
    {
      "epoch": 0.19306882903755188,
      "grad_norm": 0.7553713917732239,
      "learning_rate": 0.00016138623419248964,
      "loss": 4.0231,
      "step": 2000
    },
    {
      "epoch": 0.1978955497634907,
      "grad_norm": 0.7356720566749573,
      "learning_rate": 0.00016042089004730188,
      "loss": 4.0278,
      "step": 2050
    },
    {
      "epoch": 0.20272227048942948,
      "grad_norm": 0.7669225335121155,
      "learning_rate": 0.00015945554590211412,
      "loss": 4.0047,
      "step": 2100
    },
    {
      "epoch": 0.20754899121536827,
      "grad_norm": 0.7737132906913757,
      "learning_rate": 0.00015849020175692636,
      "loss": 4.0142,
      "step": 2150
    },
    {
      "epoch": 0.21237571194130708,
      "grad_norm": 0.7526806592941284,
      "learning_rate": 0.00015752485761173857,
      "loss": 3.9924,
      "step": 2200
    },
    {
      "epoch": 0.21720243266724587,
      "grad_norm": 0.7942695617675781,
      "learning_rate": 0.00015655951346655082,
      "loss": 4.0051,
      "step": 2250
    },
    {
      "epoch": 0.22202915339318466,
      "grad_norm": 0.7344124913215637,
      "learning_rate": 0.00015559416932136308,
      "loss": 4.0098,
      "step": 2300
    },
    {
      "epoch": 0.22685587411912347,
      "grad_norm": 0.755492091178894,
      "learning_rate": 0.00015462882517617533,
      "loss": 3.976,
      "step": 2350
    },
    {
      "epoch": 0.23168259484506226,
      "grad_norm": 0.7205665707588196,
      "learning_rate": 0.00015366348103098757,
      "loss": 4.0167,
      "step": 2400
    },
    {
      "epoch": 0.23650931557100105,
      "grad_norm": 1.2099813222885132,
      "learning_rate": 0.00015269813688579978,
      "loss": 3.9991,
      "step": 2450
    },
    {
      "epoch": 0.24133603629693987,
      "grad_norm": 0.7569350600242615,
      "learning_rate": 0.00015173279274061202,
      "loss": 4.0032,
      "step": 2500
    },
    {
      "epoch": 0.24616275702287865,
      "grad_norm": 0.7308674454689026,
      "learning_rate": 0.00015076744859542426,
      "loss": 3.9501,
      "step": 2550
    },
    {
      "epoch": 0.25098947774881747,
      "grad_norm": 0.7535284161567688,
      "learning_rate": 0.00014980210445023653,
      "loss": 3.9805,
      "step": 2600
    },
    {
      "epoch": 0.25581619847475623,
      "grad_norm": 0.6991767883300781,
      "learning_rate": 0.00014883676030504877,
      "loss": 3.9815,
      "step": 2650
    },
    {
      "epoch": 0.26064291920069504,
      "grad_norm": 0.7177867889404297,
      "learning_rate": 0.000147871416159861,
      "loss": 3.9675,
      "step": 2700
    },
    {
      "epoch": 0.26546963992663386,
      "grad_norm": 0.9342032074928284,
      "learning_rate": 0.00014690607201467323,
      "loss": 3.9879,
      "step": 2750
    },
    {
      "epoch": 0.2702963606525726,
      "grad_norm": 0.7667758464813232,
      "learning_rate": 0.00014594072786948547,
      "loss": 3.9568,
      "step": 2800
    },
    {
      "epoch": 0.27512308137851144,
      "grad_norm": 0.7430959939956665,
      "learning_rate": 0.0001449753837242977,
      "loss": 3.9711,
      "step": 2850
    },
    {
      "epoch": 0.27994980210445025,
      "grad_norm": 0.7511558532714844,
      "learning_rate": 0.00014401003957910998,
      "loss": 3.9509,
      "step": 2900
    },
    {
      "epoch": 0.284776522830389,
      "grad_norm": 0.6978034377098083,
      "learning_rate": 0.0001430446954339222,
      "loss": 3.9493,
      "step": 2950
    },
    {
      "epoch": 0.2896032435563278,
      "grad_norm": 0.7173509001731873,
      "learning_rate": 0.00014207935128873444,
      "loss": 3.977,
      "step": 3000
    },
    {
      "epoch": 0.29442996428226664,
      "grad_norm": 0.7396991848945618,
      "learning_rate": 0.00014111400714354668,
      "loss": 3.9323,
      "step": 3050
    },
    {
      "epoch": 0.2992566850082054,
      "grad_norm": 0.7496536374092102,
      "learning_rate": 0.00014014866299835892,
      "loss": 3.9466,
      "step": 3100
    },
    {
      "epoch": 0.3040834057341442,
      "grad_norm": 0.7204744815826416,
      "learning_rate": 0.00013918331885317116,
      "loss": 3.9445,
      "step": 3150
    },
    {
      "epoch": 0.30891012646008303,
      "grad_norm": 0.7677027583122253,
      "learning_rate": 0.0001382179747079834,
      "loss": 3.9466,
      "step": 3200
    },
    {
      "epoch": 0.3137368471860218,
      "grad_norm": 0.7137256264686584,
      "learning_rate": 0.00013725263056279564,
      "loss": 3.9483,
      "step": 3250
    },
    {
      "epoch": 0.3185635679119606,
      "grad_norm": 0.7099571824073792,
      "learning_rate": 0.00013628728641760788,
      "loss": 3.9529,
      "step": 3300
    },
    {
      "epoch": 0.3233902886378994,
      "grad_norm": 0.6993565559387207,
      "learning_rate": 0.00013532194227242013,
      "loss": 3.9157,
      "step": 3350
    },
    {
      "epoch": 0.3282170093638382,
      "grad_norm": 0.714036226272583,
      "learning_rate": 0.00013435659812723237,
      "loss": 3.9234,
      "step": 3400
    },
    {
      "epoch": 0.333043730089777,
      "grad_norm": 0.6913496255874634,
      "learning_rate": 0.0001333912539820446,
      "loss": 3.9372,
      "step": 3450
    },
    {
      "epoch": 0.3378704508157158,
      "grad_norm": 0.7390172481536865,
      "learning_rate": 0.00013242590983685685,
      "loss": 3.921,
      "step": 3500
    },
    {
      "epoch": 0.3426971715416546,
      "grad_norm": 0.7202169299125671,
      "learning_rate": 0.0001314605656916691,
      "loss": 3.9016,
      "step": 3550
    },
    {
      "epoch": 0.3475238922675934,
      "grad_norm": 0.7623032927513123,
      "learning_rate": 0.00013049522154648133,
      "loss": 3.9064,
      "step": 3600
    },
    {
      "epoch": 0.3523506129935322,
      "grad_norm": 0.7060894966125488,
      "learning_rate": 0.00012952987740129357,
      "loss": 3.9343,
      "step": 3650
    },
    {
      "epoch": 0.35717733371947097,
      "grad_norm": 0.7048161029815674,
      "learning_rate": 0.00012856453325610581,
      "loss": 3.9267,
      "step": 3700
    },
    {
      "epoch": 0.3620040544454098,
      "grad_norm": 0.6945081353187561,
      "learning_rate": 0.00012759918911091803,
      "loss": 3.9337,
      "step": 3750
    },
    {
      "epoch": 0.3668307751713486,
      "grad_norm": 0.690873384475708,
      "learning_rate": 0.0001266338449657303,
      "loss": 3.9313,
      "step": 3800
    },
    {
      "epoch": 0.37165749589728736,
      "grad_norm": 0.7295247316360474,
      "learning_rate": 0.00012566850082054254,
      "loss": 3.9232,
      "step": 3850
    },
    {
      "epoch": 0.3764842166232262,
      "grad_norm": 0.7360806465148926,
      "learning_rate": 0.00012470315667535478,
      "loss": 3.9147,
      "step": 3900
    },
    {
      "epoch": 0.381310937349165,
      "grad_norm": 0.7044845819473267,
      "learning_rate": 0.000123737812530167,
      "loss": 3.9398,
      "step": 3950
    },
    {
      "epoch": 0.38613765807510375,
      "grad_norm": 0.7655064463615417,
      "learning_rate": 0.00012277246838497924,
      "loss": 3.925,
      "step": 4000
    },
    {
      "epoch": 0.39096437880104257,
      "grad_norm": 0.7278352379798889,
      "learning_rate": 0.00012180712423979148,
      "loss": 3.9183,
      "step": 4050
    },
    {
      "epoch": 0.3957910995269814,
      "grad_norm": 0.7127107381820679,
      "learning_rate": 0.00012084178009460375,
      "loss": 3.906,
      "step": 4100
    },
    {
      "epoch": 0.40061782025292014,
      "grad_norm": 0.6868242025375366,
      "learning_rate": 0.00011987643594941597,
      "loss": 3.9007,
      "step": 4150
    },
    {
      "epoch": 0.40544454097885896,
      "grad_norm": 0.6835846304893494,
      "learning_rate": 0.00011891109180422821,
      "loss": 3.91,
      "step": 4200
    },
    {
      "epoch": 0.4102712617047978,
      "grad_norm": 0.6901386380195618,
      "learning_rate": 0.00011794574765904046,
      "loss": 3.9208,
      "step": 4250
    },
    {
      "epoch": 0.41509798243073653,
      "grad_norm": 0.6988122463226318,
      "learning_rate": 0.00011698040351385268,
      "loss": 3.9241,
      "step": 4300
    },
    {
      "epoch": 0.41992470315667535,
      "grad_norm": 0.7126580476760864,
      "learning_rate": 0.00011601505936866492,
      "loss": 3.927,
      "step": 4350
    },
    {
      "epoch": 0.42475142388261417,
      "grad_norm": 0.6922239661216736,
      "learning_rate": 0.00011504971522347718,
      "loss": 3.902,
      "step": 4400
    },
    {
      "epoch": 0.4295781446085529,
      "grad_norm": 0.6854774951934814,
      "learning_rate": 0.00011408437107828942,
      "loss": 3.9034,
      "step": 4450
    },
    {
      "epoch": 0.43440486533449174,
      "grad_norm": 0.7308619022369385,
      "learning_rate": 0.00011311902693310166,
      "loss": 3.8928,
      "step": 4500
    },
    {
      "epoch": 0.43923158606043056,
      "grad_norm": 0.7102764844894409,
      "learning_rate": 0.00011215368278791389,
      "loss": 3.913,
      "step": 4550
    },
    {
      "epoch": 0.4440583067863693,
      "grad_norm": 0.6939714550971985,
      "learning_rate": 0.00011118833864272613,
      "loss": 3.9256,
      "step": 4600
    },
    {
      "epoch": 0.44888502751230813,
      "grad_norm": 0.6912333369255066,
      "learning_rate": 0.00011022299449753837,
      "loss": 3.8958,
      "step": 4650
    },
    {
      "epoch": 0.45371174823824695,
      "grad_norm": 0.6989474296569824,
      "learning_rate": 0.00010925765035235063,
      "loss": 3.8896,
      "step": 4700
    },
    {
      "epoch": 0.4585384689641857,
      "grad_norm": 0.7532591819763184,
      "learning_rate": 0.00010829230620716287,
      "loss": 3.8881,
      "step": 4750
    },
    {
      "epoch": 0.4633651896901245,
      "grad_norm": 0.674447238445282,
      "learning_rate": 0.0001073269620619751,
      "loss": 3.8817,
      "step": 4800
    },
    {
      "epoch": 0.46819191041606334,
      "grad_norm": 0.6710740923881531,
      "learning_rate": 0.00010636161791678734,
      "loss": 3.89,
      "step": 4850
    },
    {
      "epoch": 0.4730186311420021,
      "grad_norm": 0.6700254082679749,
      "learning_rate": 0.00010539627377159958,
      "loss": 3.9024,
      "step": 4900
    },
    {
      "epoch": 0.4778453518679409,
      "grad_norm": 0.7700575590133667,
      "learning_rate": 0.00010443092962641181,
      "loss": 3.8951,
      "step": 4950
    },
    {
      "epoch": 0.48267207259387973,
      "grad_norm": 0.6647247672080994,
      "learning_rate": 0.00010346558548122408,
      "loss": 3.8817,
      "step": 5000
    },
    {
      "epoch": 0.4874987933198185,
      "grad_norm": 0.6763128638267517,
      "learning_rate": 0.0001025002413360363,
      "loss": 3.8988,
      "step": 5050
    },
    {
      "epoch": 0.4923255140457573,
      "grad_norm": 0.7379176020622253,
      "learning_rate": 0.00010153489719084855,
      "loss": 3.8984,
      "step": 5100
    },
    {
      "epoch": 0.4971522347716961,
      "grad_norm": 0.7066634297370911,
      "learning_rate": 0.00010056955304566079,
      "loss": 3.8974,
      "step": 5150
    },
    {
      "epoch": 0.5019789554976349,
      "grad_norm": 0.6725513339042664,
      "learning_rate": 9.960420890047303e-05,
      "loss": 3.8737,
      "step": 5200
    },
    {
      "epoch": 0.5068056762235738,
      "grad_norm": 0.6724107265472412,
      "learning_rate": 9.863886475528527e-05,
      "loss": 3.8959,
      "step": 5250
    },
    {
      "epoch": 0.5116323969495125,
      "grad_norm": 0.7416853904724121,
      "learning_rate": 9.76735206100975e-05,
      "loss": 3.8875,
      "step": 5300
    },
    {
      "epoch": 0.5164591176754513,
      "grad_norm": 0.6899508237838745,
      "learning_rate": 9.670817646490975e-05,
      "loss": 3.8864,
      "step": 5350
    },
    {
      "epoch": 0.5212858384013901,
      "grad_norm": 0.6873894333839417,
      "learning_rate": 9.574283231972199e-05,
      "loss": 3.8795,
      "step": 5400
    },
    {
      "epoch": 0.5261125591273289,
      "grad_norm": 0.678108274936676,
      "learning_rate": 9.477748817453422e-05,
      "loss": 3.8936,
      "step": 5450
    },
    {
      "epoch": 0.5309392798532677,
      "grad_norm": 0.6883854866027832,
      "learning_rate": 9.381214402934648e-05,
      "loss": 3.8803,
      "step": 5500
    },
    {
      "epoch": 0.5357660005792065,
      "grad_norm": 0.6749073266983032,
      "learning_rate": 9.28467998841587e-05,
      "loss": 3.878,
      "step": 5550
    },
    {
      "epoch": 0.5405927213051452,
      "grad_norm": 0.7037779688835144,
      "learning_rate": 9.188145573897094e-05,
      "loss": 3.8813,
      "step": 5600
    },
    {
      "epoch": 0.5454194420310841,
      "grad_norm": 0.6959786415100098,
      "learning_rate": 9.091611159378319e-05,
      "loss": 3.8719,
      "step": 5650
    },
    {
      "epoch": 0.5502461627570229,
      "grad_norm": 0.8407511711120605,
      "learning_rate": 8.995076744859543e-05,
      "loss": 3.8651,
      "step": 5700
    },
    {
      "epoch": 0.5550728834829617,
      "grad_norm": 0.6851457953453064,
      "learning_rate": 8.898542330340767e-05,
      "loss": 3.8862,
      "step": 5750
    },
    {
      "epoch": 0.5598996042089005,
      "grad_norm": 0.705254852771759,
      "learning_rate": 8.802007915821991e-05,
      "loss": 3.8721,
      "step": 5800
    },
    {
      "epoch": 0.5647263249348393,
      "grad_norm": 0.731243908405304,
      "learning_rate": 8.705473501303215e-05,
      "loss": 3.8701,
      "step": 5850
    },
    {
      "epoch": 0.569553045660778,
      "grad_norm": 0.6740369200706482,
      "learning_rate": 8.608939086784439e-05,
      "loss": 3.8757,
      "step": 5900
    },
    {
      "epoch": 0.5743797663867168,
      "grad_norm": 0.6702284812927246,
      "learning_rate": 8.512404672265663e-05,
      "loss": 3.8768,
      "step": 5950
    },
    {
      "epoch": 0.5792064871126557,
      "grad_norm": 0.7080094218254089,
      "learning_rate": 8.415870257746888e-05,
      "loss": 3.8749,
      "step": 6000
    },
    {
      "epoch": 0.5840332078385945,
      "grad_norm": 0.6758485436439514,
      "learning_rate": 8.31933584322811e-05,
      "loss": 3.8663,
      "step": 6050
    },
    {
      "epoch": 0.5888599285645333,
      "grad_norm": 0.690872848033905,
      "learning_rate": 8.222801428709336e-05,
      "loss": 3.8665,
      "step": 6100
    },
    {
      "epoch": 0.5936866492904721,
      "grad_norm": 0.7022625803947449,
      "learning_rate": 8.12626701419056e-05,
      "loss": 3.8487,
      "step": 6150
    },
    {
      "epoch": 0.5985133700164108,
      "grad_norm": 0.661059558391571,
      "learning_rate": 8.029732599671783e-05,
      "loss": 3.8882,
      "step": 6200
    },
    {
      "epoch": 0.6033400907423496,
      "grad_norm": 0.6945874094963074,
      "learning_rate": 7.933198185153008e-05,
      "loss": 3.861,
      "step": 6250
    },
    {
      "epoch": 0.6081668114682884,
      "grad_norm": 0.6950939297676086,
      "learning_rate": 7.836663770634231e-05,
      "loss": 3.8651,
      "step": 6300
    },
    {
      "epoch": 0.6129935321942273,
      "grad_norm": 0.6632453799247742,
      "learning_rate": 7.740129356115455e-05,
      "loss": 3.867,
      "step": 6350
    },
    {
      "epoch": 0.6178202529201661,
      "grad_norm": 0.6834179759025574,
      "learning_rate": 7.64359494159668e-05,
      "loss": 3.8636,
      "step": 6400
    },
    {
      "epoch": 0.6226469736461049,
      "grad_norm": 0.6888287663459778,
      "learning_rate": 7.547060527077903e-05,
      "loss": 3.8823,
      "step": 6450
    },
    {
      "epoch": 0.6274736943720436,
      "grad_norm": 0.6793876886367798,
      "learning_rate": 7.450526112559128e-05,
      "loss": 3.8476,
      "step": 6500
    },
    {
      "epoch": 0.6323004150979824,
      "grad_norm": 0.6667171120643616,
      "learning_rate": 7.353991698040352e-05,
      "loss": 3.855,
      "step": 6550
    },
    {
      "epoch": 0.6371271358239212,
      "grad_norm": 0.7094129920005798,
      "learning_rate": 7.257457283521576e-05,
      "loss": 3.8677,
      "step": 6600
    },
    {
      "epoch": 0.64195385654986,
      "grad_norm": 0.6630128622055054,
      "learning_rate": 7.1609228690028e-05,
      "loss": 3.8519,
      "step": 6650
    },
    {
      "epoch": 0.6467805772757989,
      "grad_norm": 0.6907500624656677,
      "learning_rate": 7.064388454484024e-05,
      "loss": 3.8858,
      "step": 6700
    },
    {
      "epoch": 0.6516072980017377,
      "grad_norm": 0.6718255281448364,
      "learning_rate": 6.967854039965248e-05,
      "loss": 3.865,
      "step": 6750
    },
    {
      "epoch": 0.6564340187276764,
      "grad_norm": 0.6733804941177368,
      "learning_rate": 6.871319625446472e-05,
      "loss": 3.8612,
      "step": 6800
    },
    {
      "epoch": 0.6612607394536152,
      "grad_norm": 0.6799595355987549,
      "learning_rate": 6.774785210927696e-05,
      "loss": 3.8653,
      "step": 6850
    },
    {
      "epoch": 0.666087460179554,
      "grad_norm": 0.6799284219741821,
      "learning_rate": 6.67825079640892e-05,
      "loss": 3.8685,
      "step": 6900
    },
    {
      "epoch": 0.6709141809054928,
      "grad_norm": 0.6760403513908386,
      "learning_rate": 6.581716381890143e-05,
      "loss": 3.8676,
      "step": 6950
    },
    {
      "epoch": 0.6757409016314316,
      "grad_norm": 0.670037567615509,
      "learning_rate": 6.485181967371369e-05,
      "loss": 3.8683,
      "step": 7000
    },
    {
      "epoch": 0.6805676223573704,
      "grad_norm": 0.6529073715209961,
      "learning_rate": 6.388647552852593e-05,
      "loss": 3.8576,
      "step": 7050
    },
    {
      "epoch": 0.6853943430833092,
      "grad_norm": 0.713135838508606,
      "learning_rate": 6.292113138333816e-05,
      "loss": 3.8694,
      "step": 7100
    },
    {
      "epoch": 0.690221063809248,
      "grad_norm": 0.6771262884140015,
      "learning_rate": 6.195578723815041e-05,
      "loss": 3.8353,
      "step": 7150
    },
    {
      "epoch": 0.6950477845351868,
      "grad_norm": 0.6897537112236023,
      "learning_rate": 6.099044309296265e-05,
      "loss": 3.8365,
      "step": 7200
    },
    {
      "epoch": 0.6998745052611256,
      "grad_norm": 0.6841732859611511,
      "learning_rate": 6.002509894777488e-05,
      "loss": 3.8514,
      "step": 7250
    },
    {
      "epoch": 0.7047012259870644,
      "grad_norm": 0.6685861945152283,
      "learning_rate": 5.905975480258713e-05,
      "loss": 3.8334,
      "step": 7300
    },
    {
      "epoch": 0.7095279467130032,
      "grad_norm": 0.672909677028656,
      "learning_rate": 5.8094410657399365e-05,
      "loss": 3.845,
      "step": 7350
    },
    {
      "epoch": 0.7143546674389419,
      "grad_norm": 0.6805165410041809,
      "learning_rate": 5.7129066512211606e-05,
      "loss": 3.8487,
      "step": 7400
    },
    {
      "epoch": 0.7191813881648808,
      "grad_norm": 0.6586191058158875,
      "learning_rate": 5.6163722367023854e-05,
      "loss": 3.8586,
      "step": 7450
    },
    {
      "epoch": 0.7240081088908196,
      "grad_norm": 0.6779977083206177,
      "learning_rate": 5.519837822183609e-05,
      "loss": 3.8323,
      "step": 7500
    },
    {
      "epoch": 0.7288348296167584,
      "grad_norm": 0.6673702597618103,
      "learning_rate": 5.4233034076648323e-05,
      "loss": 3.838,
      "step": 7550
    },
    {
      "epoch": 0.7336615503426972,
      "grad_norm": 0.6664059162139893,
      "learning_rate": 5.326768993146057e-05,
      "loss": 3.8524,
      "step": 7600
    },
    {
      "epoch": 0.738488271068636,
      "grad_norm": 0.6778949499130249,
      "learning_rate": 5.230234578627281e-05,
      "loss": 3.8432,
      "step": 7650
    },
    {
      "epoch": 0.7433149917945747,
      "grad_norm": 0.6616297364234924,
      "learning_rate": 5.133700164108505e-05,
      "loss": 3.8462,
      "step": 7700
    },
    {
      "epoch": 0.7481417125205135,
      "grad_norm": 0.6826649308204651,
      "learning_rate": 5.0371657495897296e-05,
      "loss": 3.8582,
      "step": 7750
    },
    {
      "epoch": 0.7529684332464524,
      "grad_norm": 0.6874954700469971,
      "learning_rate": 4.940631335070953e-05,
      "loss": 3.8455,
      "step": 7800
    },
    {
      "epoch": 0.7577951539723912,
      "grad_norm": 0.676444411277771,
      "learning_rate": 4.844096920552177e-05,
      "loss": 3.8351,
      "step": 7850
    },
    {
      "epoch": 0.76262187469833,
      "grad_norm": 0.677115797996521,
      "learning_rate": 4.747562506033401e-05,
      "loss": 3.8506,
      "step": 7900
    },
    {
      "epoch": 0.7674485954242688,
      "grad_norm": 0.677001416683197,
      "learning_rate": 4.6510280915146254e-05,
      "loss": 3.8593,
      "step": 7950
    },
    {
      "epoch": 0.7722753161502075,
      "grad_norm": 0.6511049866676331,
      "learning_rate": 4.554493676995849e-05,
      "loss": 3.8275,
      "step": 8000
    },
    {
      "epoch": 0.7771020368761463,
      "grad_norm": 0.6851549744606018,
      "learning_rate": 4.457959262477073e-05,
      "loss": 3.852,
      "step": 8050
    },
    {
      "epoch": 0.7819287576020851,
      "grad_norm": 0.6677899956703186,
      "learning_rate": 4.361424847958298e-05,
      "loss": 3.828,
      "step": 8100
    },
    {
      "epoch": 0.786755478328024,
      "grad_norm": 0.6752908825874329,
      "learning_rate": 4.264890433439521e-05,
      "loss": 3.8559,
      "step": 8150
    },
    {
      "epoch": 0.7915821990539628,
      "grad_norm": 0.6684260368347168,
      "learning_rate": 4.1683560189207454e-05,
      "loss": 3.8379,
      "step": 8200
    },
    {
      "epoch": 0.7964089197799016,
      "grad_norm": 0.687976598739624,
      "learning_rate": 4.0718216044019695e-05,
      "loss": 3.846,
      "step": 8250
    },
    {
      "epoch": 0.8012356405058403,
      "grad_norm": 0.6636492013931274,
      "learning_rate": 3.975287189883194e-05,
      "loss": 3.8276,
      "step": 8300
    },
    {
      "epoch": 0.8060623612317791,
      "grad_norm": 0.6499834656715393,
      "learning_rate": 3.878752775364418e-05,
      "loss": 3.8553,
      "step": 8350
    },
    {
      "epoch": 0.8108890819577179,
      "grad_norm": 0.6482147574424744,
      "learning_rate": 3.782218360845642e-05,
      "loss": 3.8541,
      "step": 8400
    },
    {
      "epoch": 0.8157158026836567,
      "grad_norm": 0.6818304061889648,
      "learning_rate": 3.6856839463268654e-05,
      "loss": 3.8458,
      "step": 8450
    },
    {
      "epoch": 0.8205425234095955,
      "grad_norm": 0.6819669008255005,
      "learning_rate": 3.5891495318080895e-05,
      "loss": 3.8451,
      "step": 8500
    },
    {
      "epoch": 0.8253692441355344,
      "grad_norm": 0.6625937819480896,
      "learning_rate": 3.492615117289314e-05,
      "loss": 3.829,
      "step": 8550
    },
    {
      "epoch": 0.8301959648614731,
      "grad_norm": 0.6632077097892761,
      "learning_rate": 3.396080702770538e-05,
      "loss": 3.8375,
      "step": 8600
    },
    {
      "epoch": 0.8350226855874119,
      "grad_norm": 0.660449743270874,
      "learning_rate": 3.299546288251762e-05,
      "loss": 3.836,
      "step": 8650
    },
    {
      "epoch": 0.8398494063133507,
      "grad_norm": 0.6779372096061707,
      "learning_rate": 3.203011873732986e-05,
      "loss": 3.8247,
      "step": 8700
    },
    {
      "epoch": 0.8446761270392895,
      "grad_norm": 0.66472989320755,
      "learning_rate": 3.1064774592142095e-05,
      "loss": 3.8341,
      "step": 8750
    },
    {
      "epoch": 0.8495028477652283,
      "grad_norm": 0.6680623292922974,
      "learning_rate": 3.009943044695434e-05,
      "loss": 3.8504,
      "step": 8800
    },
    {
      "epoch": 0.8543295684911671,
      "grad_norm": 0.6823517680168152,
      "learning_rate": 2.9134086301766585e-05,
      "loss": 3.8428,
      "step": 8850
    },
    {
      "epoch": 0.8591562892171059,
      "grad_norm": 0.6343401670455933,
      "learning_rate": 2.816874215657882e-05,
      "loss": 3.8452,
      "step": 8900
    },
    {
      "epoch": 0.8639830099430447,
      "grad_norm": 0.6784788370132446,
      "learning_rate": 2.7203398011391064e-05,
      "loss": 3.8364,
      "step": 8950
    },
    {
      "epoch": 0.8688097306689835,
      "grad_norm": 0.6591499447822571,
      "learning_rate": 2.6238053866203305e-05,
      "loss": 3.84,
      "step": 9000
    },
    {
      "epoch": 0.8736364513949223,
      "grad_norm": 0.6719570755958557,
      "learning_rate": 2.5272709721015543e-05,
      "loss": 3.8285,
      "step": 9050
    },
    {
      "epoch": 0.8784631721208611,
      "grad_norm": 0.6891202330589294,
      "learning_rate": 2.4307365575827785e-05,
      "loss": 3.8185,
      "step": 9100
    },
    {
      "epoch": 0.8832898928467999,
      "grad_norm": 0.6828103065490723,
      "learning_rate": 2.3342021430640023e-05,
      "loss": 3.8309,
      "step": 9150
    },
    {
      "epoch": 0.8881166135727386,
      "grad_norm": 0.6642308235168457,
      "learning_rate": 2.2376677285452267e-05,
      "loss": 3.8434,
      "step": 9200
    },
    {
      "epoch": 0.8929433342986774,
      "grad_norm": 0.6823216676712036,
      "learning_rate": 2.1411333140264505e-05,
      "loss": 3.8404,
      "step": 9250
    },
    {
      "epoch": 0.8977700550246163,
      "grad_norm": 0.6594276428222656,
      "learning_rate": 2.0445988995076747e-05,
      "loss": 3.8496,
      "step": 9300
    },
    {
      "epoch": 0.9025967757505551,
      "grad_norm": 0.6824767589569092,
      "learning_rate": 1.9480644849888988e-05,
      "loss": 3.824,
      "step": 9350
    },
    {
      "epoch": 0.9074234964764939,
      "grad_norm": 0.6534698605537415,
      "learning_rate": 1.8515300704701226e-05,
      "loss": 3.8188,
      "step": 9400
    },
    {
      "epoch": 0.9122502172024327,
      "grad_norm": 0.6572923064231873,
      "learning_rate": 1.7549956559513467e-05,
      "loss": 3.828,
      "step": 9450
    },
    {
      "epoch": 0.9170769379283714,
      "grad_norm": 0.6662108898162842,
      "learning_rate": 1.658461241432571e-05,
      "loss": 3.8351,
      "step": 9500
    },
    {
      "epoch": 0.9219036586543102,
      "grad_norm": 0.6703724265098572,
      "learning_rate": 1.561926826913795e-05,
      "loss": 3.843,
      "step": 9550
    },
    {
      "epoch": 0.926730379380249,
      "grad_norm": 0.6825469136238098,
      "learning_rate": 1.4653924123950188e-05,
      "loss": 3.8255,
      "step": 9600
    },
    {
      "epoch": 0.9315571001061879,
      "grad_norm": 0.6516594886779785,
      "learning_rate": 1.3688579978762431e-05,
      "loss": 3.8343,
      "step": 9650
    },
    {
      "epoch": 0.9363838208321267,
      "grad_norm": 0.6571301221847534,
      "learning_rate": 1.272323583357467e-05,
      "loss": 3.8298,
      "step": 9700
    },
    {
      "epoch": 0.9412105415580655,
      "grad_norm": 0.670756459236145,
      "learning_rate": 1.175789168838691e-05,
      "loss": 3.8394,
      "step": 9750
    },
    {
      "epoch": 0.9460372622840042,
      "grad_norm": 0.7010526061058044,
      "learning_rate": 1.0792547543199152e-05,
      "loss": 3.8101,
      "step": 9800
    },
    {
      "epoch": 0.950863983009943,
      "grad_norm": 0.6639676690101624,
      "learning_rate": 9.827203398011391e-06,
      "loss": 3.8048,
      "step": 9850
    },
    {
      "epoch": 0.9556907037358818,
      "grad_norm": 0.6627694368362427,
      "learning_rate": 8.861859252823631e-06,
      "loss": 3.8241,
      "step": 9900
    },
    {
      "epoch": 0.9605174244618206,
      "grad_norm": 0.7008559107780457,
      "learning_rate": 7.896515107635872e-06,
      "loss": 3.8026,
      "step": 9950
    },
    {
      "epoch": 0.9653441451877595,
      "grad_norm": 0.6720558404922485,
      "learning_rate": 6.931170962448114e-06,
      "loss": 3.8279,
      "step": 10000
    }
  ],
  "logging_steps": 50,
  "max_steps": 10359,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 5000,
  "total_flos": 8.141609631744e+16,
  "train_batch_size": 48,
  "trial_name": null,
  "trial_params": null
}
